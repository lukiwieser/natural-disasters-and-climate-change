{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Climate Change and Deaths by Natural Disasters\n",
    "\n",
    "*Sebastian Fürndraht, Hannes Rokitte, Paul Schmitt, Lukas Wieser*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Overview\n",
    "- Introduction\n",
    "    - Research Questions\n",
    "    - Used Datasets\n",
    "    - Requirements & Dependencies\n",
    "    - Constants\n",
    "    - Download Temperature Data\n",
    "- Data Integration\n",
    "    - ...\n",
    "- Prepare Datasets\n",
    "    - ...\n",
    "- Data Exploration\n",
    "    - ...\n",
    "- Conclusion\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Used Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Requirements & Dependencies\n",
    "\n",
    "This project was created using Python 3.9.\n",
    "The exact versions of the dependencies can be installed with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import requests\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RAW\n",
    "DIS_RAW_FILE = Path('data/raw/disaster/emdat_public_2022_12_22_full.xlsx')\n",
    "TEMP_RAW_FOLDER = 'data/raw/temperature/'\n",
    "PATH_COUNTRIES_LAND_FOLDER = 'countries-land/'\n",
    "PATH_REGIONS_LAND_FOLDER = 'regions-land/'\n",
    "TEMP_GLOBAL_LAND_OCEAN_FILE = 'global-land-ocean.txt'\n",
    "TEMP_RAW_COUNTRIES_LIST_FILE = \"data/raw/temperature/countries-list.csv\"\n",
    "\n",
    "UN_COUNTRY_CODES_FILE = 'data/raw/country-codes/un-country-codes.csv'\n",
    "CIA_COUNTRY_CODES_FILE = \"data/raw/country-codes/cia-country-codes.csv\"\n",
    "\n",
    "# Preprocessed (to be deleted)\n",
    "DIS_PROCESSED_FOLDER = \"data/processed/disaster\"\n",
    "DIS_PROCESSED_ALL_FILE = Path(\"data/processed/disaster/disaster-all.csv\")\n",
    "\n",
    "\n",
    "PATH_COUNTRIES_LIST_FILE = 'temp-countries-list.csv'\n",
    "COUNTRIES_LIST_FILE = \"temp-countries-list.csv\"\n",
    "\n",
    "TEMP_PROCESSED_FOLDER = \"data/processed/temperature/\"\n",
    "TEMP_PROCESSED_COUNTRIES_LIST_FILE = \"data/processed/temperature/temp-countries-list.csv\"\n",
    "\n",
    "POP_PROCESSED_FOLDER = \"data/processed/population\"\n",
    "POP_PROCESSED_GLOBAL_FILE = 'data/processed/population/population-global.csv'\n",
    "POP_PROCESSED_REGION_FILE = 'data/processed/population/population-region.csv'\n",
    "POP_PROCESSED_COUNTRY_FILE = 'data/processed/population/population-country.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Path(DIS_PROCESSED_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "Path(TEMP_PROCESSED_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "Path(POP_PROCESSED_FOLDER).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Download Temperature Data\n",
    "Automatically download the regional and country temperature data, so we don't have to download each file by ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries = pd.read_csv(TEMP_RAW_COUNTRIES_LIST_FILE, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_regions = countries[\"Region\"].dropna().unique().tolist()\n",
    "temp_countries = countries[\"Country\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_temperature_countries(country_names: list[str]):\n",
    "    for country in country_names:\n",
    "        print(f\"downloading {country}\")\n",
    "        country_encoded = urllib.parse.quote(country.lower().replace(\" \", \"-\"),encoding='cp1252')\n",
    "        url = f\"http://berkeleyearth.lbl.gov/auto/Regional/TAVG/Text/{country_encoded}-TAVG-Trend.txt\"\n",
    "        response = requests.get(url)\n",
    "        data = response.text\n",
    "        with open(f'data/raw/temperature/countries-land/{country}.txt', 'w', encoding=\"utf-8\") as file:\n",
    "            file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_temperature_regions(region_names: list[str]):\n",
    "    for region in region_names:\n",
    "        print(f\"downloading {region}\")\n",
    "        region_encoded = urllib.parse.quote(region.lower().replace(\" \", \"-\"),encoding='cp1252')\n",
    "        url = f\"http://berkeleyearth.lbl.gov/auto/Regional/TAVG/Text/{region_encoded}-TAVG-Trend.txt\"\n",
    "        response = requests.get(url)\n",
    "        data = response.text\n",
    "        with open(f'data/raw/temperature/regions-land/{region}.txt', 'w', encoding=\"utf-8\") as file:\n",
    "            file.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Change the variable `should_download_temperature_data` to `True`, to download the temperature data of countries & regions. (This should not be necessary, since the data should already be downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "should_download_temperature_data = False\n",
    "if should_download_temperature_data:\n",
    "    download_temperature_countries(temp_countries)\n",
    "    download_temperature_regions(temp_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Data Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load Raw Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukas\\anaconda3\\lib\\site-packages\\openpyxl\\styles\\stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    }
   ],
   "source": [
    "dis = pd.read_excel(DIS_RAW_FILE, skiprows=6, sheet_name=\"emdat data\")\n",
    "\n",
    "temperature_countries = pd.read_csv(\"data/raw/temperature/countries-list.csv\", sep=\";\")\n",
    "\n",
    "pop_dict = pd.read_excel('data/raw/population/gapminder-population-v7.xlsx', sheet_name=['data-for-world-by-year', 'data-for-regions-by-year', 'data-for-countries-etc-by-year'])\n",
    "pop_global_df = pop_dict.get('data-for-world-by-year')\n",
    "pop_country_df = pop_dict.get('data-for-countries-etc-by-year')\n",
    "pop_region_gapminder = pop_dict.get('data-for-regions-by-year')\n",
    "\n",
    "un_country_codes = pd.read_csv(UN_COUNTRY_CODES_FILE, sep=\";\")\n",
    "cia_country_codes = pd.read_csv(CIA_COUNTRY_CODES_FILE, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Preprocess CIA dataset (since it is a bit messy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        Entity GENC        ISO 3166 Stanag Internet Comment\n0  Afghanistan  AFG  AF | AFG | 004    AFG      .af       -\n1     Akrotiri  XQZ       - | - | -      -        -       -\n2      Albania  ALB  AL | ALB | 008    ALB      .al       -",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Entity</th>\n      <th>GENC</th>\n      <th>ISO 3166</th>\n      <th>Stanag</th>\n      <th>Internet</th>\n      <th>Comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>AFG</td>\n      <td>AF | AFG | 004</td>\n      <td>AFG</td>\n      <td>.af</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Akrotiri</td>\n      <td>XQZ</td>\n      <td>- | - | -</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Albania</td>\n      <td>ALB</td>\n      <td>AL | ALB | 008</td>\n      <td>ALB</td>\n      <td>.al</td>\n      <td>-</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cia_country_codes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        Entity GENC Stanag Internet Comment ISO-alpha2 ISO-alpha3 ISO-numeric\n0  Afghanistan  AFG    AFG      .af       -         AF        AFG         004\n1     Akrotiri  XQZ      -        -       -        NaN        NaN         NaN\n2      Albania  ALB    ALB      .al       -         AL        ALB         008",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Entity</th>\n      <th>GENC</th>\n      <th>Stanag</th>\n      <th>Internet</th>\n      <th>Comment</th>\n      <th>ISO-alpha2</th>\n      <th>ISO-alpha3</th>\n      <th>ISO-numeric</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>AFG</td>\n      <td>AFG</td>\n      <td>.af</td>\n      <td>-</td>\n      <td>AF</td>\n      <td>AFG</td>\n      <td>004</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Akrotiri</td>\n      <td>XQZ</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Albania</td>\n      <td>ALB</td>\n      <td>ALB</td>\n      <td>.al</td>\n      <td>-</td>\n      <td>AL</td>\n      <td>ALB</td>\n      <td>008</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split iso-codes in separate columns\n",
    "cia_country_codes[[\"ISO-alpha2\",\"ISO-alpha3\",\"ISO-numeric\"]] = cia_country_codes[\"ISO 3166\"].str.split(\"|\",2,expand=True)\n",
    "cia_country_codes.drop(columns=[\"ISO 3166\"], inplace=True)\n",
    "# strip whitespaces from iso-codes\n",
    "cia_country_codes[[\"ISO-alpha2\",\"ISO-alpha3\",\"ISO-numeric\"]] = cia_country_codes[[\"ISO-alpha2\",\"ISO-alpha3\",\"ISO-numeric\"]].apply(lambda x: x.str.strip())\n",
    "# replace not existing iso-codes with NaN for more clarity\n",
    "cia_country_codes[\"ISO-alpha2\"].replace(\"-\", np.nan, inplace=True)\n",
    "cia_country_codes[\"ISO-alpha3\"].replace(\"-\", np.nan, inplace=True)\n",
    "cia_country_codes[\"ISO-numeric\"].replace(\"-\", np.nan, inplace=True)\n",
    "# show preprocessed cia data\n",
    "cia_country_codes.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Determine ISO codes for temperature data\n",
    "\n",
    "Remove aggregated countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(237, 2)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_countries.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can see, that e.g. Denmark appears twice. This issue happens multiple times, and is due to the reason that some countries are aggregates of other countries e.g. `Denmark` consists of `Denmark (Europe)` also known as `Denmark Mainland`, and `Greenland`. The bearkley earth website has a worldmap on which the country is highlighted, this helped us to better understand what each of the conflicting countries is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             Country         Region\n55            Cyprus           Asia\n56    Czech Republic         Europe\n57           Denmark  North America\n58  Denmark (Europe)         Europe\n59          Djibouti         Africa",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Country</th>\n      <th>Region</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>55</th>\n      <td>Cyprus</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>Czech Republic</td>\n      <td>Europe</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>Denmark</td>\n      <td>North America</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>Denmark (Europe)</td>\n      <td>Europe</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>Djibouti</td>\n      <td>Africa</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_countries.iloc[55: 55+5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We decided to remove the \"aggregated\" country. Here is a list of the aggregate countries we removed, their individual parts still exists in the dataset:\n",
    "- Denmark (Denmark Mainland, Greenland)\n",
    "- France (France Mainland, French Guiana, French Polynesia, French Southern and Antarctic Lands)\n",
    "- Netherlands (Netherlands Mainland, Sint Maarten, Curaçao, Aruba)\n",
    "- United Kingdom (United Kingdom + Oversea territories such as Montserrat, Bermuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(233, 2)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_countries_remove = pd.DataFrame({\n",
    "    \"Country\": [\"Denmark\",\"France\", \"Netherlands\", \"United Kingdom\"],\n",
    "    \"Region\": [\"North America\", np.nan, \"Europe\", \"Europe\"]\n",
    "})\n",
    "temperature_countries_cleaned = pd.concat([temperature_countries, temperature_countries_remove]).drop_duplicates(keep=False)\n",
    "temperature_countries_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Rename Countries & Match ISO Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           Country ISO-alpha3\n18    Baker Island        NaN\n113   Kingman Reef        NaN\n161  Palmyra Atoll        NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Country</th>\n      <th>ISO-alpha3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18</th>\n      <td>Baker Island</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>Kingman Reef</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>Palmyra Atoll</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some countries need to be renamed so that we find the matching country-code later\n",
    "new_country_names = {\n",
    "    \"Denmark (Europe)\": \"Denmark\",\n",
    "    \"France (Europe)\": \"France\",\n",
    "    \"Netherlands (Europe)\": \"Netherlands\",\n",
    "    \"United Kingdom (Europe)\": \"United Kingdom\",\n",
    "    \"Åland\": \"Åland Islands\",\n",
    "    \"Czech Republic\": \"Czechia\",\n",
    "    \"Turkey\": \"Türkiye\",\n",
    "    \"Svalbard and Jan Mayen\": \"Svalbard and Jan Mayen Islands\",\n",
    "    \"Cape Verde\": \"Cabo Verde\",\n",
    "    \"Turks and Caicas Islands\": \"Turks and Caicos Islands\",\n",
    "    \"Swaziland\": \"Eswatini\",\n",
    "    \"Macedonia\": \"North Macedonia\",\n",
    "    \"Côte d'Ivoire\": \"Côte d’Ivoire\",\n",
    "    \"Federated States of Micronesia\": \"Micronesia (Federated States of)\",\n",
    "    \"South Georgia and the South Sandwich Isla\": \"South Georgia and the South Sandwich Islands\",\n",
    "    \"Bonaire, Saint Eustatius and Saba\": \"Bonaire, Sint Eustatius and Saba\",\n",
    "    \"Congo (Democratic Republic of the)\": \"Democratic Republic of the Congo\",\n",
    "    \"South Korea\": \"Korea, South\",\n",
    "    \"North Korea\": \"Korea, North\",\n",
    "    \"Palestina\": \"State of Palestine\"\n",
    "}\n",
    "\n",
    "temperature_countries_cleaned = temperature_countries_cleaned.replace({\"Country\": new_country_names}, inplace=False)\n",
    "\n",
    "# left-join cia-country-codes and un-country-codes\n",
    "temperature_countries_with_iso = temperature_countries_cleaned.merge(cia_country_codes,how=\"left\",left_on='Country', right_on='Entity')[[\"Country\",\"ISO-alpha3\"]]\n",
    "temperature_countries_with_iso = temperature_countries_with_iso.merge(un_country_codes,how=\"left\",left_on='Country', right_on='Country or Area')[[\"Country\",\"ISO-alpha3\", \"ISO-alpha3 Code\"]]\n",
    "\n",
    "# fill missing cia-country codes with un-country-codes\n",
    "temperature_countries_with_iso[\"ISO-alpha3\"].fillna(temperature_countries_with_iso[\"ISO-alpha3 Code\"], inplace=True)\n",
    "temperature_countries_with_iso.drop(columns=[\"ISO-alpha3 Code\"], inplace=True)\n",
    "\n",
    "# show countries for which we could not find an ISO code\n",
    "temperature_countries_with_iso[temperature_countries_with_iso[\"ISO-alpha3\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "These 3 countries/areas do not have any country codes in general, and are quite small, so we just ignore them later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Which countries are in which datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Disaster vs. Temperature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countries in emdat & berkely: 209\n",
      "countries in emdat but not berkely (22):\n",
      "['ANT', 'AZO', 'BMU', 'BRN', 'COK', 'CSK', 'DDR', 'DFR', 'MDV', 'MHL', 'SCG', 'SHN', 'SPI', 'SSD', 'SUN', 'TKL', 'TUV', 'VUT', 'WLF', 'YMD', 'YMN', 'YUG']\n",
      "countries in berkely but not emdat (20):\n",
      "['ABW', 'ALA', 'AND', 'ATA', 'ATF', 'BES', 'CXR', 'ESH', 'FLK', 'FRO', 'GGY', 'GRL', 'HMD', 'JEY', 'LIE', 'MCO', 'SGS', 'SJM', 'SMR', 'SPM']\n"
     ]
    }
   ],
   "source": [
    "berkely_iso_codes = set(temperature_countries_with_iso[\"ISO-alpha3\"].dropna().tolist())\n",
    "emdat_iso_codes = set(dis[\"ISO\"].unique().tolist())\n",
    "\n",
    "emdat_and_bekely = emdat_iso_codes.intersection(berkely_iso_codes)\n",
    "emdat_without_berkely = emdat_iso_codes-emdat_and_bekely\n",
    "berkely_without_emdat = berkely_iso_codes-emdat_and_bekely\n",
    "\n",
    "print(f\"countries in emdat & berkely: {len(emdat_and_bekely)}\")\n",
    "print(f\"countries in emdat but not berkely ({len(emdat_without_berkely)}):\")\n",
    "print(sorted(emdat_without_berkely))\n",
    "print(f\"countries in berkely but not emdat ({len(berkely_without_emdat)}):\")\n",
    "print(sorted(berkely_without_emdat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The Countries for which we have disaster data, but no temperature data are as follows:\n",
    "- Existing Countries (usually very small countries/islands):\n",
    "     - `AZO` Azores Islands, `BMU` Bermuda, `BRN` Brunei Darussalam, `COK` Cook Islands (the), `MDV` Maldives, `MHL` Marshall Islands (the), `SHN` Saint Helena, Ascension and Tristan da Cunha, `SSD` South Sudan, `TKL` Tokelau, `TUV` Tuvalu, `VUT` Vanuatu, `WLF` Wallis and Futuna\n",
    "- Existing Countries (but invalid country code):\n",
    "    - `SPI` Canary Islands\n",
    "- Former Countries:\n",
    "    - `ANT` Netherlands Antilles,`CSK` Czechoslovakia,`DDR` Germany Dem Rep,`DFR` Germany Fed Rep,`SCG` Serbia Montenegro,`SUN` Soviet Union,`YMD` Yemen P Dem Rep,`YMN` Yemen Arab Rep,`YUG` Yugoslavia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Disaster vs Population Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gapminder_iso_codes = set(pop_country_df[\"geo\"].str.upper().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countries in emdat & gapminder: 191\n",
      "countries in emdat but not gapminder (40):\n",
      "['AIA', 'ANT', 'ASM', 'AZO', 'BLM', 'BMU', 'COK', 'CSK', 'CUW', 'CYM', 'DDR', 'DFR', 'GLP', 'GUF', 'GUM', 'IMN', 'MAC', 'MAF', 'MNP', 'MSR', 'MTQ', 'MYT', 'NCL', 'NIU', 'PRI', 'PYF', 'REU', 'SCG', 'SHN', 'SPI', 'SUN', 'SXM', 'TCA', 'TKL', 'VGB', 'VIR', 'WLF', 'YMD', 'YMN', 'YUG']\n",
      "countries in gapminder but not emdat (6):\n",
      "['AND', 'HOS', 'LIE', 'MCO', 'NRU', 'SMR']\n"
     ]
    }
   ],
   "source": [
    "emdat_and_gapminder = emdat_iso_codes.intersection(gapminder_iso_codes)\n",
    "emdat_without_gapminder = emdat_iso_codes-emdat_and_gapminder\n",
    "gapminder_without_emdat = gapminder_iso_codes-emdat_and_gapminder\n",
    "\n",
    "print(f\"countries in emdat & gapminder: {len(emdat_and_gapminder)}\")\n",
    "print(f\"countries in emdat but not gapminder ({len(emdat_without_gapminder)}):\")\n",
    "print(sorted(emdat_without_gapminder))\n",
    "print(f\"countries in gapminder but not emdat ({len(gapminder_without_emdat)}):\")\n",
    "print(sorted(gapminder_without_emdat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The Countries for which we have disaster data, but no population data are as follows:\n",
    "- Existing Countries (independent)\n",
    "    - `COK` Cook Islands (the), `NIU` Niue\n",
    "- Existing Countries (dependent e.g .oversea territories)\n",
    "\t- `AIA` Anguilla, `ASM` American Samoa, `AZO` Azores Islands, `BLM` Saint Barthélemy, `BMU` Bermuda, `CUW` Curaçao, `CYM` Cayman Islands (the), `GLP` Guadeloupe, `GUF` French Guiana, `GUM` Guam, `IMN` Isle of Man, `MAC` Macao, `MAF` Saint Martin (French Part), `MNP` Northern Mariana Islands (the), `MSR` Montserrat, `MTQ` Martinique, `MYT` Mayotte, `NCL` New Caledonia, `PRI` Puerto Rico, `PYF` French Polynesia, `REU` Réunion, `SHN` Saint Helena, Ascension and Tristan da Cunha, `SPI` Canary Islands, `SXM` Sint Maarten (Dutch part), `TCA` Turks and Caicos Islands (the), `TKL` Tokelau, `VGB` Virgin Island (British), `VIR` Virgin Island (U.S.), `WLF` Wallis and Futuna\n",
    "- Former Countries\n",
    "\t- `ANT` Netherlands Antilles, `CSK` Czechoslovakia, `DDR` Germany Dem Rep, `DFR` Germany Fed Rep, `SCG` Serbia Montenegro, `SUN` Soviet Union, `YMD` Yemen P Dem Rep, `YMN` Yemen Arab Rep, `YUG` Yugoslavia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Which regions are in which datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Africa', 'Asia', 'Europe', 'Americas', 'Oceania']"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis[\"Continent\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['africa', 'asia', 'europe', 'americas']"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_region_gapminder[\"geo\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Asia', 'Europe', 'Africa', 'South America', 'Oceania', 'North America']"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_countries[\"Region\"].dropna().unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, each dataset has a different number of regions. Additionally, we don't know which countries belong to each region. For example the region Europe could consist of different countries the disaster dataset than in the temperature dataset. That's why we decided to compute the regional data ourselves, by aggregating the countries according to UN Regions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Which countries are need to be manually assigned a region"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check which countries are not in the UN Dataset, and thus need to be manually assigned a region:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countries in emdat, but not un (12): \n",
      "{'DDR', 'YUG', 'DFR', 'YMD', 'CSK', 'ANT', 'YMN', 'SCG', 'TWN', 'AZO', 'SPI', 'SUN'}\n",
      "countries in gapminder, but not un (2): \n",
      "{'HOS', 'TWN'}\n",
      "countries in berkely, but not un (1): \n",
      "{'TWN'}\n"
     ]
    }
   ],
   "source": [
    "un_iso_codes = set(un_country_codes[\"ISO-alpha3 Code\"].tolist())\n",
    "\n",
    "emdat_without_un = emdat_iso_codes-un_iso_codes\n",
    "gapminder_without_un = gapminder_iso_codes-un_iso_codes\n",
    "berkely_without_un = berkely_iso_codes-un_iso_codes\n",
    "\n",
    "print(f\"countries in emdat, but not un ({len(emdat_without_un)}): \\n{emdat_without_un}\")\n",
    "print(f\"countries in gapminder, but not un ({len(gapminder_without_un)}): \\n{gapminder_without_un}\")\n",
    "print(f\"countries in berkely, but not un ({len(berkely_without_un)}): \\n{berkely_without_un}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Prepare Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Disaster Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Task:\n",
    "\n",
    "The goal is to convert the data into the following formats for later use.\n",
    "Along the way, this notebook does some data-preparation\n",
    "\n",
    "\n",
    "Disaster-All\n",
    "disaster/disaster-all:\n",
    "Columns: disaster_no, year, subgroup, type, total_deaths, dis_mag_value, dis_mag_scale, start_year, end_year\n",
    "Other interesting columns?\n",
    "\n",
    "Prefix: dis\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Publisher: Centre for Research on the Epidemiology of Disasters (CRED)\n",
    "\n",
    "CRED defines a disaster as “a situation or event that overwhelms local capacity, necessitating a\n",
    "request at the national or international level for external assistance; an unforeseen and often sudden\n",
    "event that causes great damage, destruction and human suffering”\n",
    "\n",
    "For a disaster to be entered into the database at least one of the following criteria must be fulfilled:\n",
    "\n",
    "- 10 or more people reported killed\n",
    "- 100 or more people reported affected\n",
    "- Declaration of a state of emergency\n",
    "- Call for international assistance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First look at the disaster data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "          Dis No  Year   Seq Glide Disaster Group Disaster Subgroup  \\\n0  1900-9002-CPV  1900  9002   NaN        Natural    Climatological   \n1  1900-9001-IND  1900  9001   NaN        Natural    Climatological   \n2  1901-0003-BEL  1901     3   NaN  Technological     Technological   \n3  1902-0012-GTM  1902    12   NaN        Natural       Geophysical   \n4  1902-0003-GTM  1902     3   NaN        Natural       Geophysical   \n\n         Disaster Type Disaster Subtype Disaster Subsubtype   Event Name  ...  \\\n0              Drought          Drought                 NaN          NaN  ...   \n1              Drought          Drought                 NaN          NaN  ...   \n2  Industrial accident        Explosion                 NaN    Coal mine  ...   \n3           Earthquake  Ground movement                 NaN          NaN  ...   \n4    Volcanic activity         Ash fall                 NaN  Santa Maria  ...   \n\n  Reconstruction Costs, Adjusted ('000 US$) Insured Damages ('000 US$)  \\\n0                                       NaN                        NaN   \n1                                       NaN                        NaN   \n2                                       NaN                        NaN   \n3                                       NaN                        NaN   \n4                                       NaN                        NaN   \n\n  Insured Damages, Adjusted ('000 US$) Total Damages ('000 US$)  \\\n0                                  NaN                      NaN   \n1                                  NaN                      NaN   \n2                                  NaN                      NaN   \n3                                  NaN                  25000.0   \n4                                  NaN                      NaN   \n\n  Total Damages, Adjusted ('000 US$)       CPI Adm Level Admin1 Code  \\\n0                                NaN  3.077091       NaN         NaN   \n1                                NaN  3.077091       NaN         NaN   \n2                                NaN  3.077091       NaN         NaN   \n3                           781207.0  3.200175       NaN         NaN   \n4                                NaN  3.200175       NaN         NaN   \n\n  Admin2 Code Geo Locations  \n0         NaN           NaN  \n1         NaN           NaN  \n2         NaN           NaN  \n3         NaN           NaN  \n4         NaN           NaN  \n\n[5 rows x 50 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dis No</th>\n      <th>Year</th>\n      <th>Seq</th>\n      <th>Glide</th>\n      <th>Disaster Group</th>\n      <th>Disaster Subgroup</th>\n      <th>Disaster Type</th>\n      <th>Disaster Subtype</th>\n      <th>Disaster Subsubtype</th>\n      <th>Event Name</th>\n      <th>...</th>\n      <th>Reconstruction Costs, Adjusted ('000 US$)</th>\n      <th>Insured Damages ('000 US$)</th>\n      <th>Insured Damages, Adjusted ('000 US$)</th>\n      <th>Total Damages ('000 US$)</th>\n      <th>Total Damages, Adjusted ('000 US$)</th>\n      <th>CPI</th>\n      <th>Adm Level</th>\n      <th>Admin1 Code</th>\n      <th>Admin2 Code</th>\n      <th>Geo Locations</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1900-9002-CPV</td>\n      <td>1900</td>\n      <td>9002</td>\n      <td>NaN</td>\n      <td>Natural</td>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>Drought</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.077091</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1900-9001-IND</td>\n      <td>1900</td>\n      <td>9001</td>\n      <td>NaN</td>\n      <td>Natural</td>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>Drought</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.077091</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1901-0003-BEL</td>\n      <td>1901</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>Technological</td>\n      <td>Technological</td>\n      <td>Industrial accident</td>\n      <td>Explosion</td>\n      <td>NaN</td>\n      <td>Coal mine</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.077091</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1902-0012-GTM</td>\n      <td>1902</td>\n      <td>12</td>\n      <td>NaN</td>\n      <td>Natural</td>\n      <td>Geophysical</td>\n      <td>Earthquake</td>\n      <td>Ground movement</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>25000.0</td>\n      <td>781207.0</td>\n      <td>3.200175</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1902-0003-GTM</td>\n      <td>1902</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>Natural</td>\n      <td>Geophysical</td>\n      <td>Volcanic activity</td>\n      <td>Ash fall</td>\n      <td>NaN</td>\n      <td>Santa Maria</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.200175</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 50 columns</p>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Select & Rename Attributes\n",
    "\n",
    "1. Replace whitespaces with underscores\n",
    "2. Convert every character to lowercase\n",
    "3. Rename specific columns to ensure uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove whitespaces from all col-names and convert them to lower-case\n",
    "dis.columns = [c.replace(' ', '_').lower() for c in dis.columns]\n",
    "dis.rename(columns={'country':'country_name', 'iso':'country_code', 'disaster_group':'group','disaster_subgroup':'subgroup', 'disaster_subtype':'subtype', 'disaster_type':'type', 'total_deaths':'deaths'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the most interesting columns\n",
    "dis_all_col_names = [\"year\", \"dis_no\", \"country_name\", \"country_code\", \"location\",\"group\",\"subgroup\", \"type\", \"subtype\", \"deaths\", \"dis_mag_value\", \"dis_mag_scale\", \"start_year\", \"end_year\"]\n",
    "dis_all = dis.filter(items=dis_all_col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Which disaster groups are present in the dataset ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There are Natural disasters, technological disasters as well as complex disasters that represent specific events (e.g. famine) which are not directly linked to a natural hazard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['Natural', 'Technological', 'Complex Disasters'], dtype=object)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all[\"group\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We only focus on disasters which have a natural causation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis_all = dis_all[dis_all[\"group\"] == \"Natural\"].copy()\n",
    "dis_all.drop(columns=\"group\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Which types of natural disasters are there ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             subgroup                   type      deaths\n0          Biological        Animal accident        12.0\n1          Biological               Epidemic   9618804.0\n2          Biological     Insect infestation         0.0\n3      Climatological                Drought  11733889.0\n4      Climatological  Glacial lake outburst       262.0\n5      Climatological               Wildfire      4653.0\n6   Extra-terrestrial                 Impact         0.0\n7         Geophysical             Earthquake   2343912.0\n8         Geophysical    Mass movement (dry)      4644.0\n9         Geophysical      Volcanic activity     86893.0\n10       Hydrological                  Flood   7002992.0\n11       Hydrological              Landslide     67477.0\n12     Meteorological   Extreme temperature     194056.0\n13     Meteorological                    Fog      4000.0\n14     Meteorological                  Storm   1403609.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subgroup</th>\n      <th>type</th>\n      <th>deaths</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Biological</td>\n      <td>Animal accident</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Biological</td>\n      <td>Epidemic</td>\n      <td>9618804.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Biological</td>\n      <td>Insect infestation</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>11733889.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Climatological</td>\n      <td>Glacial lake outburst</td>\n      <td>262.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Climatological</td>\n      <td>Wildfire</td>\n      <td>4653.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Extra-terrestrial</td>\n      <td>Impact</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Geophysical</td>\n      <td>Earthquake</td>\n      <td>2343912.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Geophysical</td>\n      <td>Mass movement (dry)</td>\n      <td>4644.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Geophysical</td>\n      <td>Volcanic activity</td>\n      <td>86893.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Hydrological</td>\n      <td>Flood</td>\n      <td>7002992.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Hydrological</td>\n      <td>Landslide</td>\n      <td>67477.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Meteorological</td>\n      <td>Extreme temperature</td>\n      <td>194056.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Meteorological</td>\n      <td>Fog</td>\n      <td>4000.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Meteorological</td>\n      <td>Storm</td>\n      <td>1403609.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all.groupby([\"subgroup\",\"type\"]).agg({\"deaths\":\"sum\"}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The types of disasters are mostly the ones a normal person would expect when thinking about natural disasters. But there are some strange types like insect-infestations or animal-accident which are not that obvious to understand, they also have basically no deaths. Also for our research we want to exclude Epidemics since it would go beyond the scope of this task.\n",
    "\n",
    "Therefore also decided to omit disasters of the subgroups `Biological` and `Extra-terrestrial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis_all = dis_all[(dis_all[\"subgroup\"] != \"Biological\") & (dis_all[\"subgroup\"] != \"Extra-terrestrial\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We only consider the following types of disasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['Drought', 'Earthquake', 'Volcanic activity',\n       'Mass movement (dry)', 'Storm', 'Flood', 'Landslide', 'Wildfire',\n       'Extreme temperature ', 'Fog', 'Glacial lake outburst'],\n      dtype=object)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all[\"type\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Handle Missing Values\n",
    "\n",
    "Fill missing Values for the number of deaths\n",
    "\n",
    "We can assume that missing values for the number of deaths of a particular disaster means that the deathtoll was 0.\n",
    "\n",
    "For the subtype we take a look for which type of natural disasters a subtype is not provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "year                0\ndis_no              0\ncountry_name        0\ncountry_code        0\nlocation         1449\nsubgroup            0\ntype                0\nsubtype          3116\ndeaths           4412\ndis_mag_value    9903\ndis_mag_scale    1114\nstart_year          0\nend_year            0\ndtype: int64"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['Flood', 'Storm', 'Landslide', 'Wildfire', 'Fog',\n       'Mass movement (dry)', 'Volcanic activity', 'Drought',\n       'Earthquake', 'Glacial lake outburst'], dtype=object)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all[dis_all[\"subtype\"].isna()][\"type\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Unfortunately the missing values in the subtype column do not correspond to specific types of disasters.\n",
    "We can not conclude that easily what caused the values to be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['Drought', nan], dtype=object)"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all[dis_all[\"type\"] == \"Drought\"].subtype.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Disasters of type drought have no further more specific subtype.\n",
    "Consequently, we assign all droughts with missing values for subtype, the general type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis_all.loc[dis_all[\"type\"] == \"Drought\", \"subtype\"] = dis_all.loc[dis_all[\"type\"] == \"Drought\", \"subtype\"].fillna(\"Drought\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For all other types, we unfortunately can not guess the subtype.\n",
    "Therefore, we assign a custom label \"Uncategorized\" + the type of the disaster for each missing subtype\n",
    "We do not need to do this for disasters of type extreme temperature, since subtypes for all observations are specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis_all.loc[dis_all[\"type\"] == \"Earthquake\", \"subtype\"] = dis_all.loc[dis_all[\"type\"] == \"Earthquake\", \"subtype\"].fillna(\"Uncategorized_Earthquake\")\n",
    "dis_all.loc[dis_all[\"type\"] == \"Storm\", \"subtype\"] = dis_all.loc[dis_all[\"type\"] == \"Storm\", \"subtype\"].fillna(\"Uncategorized_Storm\")\n",
    "dis_all.loc[dis_all[\"type\"] == \"Flood\", \"subtype\"] = dis_all.loc[dis_all[\"type\"] == \"Flood\", \"subtype\"].fillna(\"Uncategorized_Flood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We assume that disasters with no death toll reported have a death toll of 0.\n",
    "This also aligns with the information we get from emdat (deaths < 10 are missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis_all[['deaths']] = dis_all[['deaths']].fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Determine Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As mentioned in `data integration` we want to compute the region of each disaster, by taking the UN Region that is assigned to each Country, in which the disaster occurred.\n",
    "Some country codes are not in the list of UN countries, thus we handle them specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countries in emdat, but not un (12): \n",
      "{'DDR', 'YUG', 'DFR', 'YMD', 'CSK', 'ANT', 'YMN', 'SCG', 'TWN', 'AZO', 'SPI', 'SUN'}\n"
     ]
    }
   ],
   "source": [
    "dis_iso_codes = set(dis_all[\"country_code\"].unique())\n",
    "un_iso_codes = set(un_country_codes[\"ISO-alpha3 Code\"].tolist())\n",
    "emdat_without_un = dis_iso_codes-un_iso_codes\n",
    "print(f\"countries in emdat, but not un ({len(emdat_without_un)}): \\n{emdat_without_un}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Automatically assign regions with UN Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For countries that were split in the past, but are now unified, we can just assign the unified country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Germany\n",
    "dis_all.loc[dis_all['country_code'] == \"DFR\",'country_code'] = \"DEU\"\n",
    "dis_all.loc[dis_all['country_code'] == \"DDR\",'country_code'] = \"DEU\"\n",
    "# Yemen\n",
    "dis_all.loc[dis_all['country_code'] == \"YMD\",'country_code'] = \"YEM\"\n",
    "dis_all.loc[dis_all['country_code'] == \"YMN\",'country_code'] = \"YEM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next we determine the region by using the un dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis_all = pd.merge(dis_all, un_country_codes[[\"ISO-alpha3 Code\",\"Region Name\", \"Region Code\"]], left_on='country_code', right_on='ISO-alpha3 Code', how='left')\n",
    "dis_all.rename(columns={\"Region Code\": \"region_code\", \"Region Name\": \"region_name\"}, inplace=True)\n",
    "dis_all.drop(columns=[\"ISO-alpha3 Code\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Manually Assign Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Some countries clearly belong to one region, so we can assign the disasters manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taiwan\n",
    "dis_all.loc[dis_all['country_code']=='TWN','region_name'] = 'Asia'\n",
    "dis_all.loc[dis_all['country_code']=='TWN','region_code'] = 142\n",
    "# Czechoslovakia\n",
    "dis_all.loc[dis_all['country_code']=='CSK','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='CSK','region_code'] = 150\n",
    "# Yugoslavia\n",
    "dis_all.loc[dis_all['country_code']=='YUG','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='YUG','region_code'] = 150\n",
    "# Serbia Montenegro\n",
    "dis_all.loc[dis_all['country_code']=='SCG','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='SCG','region_code'] = 150\n",
    "# Netherlands Antilles\n",
    "dis_all.loc[dis_all['country_code']=='ANT','region_name'] = 'Americas'\n",
    "dis_all.loc[dis_all['country_code']=='ANT','region_code'] = 19\n",
    "# Azores Islands\n",
    "dis_all.loc[dis_all['country_code']=='AZO','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='AZO','region_code'] = 150\n",
    "# Canary Islands\n",
    "dis_all.loc[dis_all['country_code']=='SPI','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='SPI','region_code'] = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Manually Assign Regions of Disasters Soviet Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For the Soviet Union disasters can occur in the european and/or asian parts.\n",
    "Thus we also take the \"location\" attribute into account and try to derive if the disaster was in europe or asia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask_europe = dis_all.loc[dis_all['country_code']=='SUN'][\"location\"].str.contains(\"Russian Federation|Ukraine|Moldavia|Siberia\").fillna(False)\n",
    "mask_asia = dis_all.loc[dis_all['country_code']=='SUN'][\"location\"].str.contains(\"Kazakhstan|Azerbaijan|Uzbekistan|Turkmenistan|Georgia|Armenia|Kyrgystan|Tajikistan|Tajiskistan|Tadzhikistan|Tadjikistan|Caucasus region|Dushanbe\", case=False).fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     year         dis_no  country_name country_code  \\\n849  1921  1921-9001-SUN  Soviet Union          SUN   \n\n                                              location        subgroup  \\\n849  South Ukraine, Volga, Ural (Kazakhstan,Russian...  Climatological   \n\n        type  subtype     deaths  dis_mag_value dis_mag_scale  start_year  \\\n849  Drought  Drought  1200000.0            NaN           Km2        1921   \n\n     end_year region_name  region_code  \n849      1921         NaN          NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>dis_no</th>\n      <th>country_name</th>\n      <th>country_code</th>\n      <th>location</th>\n      <th>subgroup</th>\n      <th>type</th>\n      <th>subtype</th>\n      <th>deaths</th>\n      <th>dis_mag_value</th>\n      <th>dis_mag_scale</th>\n      <th>start_year</th>\n      <th>end_year</th>\n      <th>region_name</th>\n      <th>region_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>849</th>\n      <td>1921</td>\n      <td>1921-9001-SUN</td>\n      <td>Soviet Union</td>\n      <td>SUN</td>\n      <td>South Ukraine, Volga, Ural (Kazakhstan,Russian...</td>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>Drought</td>\n      <td>1200000.0</td>\n      <td>NaN</td>\n      <td>Km2</td>\n      <td>1921</td>\n      <td>1921</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all[dis_all['country_code']=='SUN'][mask_europe & mask_asia]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Only one Event happened in both the asian as well as the european part of the soviet union.\n",
    "It is also a major event since it is a drought which caused the death of 1.2 million people.\n",
    "\n",
    "Researching the details of this event one can conclude that this observation can only be the Russian famine of 1921–1922.\n",
    "It mostly affected people living in europe, hence we assign this single observation the region europe.\n",
    "(https://en.wikipedia.org/wiki/Russian_famine_of_1921%E2%80%931922)\n",
    "\n",
    "For all other observations, the region should be unambiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis_all.loc[(dis_all['country_code']=='SUN') & mask_europe, \"region_name\"] = \"Europe\"\n",
    "dis_all.loc[(dis_all['country_code']=='SUN') & mask_europe, \"region_code\"] = 150\n",
    "\n",
    "dis_all.loc[(dis_all['country_code']=='SUN') & mask_asia, \"region_name\"] = \"Asia\"\n",
    "dis_all.loc[(dis_all['country_code']=='SUN') & mask_asia, \"region_code\"] = 142\n",
    "\n",
    "dis_all.loc[dis_all['dis_no']=='1921-9001-SUN', \"region_name\"] = \"Europe\"\n",
    "dis_all.loc[dis_all['dis_no']=='1921-9001-SUN', \"region_code\"] = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The only disasters without a region are now these 3 in the soviet Union. However, since they have no death count we can safely ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       year         dis_no                            country_name  \\\n0      1900  1900-9002-CPV                              Cabo Verde   \n1      1900  1900-9001-IND                                   India   \n2      1902  1902-0012-GTM                               Guatemala   \n3      1902  1902-0003-GTM                               Guatemala   \n4      1902  1902-0010-GTM                               Guatemala   \n...     ...            ...                                     ...   \n14866  2022  2022-0788-ZAF                            South Africa   \n14867  2022  2022-0313-ZAF                            South Africa   \n14868  2022  2022-0356-ZAF                            South Africa   \n14869  2022  2022-0209-COD  Congo (the Democratic Republic of the)   \n14870  2022  2022-0650-SSD                             South Sudan   \n\n      country_code                                           location  \\\n0              CPV                                        Countrywide   \n1              IND                                             Bengal   \n2              GTM                          Quezaltenango, San Marcos   \n3              GTM                                                NaN   \n4              GTM                                                NaN   \n...            ...                                                ...   \n14866          ZAF               Johannesburg City (Gauteng Province)   \n14867          ZAF                    Durban (KwaZulu-Natal province)   \n14868          ZAF  Kleinvlei, Ravensmead, Goodwood, Strand, Bonte...   \n14869          COD                                                NaN   \n14870          SSD  Western Equatoria, Northern Bahr el Ghazal, Wa...   \n\n             subgroup               type              subtype     deaths  \\\n0      Climatological            Drought              Drought    11000.0   \n1      Climatological            Drought              Drought  1250000.0   \n2         Geophysical         Earthquake      Ground movement     2000.0   \n3         Geophysical  Volcanic activity             Ash fall     1000.0   \n4         Geophysical  Volcanic activity             Ash fall     6000.0   \n...               ...                ...                  ...        ...   \n14866    Hydrological              Flood          Flash flood       17.0   \n14867    Hydrological              Flood  Uncategorized_Flood        0.0   \n14868    Hydrological              Flood  Uncategorized_Flood        0.0   \n14869  Meteorological              Storm     Convective storm       26.0   \n14870    Hydrological              Flood  Uncategorized_Flood        0.0   \n\n       dis_mag_value dis_mag_scale  start_year  end_year region_name  \\\n0                NaN           Km2        1900      1900      Africa   \n1                NaN           Km2        1900      1900        Asia   \n2                8.0       Richter        1902      1902    Americas   \n3                NaN           NaN        1902      1902    Americas   \n4                NaN           NaN        1902      1902    Americas   \n...              ...           ...         ...       ...         ...   \n14866            NaN           Km2        2022      2022      Africa   \n14867            NaN           Km2        2022      2022      Africa   \n14868            NaN           Km2        2022      2022      Africa   \n14869            NaN           Kph        2022      2022      Africa   \n14870            NaN           Km2        2022      2022      Africa   \n\n       region_code  \n0              2.0  \n1            142.0  \n2             19.0  \n3             19.0  \n4             19.0  \n...            ...  \n14866          2.0  \n14867          2.0  \n14868          2.0  \n14869          2.0  \n14870          2.0  \n\n[14871 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>dis_no</th>\n      <th>country_name</th>\n      <th>country_code</th>\n      <th>location</th>\n      <th>subgroup</th>\n      <th>type</th>\n      <th>subtype</th>\n      <th>deaths</th>\n      <th>dis_mag_value</th>\n      <th>dis_mag_scale</th>\n      <th>start_year</th>\n      <th>end_year</th>\n      <th>region_name</th>\n      <th>region_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1900</td>\n      <td>1900-9002-CPV</td>\n      <td>Cabo Verde</td>\n      <td>CPV</td>\n      <td>Countrywide</td>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>Drought</td>\n      <td>11000.0</td>\n      <td>NaN</td>\n      <td>Km2</td>\n      <td>1900</td>\n      <td>1900</td>\n      <td>Africa</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1900</td>\n      <td>1900-9001-IND</td>\n      <td>India</td>\n      <td>IND</td>\n      <td>Bengal</td>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>Drought</td>\n      <td>1250000.0</td>\n      <td>NaN</td>\n      <td>Km2</td>\n      <td>1900</td>\n      <td>1900</td>\n      <td>Asia</td>\n      <td>142.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1902</td>\n      <td>1902-0012-GTM</td>\n      <td>Guatemala</td>\n      <td>GTM</td>\n      <td>Quezaltenango, San Marcos</td>\n      <td>Geophysical</td>\n      <td>Earthquake</td>\n      <td>Ground movement</td>\n      <td>2000.0</td>\n      <td>8.0</td>\n      <td>Richter</td>\n      <td>1902</td>\n      <td>1902</td>\n      <td>Americas</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1902</td>\n      <td>1902-0003-GTM</td>\n      <td>Guatemala</td>\n      <td>GTM</td>\n      <td>NaN</td>\n      <td>Geophysical</td>\n      <td>Volcanic activity</td>\n      <td>Ash fall</td>\n      <td>1000.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1902</td>\n      <td>1902</td>\n      <td>Americas</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1902</td>\n      <td>1902-0010-GTM</td>\n      <td>Guatemala</td>\n      <td>GTM</td>\n      <td>NaN</td>\n      <td>Geophysical</td>\n      <td>Volcanic activity</td>\n      <td>Ash fall</td>\n      <td>6000.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1902</td>\n      <td>1902</td>\n      <td>Americas</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14866</th>\n      <td>2022</td>\n      <td>2022-0788-ZAF</td>\n      <td>South Africa</td>\n      <td>ZAF</td>\n      <td>Johannesburg City (Gauteng Province)</td>\n      <td>Hydrological</td>\n      <td>Flood</td>\n      <td>Flash flood</td>\n      <td>17.0</td>\n      <td>NaN</td>\n      <td>Km2</td>\n      <td>2022</td>\n      <td>2022</td>\n      <td>Africa</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>14867</th>\n      <td>2022</td>\n      <td>2022-0313-ZAF</td>\n      <td>South Africa</td>\n      <td>ZAF</td>\n      <td>Durban (KwaZulu-Natal province)</td>\n      <td>Hydrological</td>\n      <td>Flood</td>\n      <td>Uncategorized_Flood</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>Km2</td>\n      <td>2022</td>\n      <td>2022</td>\n      <td>Africa</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>14868</th>\n      <td>2022</td>\n      <td>2022-0356-ZAF</td>\n      <td>South Africa</td>\n      <td>ZAF</td>\n      <td>Kleinvlei, Ravensmead, Goodwood, Strand, Bonte...</td>\n      <td>Hydrological</td>\n      <td>Flood</td>\n      <td>Uncategorized_Flood</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>Km2</td>\n      <td>2022</td>\n      <td>2022</td>\n      <td>Africa</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>14869</th>\n      <td>2022</td>\n      <td>2022-0209-COD</td>\n      <td>Congo (the Democratic Republic of the)</td>\n      <td>COD</td>\n      <td>NaN</td>\n      <td>Meteorological</td>\n      <td>Storm</td>\n      <td>Convective storm</td>\n      <td>26.0</td>\n      <td>NaN</td>\n      <td>Kph</td>\n      <td>2022</td>\n      <td>2022</td>\n      <td>Africa</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>14870</th>\n      <td>2022</td>\n      <td>2022-0650-SSD</td>\n      <td>South Sudan</td>\n      <td>SSD</td>\n      <td>Western Equatoria, Northern Bahr el Ghazal, Wa...</td>\n      <td>Hydrological</td>\n      <td>Flood</td>\n      <td>Uncategorized_Flood</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>Km2</td>\n      <td>2022</td>\n      <td>2022</td>\n      <td>Africa</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>14871 rows × 15 columns</p>\n</div>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Save File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Todo: Remove later\n",
    "dis_all.to_csv(DIS_PROCESSED_ALL_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "   year         dis_no country_name country_code                   location  \\\n0  1900  1900-9002-CPV   Cabo Verde          CPV                Countrywide   \n1  1900  1900-9001-IND        India          IND                     Bengal   \n2  1902  1902-0012-GTM    Guatemala          GTM  Quezaltenango, San Marcos   \n\n         subgroup        type          subtype     deaths  dis_mag_value  \\\n0  Climatological     Drought          Drought    11000.0            NaN   \n1  Climatological     Drought          Drought  1250000.0            NaN   \n2     Geophysical  Earthquake  Ground movement     2000.0            8.0   \n\n  dis_mag_scale  start_year  end_year region_name  region_code  \n0           Km2        1900      1900      Africa          2.0  \n1           Km2        1900      1900        Asia        142.0  \n2       Richter        1902      1902    Americas         19.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>dis_no</th>\n      <th>country_name</th>\n      <th>country_code</th>\n      <th>location</th>\n      <th>subgroup</th>\n      <th>type</th>\n      <th>subtype</th>\n      <th>deaths</th>\n      <th>dis_mag_value</th>\n      <th>dis_mag_scale</th>\n      <th>start_year</th>\n      <th>end_year</th>\n      <th>region_name</th>\n      <th>region_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1900</td>\n      <td>1900-9002-CPV</td>\n      <td>Cabo Verde</td>\n      <td>CPV</td>\n      <td>Countrywide</td>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>Drought</td>\n      <td>11000.0</td>\n      <td>NaN</td>\n      <td>Km2</td>\n      <td>1900</td>\n      <td>1900</td>\n      <td>Africa</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1900</td>\n      <td>1900-9001-IND</td>\n      <td>India</td>\n      <td>IND</td>\n      <td>Bengal</td>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>Drought</td>\n      <td>1250000.0</td>\n      <td>NaN</td>\n      <td>Km2</td>\n      <td>1900</td>\n      <td>1900</td>\n      <td>Asia</td>\n      <td>142.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1902</td>\n      <td>1902-0012-GTM</td>\n      <td>Guatemala</td>\n      <td>GTM</td>\n      <td>Quezaltenango, San Marcos</td>\n      <td>Geophysical</td>\n      <td>Earthquake</td>\n      <td>Ground movement</td>\n      <td>2000.0</td>\n      <td>8.0</td>\n      <td>Richter</td>\n      <td>1902</td>\n      <td>1902</td>\n      <td>Americas</td>\n      <td>19.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2 Population Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nPopulation data\\n\\nThe population dataset by gapminder is a dataset containing information about the worlds population from 1800 until 2100.\\nThe data is a composition from different sources. There a two main sources - a dataset by Angus Maddison and the CLIO Infra Project -  and the World Population Prospects (WPP) provided by the UN.\\nThe dataset by Angus Maddison provides the data for the years 1800 - 1950. Population data after 1950 was taken from the WPP dataset.\\nAdditional sources were used, to fill missing values for years or regions. A details list of sources can be found in the documentation. (https://www.gapminder.org/data/documentation/gd003/)\\nThe primary data from the sources originates from census, informal census, indirect estimate and arbitrary guesses.\\n\\nModifications and Estimations:\\n- Summations of parts\\n- Larger area minus non-included parts\\n- Geographical interpolation\\n- Geographical extrapolation\\n- Temporal interpolation\\n- Temporal extrapolation\\n- Adjustments for under-enumeration\\n- Recalculated to fit present borders\\n\\nSince the data for the year 1950 of the two main datasets did not match for every country, small adjustments and smoothing were applied.\\n\\nPreprocessing:\\nFor our purposes, the subdatasets \"data-for-world-by-year\", \"data-for-regions-by-year\" and \"data-for-countries-by-year\" are relevant.\\nWe consider the period from 1900 until 2021 and remove redundant rows. We do this because the data after 2021 are based on estimated values.\\n\\nThere are no missing values.\\nSince the regions of the dataset did not match with the regions of our other datasets, we chose to compute the regions-population according to the regions from the un-country-codes.csv file.\\n\\nSubdataset:\\n    population-global.csv\\n        Columns: year, population\\n\\n    population-region.csv\\n        Columns: region_code, region_name, year, population\\n\\n    population-country.csv\\n        Columns: country_code, country_name, year, population\\n'"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Population data\n",
    "\n",
    "The population dataset by gapminder is a dataset containing information about the worlds population from 1800 until 2100.\n",
    "The data is a composition from different sources. There a two main sources - a dataset by Angus Maddison and the CLIO Infra Project -  and the World Population Prospects (WPP) provided by the UN.\n",
    "The dataset by Angus Maddison provides the data for the years 1800 - 1950. Population data after 1950 was taken from the WPP dataset.\n",
    "Additional sources were used, to fill missing values for years or regions. A details list of sources can be found in the documentation. (https://www.gapminder.org/data/documentation/gd003/)\n",
    "The primary data from the sources originates from census, informal census, indirect estimate and arbitrary guesses.\n",
    "\n",
    "Modifications and Estimations:\n",
    "- Summations of parts\n",
    "- Larger area minus non-included parts\n",
    "- Geographical interpolation\n",
    "- Geographical extrapolation\n",
    "- Temporal interpolation\n",
    "- Temporal extrapolation\n",
    "- Adjustments for under-enumeration\n",
    "- Recalculated to fit present borders\n",
    "\n",
    "Since the data for the year 1950 of the two main datasets did not match for every country, small adjustments and smoothing were applied.\n",
    "\n",
    "Preprocessing:\n",
    "For our purposes, the subdatasets \"data-for-world-by-year\", \"data-for-regions-by-year\" and \"data-for-countries-by-year\" are relevant.\n",
    "We consider the period from 1900 until 2021 and remove redundant rows. We do this because the data after 2021 are based on estimated values.\n",
    "\n",
    "There are no missing values.\n",
    "Since the regions of the dataset did not match with the regions of our other datasets, we chose to compute the regions-population according to the regions from the un-country-codes.csv file.\n",
    "\n",
    "Subdataset:\n",
    "    population-global.csv\n",
    "        Columns: year, population\n",
    "\n",
    "    population-region.csv\n",
    "        Columns: region_code, region_name, year, population\n",
    "\n",
    "    population-country.csv\n",
    "        Columns: country_code, country_name, year, population\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Preprocess global data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data for world population\n",
      "     geo   name    time   Population\n",
      "0  world  world  1800.0  985083734.9\n",
      "1  world  world  1801.0  988518009.0\n",
      "2  world  world  1802.0  991993182.0\n",
      "extract population data from 1900 until now\n",
      "       year  population\n",
      "100  1900.0  1627123965\n",
      "101  1901.0  1639684222\n",
      "102  1902.0  1652898195\n",
      "103  1903.0  1666760587\n",
      "104  1904.0  1680799825\n",
      "..      ...         ...\n",
      "217  2017.0 -2147483648\n",
      "218  2018.0 -2147483648\n",
      "219  2019.0 -2147483648\n",
      "220  2020.0 -2147483648\n",
      "221  2021.0 -2147483648\n",
      "\n",
      "[122 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukas\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:5039: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    }
   ],
   "source": [
    "print('raw data for world population')\n",
    "print(pop_global_df.head(3))\n",
    "\n",
    "# remove unnecessary columns\n",
    "pop_global_df = pop_global_df[['time', 'Population']]\n",
    "pop_global_df.rename(columns={'Population': 'population', 'time': 'year'}, inplace=True)\n",
    "pop_global_df.set_index('year')\n",
    "\n",
    "print('extract population data from 1900 until now')\n",
    "pop_global_df = pop_global_df[pop_global_df['year'] >= 1900]\n",
    "pop_global_df = pop_global_df[pop_global_df['year'] <= 2021]\n",
    "pop_global_df['population'] = pop_global_df['population'].astype(int)\n",
    "print(pop_global_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Prepare dataset for countries and regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     country_code country_name    year  population  region_code region_name\n",
      "8885          HOS     Holy See  2001.0         NaN        150.0      Europe\n",
      "8886          HOS     Holy See  2002.0         NaN        150.0      Europe\n",
      "8887          HOS     Holy See  2003.0         NaN        150.0      Europe\n",
      "8888          HOS     Holy See  2004.0         NaN        150.0      Europe\n",
      "8889          HOS     Holy See  2005.0         NaN        150.0      Europe\n",
      "8890          HOS     Holy See  2006.0         NaN        150.0      Europe\n",
      "8891          HOS     Holy See  2007.0         NaN        150.0      Europe\n",
      "8892          HOS     Holy See  2008.0         NaN        150.0      Europe\n",
      "8893          HOS     Holy See  2009.0         NaN        150.0      Europe\n",
      "8894          HOS     Holy See  2010.0         NaN        150.0      Europe\n",
      "8895          HOS     Holy See  2011.0         NaN        150.0      Europe\n",
      "8896          HOS     Holy See  2012.0         NaN        150.0      Europe\n",
      "8897          HOS     Holy See  2013.0         NaN        150.0      Europe\n",
      "8898          HOS     Holy See  2014.0         NaN        150.0      Europe\n",
      "8899          HOS     Holy See  2015.0         NaN        150.0      Europe\n",
      "8900          HOS     Holy See  2016.0         NaN        150.0      Europe\n",
      "8901          HOS     Holy See  2017.0         NaN        150.0      Europe\n",
      "8902          HOS     Holy See  2018.0         NaN        150.0      Europe\n",
      "8903          HOS     Holy See  2019.0         NaN        150.0      Europe\n",
      "8904          HOS     Holy See  2020.0         NaN        150.0      Europe\n",
      "8905          HOS     Holy See  2021.0         NaN        150.0      Europe\n"
     ]
    }
   ],
   "source": [
    "un_country_codes = un_country_codes[['Region Code', 'Region Name', 'ISO-alpha3 Code']]\n",
    "\n",
    "# extract population data from 1900 until now\n",
    "pop_country_df = pop_country_df[pop_country_df['time'] >= 1900]\n",
    "pop_country_df = pop_country_df[pop_country_df['time'] <= 2021]\n",
    "\n",
    "# format column that will be joined\n",
    "pop_country_df['geo'] = pop_country_df['geo'].str.upper()\n",
    "\n",
    "# merge country-population and un-country-codes\n",
    "pop_country_region_df = pd.merge(pop_country_df, un_country_codes, how='left', left_on='geo', right_on='ISO-alpha3 Code')\n",
    "\n",
    "pop_country_region_df.rename(columns={'Region Code': 'region_code', 'Region Name': 'region_name', 'time': 'year', 'Population': 'population', 'geo': 'country_code', 'name': 'country_name'}, inplace=True)\n",
    "\n",
    "# add missing region code to holy see and taiwan\n",
    "pop_country_region_df.loc[pop_country_region_df['country_code'] == 'HOS', 'region_code'] = 150\n",
    "pop_country_region_df.loc[pop_country_region_df['country_code'] == 'HOS', 'region_name'] = 'Europe'\n",
    "\n",
    "pop_country_region_df.loc[pop_country_region_df['country_code'] == 'TWN', 'region_code'] = 142\n",
    "pop_country_region_df.loc[pop_country_region_df['country_code'] == 'TWN', 'region_name'] = 'Asia'\n",
    "\n",
    "# remove unused column 'ISO-alpha3 Code'\n",
    "pop_country_region_df.drop('ISO-alpha3 Code', axis=1, inplace=True)\n",
    "\n",
    "# check if we worked correctly\n",
    "print(pop_country_region_df[pop_country_region_df.isnull().any(axis=1)])\n",
    "\n",
    "# the remaining missing values are population data for the country 'holy see' since the missing population is in the range < 1000 we, accept the missing values and calculate with the latest documented value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Save population regions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns_regions = ['region_code', 'region_name', 'country_code', 'country_name', 'year', 'population']\n",
    "\n",
    "# select columns\n",
    "pop_regions_df = pop_country_region_df[columns_regions]\n",
    "pop_regions_df = pop_regions_df.groupby(['region_code', 'region_name', 'year'], as_index=False)['population'].sum()\n",
    "pop_regions_df['population'] = pop_regions_df['population'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Save population countries dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns_country = ['country_code', 'country_name', 'year', 'population']\n",
    "\n",
    "# select columns\n",
    "pop_country_df = pop_country_region_df[columns_country]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Compare calculated region population to gapminder dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total population from the year 2021\n",
      "Gapminder 7898737625.0\n",
      "Calculated 1057364421\n",
      "\n",
      "\n",
      "Gapminder region population\n",
      "          name    Population\n",
      "221     africa  1.391823e+09\n",
      "522       asia  4.634610e+09\n",
      "823     europe  8.460503e+08\n",
      "1124  americas  1.026254e+09\n",
      "\n",
      "\n",
      "Calculated region population\n",
      "    region_name  population\n",
      "121      Africa  1391823318\n",
      "243     Oceania    43602426\n",
      "365    Americas  1026253579\n",
      "487        Asia -2147483648\n",
      "609      Europe   743168746\n"
     ]
    }
   ],
   "source": [
    "regions_population_2021_gapminder = pop_region_gapminder[pop_region_gapminder['time'] == 2021]\n",
    "total_population_gapminder = regions_population_2021_gapminder['Population'].sum()\n",
    "\n",
    "regions_population_2021_calc = pop_regions_df[pop_regions_df['year'] == 2021]\n",
    "total_population_calc = regions_population_2021_calc['population'].sum()\n",
    "\n",
    "print('Total population from the year 2021')\n",
    "print('Gapminder ' + str(total_population_gapminder))\n",
    "print('Calculated ' + str(total_population_calc))\n",
    "\n",
    "print('\\n')\n",
    "print('Gapminder region population')\n",
    "print(regions_population_2021_gapminder[['name', 'Population']])\n",
    "\n",
    "print('\\n')\n",
    "print('Calculated region population')\n",
    "print(regions_population_2021_calc[['region_name', 'population']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As we can see, we get the same results for the total populations in the year 2021.\n",
    "If we take a look at the region population, we see that the calculated population of america and africa shows no difference to the gapminder-dataset population. For the regions asia and europe, we get different results. Furthermore, the calculated region oceania does not exist in the gapminder dataset. Since the documentation of the gapminder dataset does not include the country-to-region assignment, we cannot compare the differences between our calculation and gapminder. However, since the overall score is equal, we can argue that those differences occur tue to different country-to-region assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# Todo: Remove later\n",
    "pop_global_df.to_csv(POP_PROCESSED_GLOBAL_FILE, sep=';', index=False, header=True)\n",
    "#store in csv file\n",
    "pop_country_df.to_csv(POP_PROCESSED_COUNTRY_FILE, sep=';', index=False, header=True)\n",
    "#store in csv file\n",
    "pop_regions_df.to_csv(POP_PROCESSED_REGION_FILE, sep=';', index=False, header=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "       year  population\n100  1900.0  1627123965\n101  1901.0  1639684222\n102  1902.0  1652898195",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>100</th>\n      <td>1900.0</td>\n      <td>1627123965</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>1901.0</td>\n      <td>1639684222</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>1902.0</td>\n      <td>1652898195</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_global_df.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "  country_code country_name    year  population\n0          AFG  Afghanistan  1900.0   4707744.0\n1          AFG  Afghanistan  1901.0   4751177.0\n2          AFG  Afghanistan  1902.0   4802500.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country_code</th>\n      <th>country_name</th>\n      <th>year</th>\n      <th>population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1900.0</td>\n      <td>4707744.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1901.0</td>\n      <td>4751177.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1902.0</td>\n      <td>4802500.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_country_df.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "   region_code region_name    year  population\n0          2.0      Africa  1900.0   138578556\n1          2.0      Africa  1901.0   139018147\n2          2.0      Africa  1902.0   139489077",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>region_code</th>\n      <th>region_name</th>\n      <th>year</th>\n      <th>population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1900.0</td>\n      <td>138578556</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1901.0</td>\n      <td>139018147</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1902.0</td>\n      <td>139489077</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_regions_df.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3 Temperature Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Load temp-land-country data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "temp_countries = temperature_countries_with_iso"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this section we load Berkleys land-temperature data for each country from the corresponding .txt files and combine them into a single dataframe. We also change the naming of the countries according to the naming standard of the UN and add their ISO3-Country code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  country_code country_name  year month  monthly_anomaly  monthly_unc  \\\n0          AFG  Afghanistan  1848     5           -0.297        2.037   \n1          AFG  Afghanistan  1848     6           -0.796        2.136   \n2          AFG  Afghanistan  1848     7           -0.113        1.937   \n3          AFG  Afghanistan  1848     8           -0.462        1.937   \n4          AFG  Afghanistan  1848     9           -1.272        1.865   \n\n   annual_anomaly  annual_unc  five_year_anomaly  five_year_unc  \\\n0             NaN         NaN                NaN            NaN   \n1             NaN         NaN                NaN            NaN   \n2          -0.777       0.639                NaN            NaN   \n3          -0.743       0.644                NaN            NaN   \n4          -0.676       0.669                NaN            NaN   \n\n   ten_year_anomaly  ten_year_unc  twenty_year_anomaly  twenty_year_unc  \n0               NaN           NaN                  NaN              NaN  \n1               NaN           NaN                  NaN              NaN  \n2               NaN           NaN                  NaN              NaN  \n3               NaN           NaN                  NaN              NaN  \n4               NaN           NaN                  NaN              NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country_code</th>\n      <th>country_name</th>\n      <th>year</th>\n      <th>month</th>\n      <th>monthly_anomaly</th>\n      <th>monthly_unc</th>\n      <th>annual_anomaly</th>\n      <th>annual_unc</th>\n      <th>five_year_anomaly</th>\n      <th>five_year_unc</th>\n      <th>ten_year_anomaly</th>\n      <th>ten_year_unc</th>\n      <th>twenty_year_anomaly</th>\n      <th>twenty_year_unc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1848</td>\n      <td>5</td>\n      <td>-0.297</td>\n      <td>2.037</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1848</td>\n      <td>6</td>\n      <td>-0.796</td>\n      <td>2.136</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1848</td>\n      <td>7</td>\n      <td>-0.113</td>\n      <td>1.937</td>\n      <td>-0.777</td>\n      <td>0.639</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1848</td>\n      <td>8</td>\n      <td>-0.462</td>\n      <td>1.937</td>\n      <td>-0.743</td>\n      <td>0.644</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1848</td>\n      <td>9</td>\n      <td>-1.272</td>\n      <td>1.865</td>\n      <td>-0.676</td>\n      <td>0.669</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_land_country_column_names = [\"country_code\", \"country_name\", \"year\", \"month\", \"monthly_anomaly\", \"monthly_unc\", \"annual_anomaly\", \"annual_unc\", \"five_year_anomaly\", \"five_year_unc\", \"ten_year_anomaly\", \"ten_year_unc\", \"twenty_year_anomaly\", \"twenty_year_unc\"]\n",
    "temp_country = pd.DataFrame(columns=temp_land_country_column_names)\n",
    "\n",
    "temp_country_names = temp_countries['Country']\n",
    "\n",
    "# Todo when combining use the dictionary in the data-integration notebook\n",
    "new_country_names = {\n",
    "    \"Denmark (Europe)\": \"Denmark\",\n",
    "    \"France (Europe)\": \"France\",\n",
    "    \"Netherlands (Europe)\": \"Netherlands\",\n",
    "    \"United Kingdom (Europe)\": \"United Kingdom\",\n",
    "    \"Åland\": \"Åland Islands\",\n",
    "    \"Czech Republic\": \"Czechia\",\n",
    "    \"Turkey\": \"Türkiye\",\n",
    "    \"Svalbard and Jan Mayen\": \"Svalbard and Jan Mayen Islands\",\n",
    "    \"Cape Verde\": \"Cabo Verde\",\n",
    "    \"Turks and Caicas Islands\": \"Turks and Caicos Islands\",\n",
    "    \"Swaziland\": \"Eswatini\",\n",
    "    \"Macedonia\": \"North Macedonia\",\n",
    "    \"Côte d'Ivoire\": \"Côte d’Ivoire\",\n",
    "    \"Federated States of Micronesia\": \"Micronesia (Federated States of)\",\n",
    "    \"South Georgia and the South Sandwich Isla\": \"South Georgia and the South Sandwich Islands\",\n",
    "    \"Bonaire, Saint Eustatius and Saba\": \"Bonaire, Sint Eustatius and Saba\",\n",
    "    \"Congo (Democratic Republic of the)\": \"Democratic Republic of the Congo\",\n",
    "    \"South Korea\": \"Korea, South\",\n",
    "    \"North Korea\": \"Korea, North\",\n",
    "    \"Palestina\": \"State of Palestine\"\n",
    "}\n",
    "\n",
    "def swap_keys_values(d):\n",
    "    return {v: k for k, v in d.items()}\n",
    "\n",
    "map_country_to_filename = swap_keys_values(new_country_names)\n",
    "\n",
    "for temp_country_name in temp_country_names:\n",
    "    temp_country_file_name = temp_country_name\n",
    "    if temp_country_name in map_country_to_filename:\n",
    "        temp_country_file_name = map_country_to_filename[temp_country_name]\n",
    "    path_temp_country_txt_file = TEMP_RAW_FOLDER + PATH_COUNTRIES_LAND_FOLDER + temp_country_file_name + '.txt'\n",
    "    temp_land_one_country = pd.read_csv(path_temp_country_txt_file, comment=\"%\", header=None, delim_whitespace=True)\n",
    "    country_iso_name = temp_countries.loc[temp_countries['Country'] == temp_country_name]['ISO-alpha3'].iloc[0]\n",
    "\n",
    "    temp_land_one_country.insert(0, 'country_code', country_iso_name)\n",
    "    temp_land_one_country.insert(1, 'country_name', temp_country_name)\n",
    "\n",
    "    temp_land_one_country.columns=temp_land_country_column_names\n",
    "\n",
    "    temp_country = pd.concat([temp_country, temp_land_one_country])\n",
    "\n",
    "temp_country.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Create temp-land-region data from temp-land-country data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Berkleys offers a dataset with regional temperature anomly data, however because we also use datasets for natural disasters and population we have to ensure that regions in all datasets are including the same countries. The regions defined for Berkleys regional temp anomaly datasets differ from the definition of regions in the UN dataset we use, therefore we decided to calculate regional temp anomalies based on the berkleys temp anomaly data for individual countries for regions based on the region data of the UN dataset.\n",
    "\n",
    "Temperature anomalies tend to be similar across large regions geographical regions, even if the absolute temperature of two different measuring points differs for the same time period, their anomalies tend to be quite similar. To create regional temperature anomaly data we calculate the mean of temperature anomaly measures for all countries in a given region on a monthly basis. Source: https://data.giss.nasa.gov/gistemp/faq/#q101\n",
    "\n",
    "When computing a region it is possible that some countries are not included, because it is possible that countries started reporting earlier than others or stopped reporting for some time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_region_anomaly(temp_land_country_regions):\n",
    "    temp_return_land_region = temp_land_country_regions.groupby(['region_code', 'region_name', 'year', 'month'], as_index=False)['monthly_anomaly'].mean()\n",
    "    temp_return_land_region.rename({ 'monthly_anomaly': 'temperature_anomaly'}, axis=1,inplace=True)\n",
    "    return temp_return_land_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   region_code region_name  year  month  temperature_anomaly\n0          2.0      Africa  1787      1               0.1520\n1          2.0      Africa  1787      2              -0.3142\n2          2.0      Africa  1787      3              -0.7876\n3          2.0      Africa  1787      4              -0.9146\n4          2.0      Africa  1787      5              -0.5570",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>region_code</th>\n      <th>region_name</th>\n      <th>year</th>\n      <th>month</th>\n      <th>temperature_anomaly</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1787</td>\n      <td>1</td>\n      <td>0.1520</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1787</td>\n      <td>2</td>\n      <td>-0.3142</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1787</td>\n      <td>3</td>\n      <td>-0.7876</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1787</td>\n      <td>4</td>\n      <td>-0.9146</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1787</td>\n      <td>5</td>\n      <td>-0.5570</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Todo: only load UN country codes once after combining notebook\n",
    "# load and format un-country-codes data\n",
    "\n",
    "un_country_codes = un_country_codes[['Region Code', 'Region Name', 'ISO-alpha3 Code']]\n",
    "un_country_codes.columns=['region_code', 'region_name', 'country_code']\n",
    "\n",
    "# Join UN country codes to temp data\n",
    "# exclude temp data for antarctica when calculating regional temp data\n",
    "temp_land_country_no_antarctica = temp_country[temp_country.country_name != 'Antarctica']\n",
    "temp_land_country_regions = pd.merge(temp_land_country_no_antarctica, un_country_codes, on='country_code', how='left')\n",
    "temp_land_country_regions.loc[temp_land_country_regions['country_name']=='Taiwan','region_name'] = 'Asia'\n",
    "temp_land_country_regions.loc[temp_land_country_regions['country_name']=='Taiwan','region_code'] = 142\n",
    "\n",
    "# calculate anomaly data for regions\n",
    "temp_region = calc_region_anomaly(temp_land_country_regions)\n",
    "\n",
    "temp_region.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load berkleys regional data\n",
    "\n",
    "We load Berkleys regional temperature anomaly data to compare it to the regional temperature anomaly data we calculated from the country temperature anomaly data and UN regions combined.\n",
    "\n",
    "We combine Berkleys North and South America region to one Region 'Americas' to be able to compare it to the combined 'Americas' region of the UN data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  region_name  year  month  temperature_anomaly\n0      Africa  1880      8               -0.181\n1      Africa  1880      9               -0.389\n2      Africa  1880     10               -0.274\n3      Africa  1880     11               -0.169\n4      Africa  1880     12               -0.396",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>region_name</th>\n      <th>year</th>\n      <th>month</th>\n      <th>temperature_anomaly</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Africa</td>\n      <td>1880</td>\n      <td>8</td>\n      <td>-0.181</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Africa</td>\n      <td>1880</td>\n      <td>9</td>\n      <td>-0.389</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Africa</td>\n      <td>1880</td>\n      <td>10</td>\n      <td>-0.274</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Africa</td>\n      <td>1880</td>\n      <td>11</td>\n      <td>-0.169</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Africa</td>\n      <td>1880</td>\n      <td>12</td>\n      <td>-0.396</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Todo: make this code segment prettier; reuse other code segments\n",
    "\n",
    "temp_land_country_column_names = [\"region_name\", \"year\", \"month\", \"monthly_anomaly\", \"monthly_unc\", \"annual_anomaly\", \"annual_unc\", \"five_year_anomaly\", \"five_year_unc\", \"ten_year_anomaly\", \"ten_year_unc\", \"twenty_year_anomaly\", \"twenty_year_unc\"]\n",
    "temp_berkley_regions = ['Africa', 'Asia', 'Europe', 'North America', 'Oceania', 'South America']\n",
    "temp_berkleys_region = pd.DataFrame(columns=temp_land_country_column_names)\n",
    "\n",
    "for temp_berkley_region in temp_berkley_regions:\n",
    "    region_file_name = temp_berkley_region\n",
    "    path = TEMP_RAW_FOLDER + PATH_REGIONS_LAND_FOLDER + region_file_name + '.txt'\n",
    "    one_region = pd.read_csv(path, comment=\"%\", header=None, delim_whitespace=True)\n",
    "    one_region.insert(0, 'region_name', temp_berkley_region)\n",
    "    one_region.columns=temp_land_country_column_names\n",
    "    temp_berkleys_region = pd.concat([temp_berkleys_region, one_region])\n",
    "\n",
    "temp_berkleys_region.loc[(temp_berkleys_region['region_name'] == 'South America') | (temp_berkleys_region['region_name'] == 'North America'), 'region_name'] = 'Americas'\n",
    "temp_berkleys_region = temp_berkleys_region[['region_name', 'year', 'month', 'monthly_anomaly']]\n",
    "temp_berkleys_region = temp_berkleys_region.groupby(by=['region_name', 'year', 'month'])['monthly_anomaly'].mean()\n",
    "temp_berkleys_region = temp_berkleys_region.reset_index()\n",
    "temp_berkleys_region = temp_berkleys_region.rename(columns={'monthly_anomaly': 'temperature_anomaly'})\n",
    "\n",
    "temp_berkleys_region.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Load temp-land-ocean-global data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Two versions exist that treat temperature anomalies at locations with sea ice:\n",
    "1. Anomalies are extrapolated from land-surface air temperature anomalies.\n",
    "2. Anomalies are extrapolated from sea-surface water temperature anomalies (usually collected from open water areas on the periphery of the sea ice).\n",
    "\n",
    "We choose to use the air temperature version based on Berkleys remark:\n",
    "\"We believe that the use of air temperatures above sea ice provides a more natural means of describing changes in Earth's surface temperature.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   year  month  monthly_anomaly  monthly_unc  annual_anomaly  annual_unc  \\\n0  1850      1           -0.736        0.389             NaN         NaN   \n1  1850      2           -0.202        0.526             NaN         NaN   \n2  1850      3           -0.363        0.333             NaN         NaN   \n3  1850      4           -0.589        0.334             NaN         NaN   \n4  1850      5           -0.614        0.217             NaN         NaN   \n\n   five_year_anomaly  five_year_unc  ten_year_anomaly  ten_year_unc  \\\n0                NaN            NaN               NaN           NaN   \n1                NaN            NaN               NaN           NaN   \n2                NaN            NaN               NaN           NaN   \n3                NaN            NaN               NaN           NaN   \n4                NaN            NaN               NaN           NaN   \n\n   twenty_year_anomaly  twenty_year_unc  \n0                  NaN              NaN  \n1                  NaN              NaN  \n2                  NaN              NaN  \n3                  NaN              NaN  \n4                  NaN              NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>month</th>\n      <th>monthly_anomaly</th>\n      <th>monthly_unc</th>\n      <th>annual_anomaly</th>\n      <th>annual_unc</th>\n      <th>five_year_anomaly</th>\n      <th>five_year_unc</th>\n      <th>ten_year_anomaly</th>\n      <th>ten_year_unc</th>\n      <th>twenty_year_anomaly</th>\n      <th>twenty_year_unc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1850</td>\n      <td>1</td>\n      <td>-0.736</td>\n      <td>0.389</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1850</td>\n      <td>2</td>\n      <td>-0.202</td>\n      <td>0.526</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1850</td>\n      <td>3</td>\n      <td>-0.363</td>\n      <td>0.333</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1850</td>\n      <td>4</td>\n      <td>-0.589</td>\n      <td>0.334</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1850</td>\n      <td>5</td>\n      <td>-0.614</td>\n      <td>0.217</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only include the dataset where anomalies are extrapolated from land-surface air temperature anomalies.\n",
    "temp_global = pd.read_csv(TEMP_RAW_FOLDER + TEMP_GLOBAL_LAND_OCEAN_FILE, comment=\"%\", header=None, delim_whitespace=True, engine='python', skipfooter=2079)\n",
    "temp_global.columns = [\"year\", \"month\", \"monthly_anomaly\", \"monthly_unc\", \"annual_anomaly\", \"annual_unc\", \"five_year_anomaly\", \"five_year_unc\", \"ten_year_anomaly\", \"ten_year_unc\", \"twenty_year_anomaly\", \"twenty_year_unc\"]\n",
    "temp_global.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Format the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We only keep entries after the year 1900 because our research questions focus on the past 100 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cut_before_1900(temp_data):\n",
    "    return temp_data[temp_data['year'] >= 1900]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We only keep monthly anomaly data and rename the column to temperature_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def keep_monthly_anomalies(temp_data):\n",
    "    drop_labels = [\"monthly_unc\", \"annual_anomaly\", \"annual_unc\", \"five_year_anomaly\", \"five_year_unc\", \"ten_year_anomaly\", \"ten_year_unc\", \"twenty_year_anomaly\", \"twenty_year_unc\"]\n",
    "    temp_data = temp_data.drop(labels=drop_labels, axis=1)\n",
    "    return temp_data.rename(columns={'monthly_anomaly': 'temperature_anomaly'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Format temp-land-country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Apply formatting functions for only keeping monthly anomalies and cut values before 1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_country = cut_before_1900(temp_country)\n",
    "temp_country = keep_monthly_anomalies(temp_country)\n",
    "temp_country = temp_country.groupby([\"year\",\"country_code\"]).agg({\"temperature_anomaly\":\"mean\",\"country_name\":\"first\"})\n",
    "temp_country.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Format temp-land-region and berkleys regional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Apply formatting function to cut values before 1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_region = cut_before_1900(temp_region)\n",
    "temp_berkleys_region = cut_before_1900(temp_berkleys_region)\n",
    "temp_region = temp_region.groupby([\"year\",\"region_code\"]).agg({\"temperature_anomaly\":\"mean\",\"region_name\":\"first\"})\n",
    "temp_region.reset_index(inplace=True)\n",
    "temp_berkleys_region.reset_index(inplace=True)\n",
    "temp_berkleys_region = temp_berkleys_region.groupby([\"year\", \"region_name\"]).agg({\"temperature_anomaly\":\"mean\"})\n",
    "temp_berkleys_region.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Format temp-land-ocean-global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Apply formatting functions for only keeping monthly anomalies and cut values before 1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_global = cut_before_1900(temp_global)\n",
    "temp_global = keep_monthly_anomalies(temp_global)\n",
    "temp_global = temp_global.groupby([\"year\"])[\"temperature_anomaly\"].mean()\n",
    "temp_global = temp_global.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Compare berkleys and our regional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def temp_calc_region_corr(temp_single_region):\n",
    "    temp_land_single_region = temp_region[(temp_region['region_name'] == temp_single_region)].set_index(['year'])\n",
    "    temp_berkleys_land_single_region = temp_berkleys_region[(temp_berkleys_region['region_name'] == temp_single_region)].set_index(['year'])\n",
    "\n",
    "    comp_temp_land_africa_joined = temp_berkleys_land_single_region.join(temp_land_single_region, on=['year'], lsuffix='_berkleys', rsuffix='_self')\n",
    "\n",
    "    temp_land_region_corr = comp_temp_land_africa_joined['temperature_anomaly_berkleys'].corr(comp_temp_land_africa_joined['temperature_anomaly_self'])\n",
    "\n",
    "    print('Correlation for region ' + temp_single_region + ' ' + str(temp_land_region_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Correlation of temperature anomalies between our and berkleys regional temp data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation for region Africa 0.9940777030851426\n",
      "Correlation for region Asia 0.8705964680397756\n",
      "Correlation for region Europe 0.9932100755076412\n",
      "Correlation for region Americas 0.94302727003253\n",
      "Correlation for region Oceania 0.8531672929606253\n"
     ]
    }
   ],
   "source": [
    "temp_berkley_regions = ['Africa', 'Asia', 'Europe', 'Americas', 'Oceania']\n",
    "\n",
    "for temp_berkley_region in temp_berkley_regions:\n",
    "    temp_calc_region_corr(temp_berkley_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Check temperature_anomaly columns for NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: 5\n",
      "Region: 0\n",
      "Global: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Country: \" + str(temp_country['temperature_anomaly'].isna().sum()))\n",
    "print(\"Region: \" + str(temp_region['temperature_anomaly'].isna().sum()))\n",
    "print(\"Global: \" + str(temp_global['temperature_anomaly'].isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Handle NaN values in temp_land_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "country_code\nGUM    2\nPNG    2\nSLB    1\ndtype: int64"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_country[temp_country['temperature_anomaly'].isna()].groupby('country_code').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_country['temperature_anomaly'] = temp_country['temperature_anomaly'].interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_country[temp_country['temperature_anomaly'].isna()==True].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Save the data as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Todo: Remove later\n",
    "temp_country.to_csv(TEMP_PROCESSED_FOLDER + 'temp-land-country.csv', index=False)\n",
    "temp_region.to_csv(TEMP_PROCESSED_FOLDER + 'temp-land-region.csv', index=False)\n",
    "temp_global.to_csv(TEMP_PROCESSED_FOLDER + 'temp-land-ocean-global.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "   year country_code  temperature_anomaly country_name\n0  1900          ABW            -0.061750        Aruba\n1  1900          AFG            -0.427250  Afghanistan\n2  1900          AGO             0.146167       Angola",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>country_code</th>\n      <th>temperature_anomaly</th>\n      <th>country_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1900</td>\n      <td>ABW</td>\n      <td>-0.061750</td>\n      <td>Aruba</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1900</td>\n      <td>AFG</td>\n      <td>-0.427250</td>\n      <td>Afghanistan</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1900</td>\n      <td>AGO</td>\n      <td>0.146167</td>\n      <td>Angola</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_country.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "   year  region_code  temperature_anomaly region_name\n0  1900          2.0            -0.016705      Africa\n1  1900          9.0            -0.307362     Oceania\n2  1900         19.0            -0.160803    Americas",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>region_code</th>\n      <th>temperature_anomaly</th>\n      <th>region_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1900</td>\n      <td>2.0</td>\n      <td>-0.016705</td>\n      <td>Africa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1900</td>\n      <td>9.0</td>\n      <td>-0.307362</td>\n      <td>Oceania</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1900</td>\n      <td>19.0</td>\n      <td>-0.160803</td>\n      <td>Americas</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_region.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "   year  temperature_anomaly\n0  1900            -0.125167\n1  1901            -0.199333\n2  1902            -0.362167",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>temperature_anomaly</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1900</td>\n      <td>-0.125167</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1901</td>\n      <td>-0.199333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1902</td>\n      <td>-0.362167</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_global.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
