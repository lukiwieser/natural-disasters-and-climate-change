{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Climate Change and Deaths by Natural Disasters\n",
    "\n",
    "*Sebastian Fürndraht, Hannes Rokitte, Paul Schmitt, Lukas Wieser*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Overview\n",
    "- Introduction\n",
    "    - Research Questions\n",
    "    - Used Datasets\n",
    "    - Requirements & Dependencies\n",
    "    - Constants\n",
    "    - Download Temperature Data\n",
    "- Data Integration\n",
    "    - ...\n",
    "- Prepare Datasets\n",
    "    - ...\n",
    "- Data Exploration\n",
    "    - ...\n",
    "- Conclusion\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Used Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Requirements & Dependencies\n",
    "\n",
    "This project was created using Python 3.9.\n",
    "The exact versions of the dependencies can be installed with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import requests\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Constants should be uppercase\n",
    "# Path variables should end with FOLDER or FILE\n",
    "RAW_TEMP_DATA_FOLDER = \"data/raw/temperature/\"\n",
    "COUNTRIES_LIST_FILE = \"temp-countries-list.csv\"\n",
    "\n",
    "DIS_RAW_FILE = Path('data/raw/disaster/emdat_public_2022_12_22_full.xlsx')\n",
    "DIS_PROCESSED_ALL_FILE = Path(\"data/processed/disaster/disaster-all.csv\")\n",
    "DIS_PROCESSED_FOLDER = \"data/processed/disaster\"\n",
    "\n",
    "TEMP_PROCESSED_FOLDER = \"data/processed/temperature\"\n",
    "POP_PROCESSED_FOLDER = \"data/processed/population\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Path(DIS_PROCESSED_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "Path(TEMP_PROCESSED_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "Path(POP_PROCESSED_FOLDER).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Download Temperature Data\n",
    "Automatically download the regional and country temperature data, so we don't have to download each file by ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries = pd.read_csv(\"data/raw/temperature/countries-list.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_regions = countries[\"Region\"].dropna().unique().tolist()\n",
    "temp_countries = countries[\"Country\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_temperature_countries(country_names: list[str]):\n",
    "    for country in country_names:\n",
    "        print(f\"downloading {country}\")\n",
    "        country_encoded = urllib.parse.quote(country.lower().replace(\" \", \"-\"),encoding='cp1252')\n",
    "        url = f\"http://berkeleyearth.lbl.gov/auto/Regional/TAVG/Text/{country_encoded}-TAVG-Trend.txt\"\n",
    "        response = requests.get(url)\n",
    "        data = response.text\n",
    "        with open(f'data/raw/temperature/countries-land/{country}.txt', 'w', encoding=\"utf-8\") as file:\n",
    "            file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_temperature_regions(region_names: list[str]):\n",
    "    for region in region_names:\n",
    "        print(f\"downloading {region}\")\n",
    "        region_encoded = urllib.parse.quote(region.lower().replace(\" \", \"-\"),encoding='cp1252')\n",
    "        url = f\"http://berkeleyearth.lbl.gov/auto/Regional/TAVG/Text/{region_encoded}-TAVG-Trend.txt\"\n",
    "        response = requests.get(url)\n",
    "        data = response.text\n",
    "        with open(f'data/raw/temperature/regions-land/{region}.txt', 'w', encoding=\"utf-8\") as file:\n",
    "            file.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Change the variable `should_download_temperature_data` to `True`, to download the temperature data of countries & regions. (This should not be necessary, since the data should already be downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "should_download_temperature_data = False\n",
    "if should_download_temperature_data:\n",
    "    download_temperature_countries(temp_countries)\n",
    "    download_temperature_regions(temp_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Data Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukas\\anaconda3\\lib\\site-packages\\openpyxl\\styles\\stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    }
   ],
   "source": [
    "disasters = pd.read_excel('data/raw/disaster/emdat_public_2022_12_22_full.xlsx', skiprows=6, sheet_name=\"emdat data\")\n",
    "\n",
    "temperature_countries = pd.read_csv(\"data/raw/temperature/countries-list.csv\", sep=\";\")\n",
    "population_by_country = pd.read_excel('data/raw/population/gapminder-population-v7.xlsx', sheet_name=\"data-for-countries-etc-by-year\")\n",
    "population_by_region = pd.read_excel('data/raw/population/gapminder-population-v7.xlsx', sheet_name=\"data-for-regions-by-year\")\n",
    "\n",
    "un_country_codes = pd.read_csv(\"data/raw/country-codes/un-country-codes.csv\", sep=\";\")\n",
    "cia_country_codes = pd.read_csv(\"data/raw/country-codes/cia-country-codes.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis = pd.read_excel(DIS_RAW_FILE, skiprows = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Preprocess CIA dataset (since it is a bit messy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        Entity GENC        ISO 3166 Stanag Internet Comment\n0  Afghanistan  AFG  AF | AFG | 004    AFG      .af       -\n1     Akrotiri  XQZ       - | - | -      -        -       -\n2      Albania  ALB  AL | ALB | 008    ALB      .al       -",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Entity</th>\n      <th>GENC</th>\n      <th>ISO 3166</th>\n      <th>Stanag</th>\n      <th>Internet</th>\n      <th>Comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>AFG</td>\n      <td>AF | AFG | 004</td>\n      <td>AFG</td>\n      <td>.af</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Akrotiri</td>\n      <td>XQZ</td>\n      <td>- | - | -</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Albania</td>\n      <td>ALB</td>\n      <td>AL | ALB | 008</td>\n      <td>ALB</td>\n      <td>.al</td>\n      <td>-</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cia_country_codes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        Entity GENC Stanag Internet Comment ISO-alpha2 ISO-alpha3 ISO-numeric\n0  Afghanistan  AFG    AFG      .af       -         AF        AFG         004\n1     Akrotiri  XQZ      -        -       -        NaN        NaN         NaN\n2      Albania  ALB    ALB      .al       -         AL        ALB         008",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Entity</th>\n      <th>GENC</th>\n      <th>Stanag</th>\n      <th>Internet</th>\n      <th>Comment</th>\n      <th>ISO-alpha2</th>\n      <th>ISO-alpha3</th>\n      <th>ISO-numeric</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>AFG</td>\n      <td>AFG</td>\n      <td>.af</td>\n      <td>-</td>\n      <td>AF</td>\n      <td>AFG</td>\n      <td>004</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Akrotiri</td>\n      <td>XQZ</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Albania</td>\n      <td>ALB</td>\n      <td>ALB</td>\n      <td>.al</td>\n      <td>-</td>\n      <td>AL</td>\n      <td>ALB</td>\n      <td>008</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split iso-codes in separate columns\n",
    "cia_country_codes[[\"ISO-alpha2\",\"ISO-alpha3\",\"ISO-numeric\"]] = cia_country_codes[\"ISO 3166\"].str.split(\"|\",2,expand=True)\n",
    "cia_country_codes.drop(columns=[\"ISO 3166\"], inplace=True)\n",
    "# strip whitespaces from iso-codes\n",
    "cia_country_codes[[\"ISO-alpha2\",\"ISO-alpha3\",\"ISO-numeric\"]] = cia_country_codes[[\"ISO-alpha2\",\"ISO-alpha3\",\"ISO-numeric\"]].apply(lambda x: x.str.strip())\n",
    "# replace not existing iso-codes with NaN for more clarity\n",
    "cia_country_codes[\"ISO-alpha2\"].replace(\"-\", np.nan, inplace=True)\n",
    "cia_country_codes[\"ISO-alpha3\"].replace(\"-\", np.nan, inplace=True)\n",
    "cia_country_codes[\"ISO-numeric\"].replace(\"-\", np.nan, inplace=True)\n",
    "# show preprocessed cia data\n",
    "cia_country_codes.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Determine ISO codes for temperature data\n",
    "\n",
    "Remove aggregated countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(237, 2)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_countries.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can see, that e.g. Denmark appears twice. This issue happens multiple times, and is due to the reason that some countries are aggregates of other countries e.g. `Denmark` consists of `Denmark (Europe)` also known as `Denmark Mainland`, and `Greenland`. The bearkley earth website has a worldmap on which the country is highlighted, this helped us to better understand what each of the conflicting countries is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             Country         Region\n55            Cyprus           Asia\n56    Czech Republic         Europe\n57           Denmark  North America\n58  Denmark (Europe)         Europe\n59          Djibouti         Africa",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Country</th>\n      <th>Region</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>55</th>\n      <td>Cyprus</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>Czech Republic</td>\n      <td>Europe</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>Denmark</td>\n      <td>North America</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>Denmark (Europe)</td>\n      <td>Europe</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>Djibouti</td>\n      <td>Africa</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_countries.iloc[55: 55+5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We decided to remove the \"aggregated\" country. Here is a list of the aggregate countries we removed, their individual parts still exists in the dataset:\n",
    "- Denmark (Denmark Mainland, Greenland)\n",
    "- France (France Mainland, French Guiana, French Polynesia, French Southern and Antarctic Lands)\n",
    "- Netherlands (Netherlands Mainland, Sint Maarten, Curaçao, Aruba)\n",
    "- United Kingdom (United Kingdom + Oversea territories such as Montserrat, Bermuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(233, 2)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_countries_remove = pd.DataFrame({\n",
    "    \"Country\": [\"Denmark\",\"France\", \"Netherlands\", \"United Kingdom\"],\n",
    "    \"Region\": [\"North America\", np.nan, \"Europe\", \"Europe\"]\n",
    "})\n",
    "temperature_countries_cleaned = pd.concat([temperature_countries, temperature_countries_remove]).drop_duplicates(keep=False)\n",
    "temperature_countries_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Rename Countries & Match ISO Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           Country ISO-alpha3\n18    Baker Island        NaN\n113   Kingman Reef        NaN\n161  Palmyra Atoll        NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Country</th>\n      <th>ISO-alpha3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18</th>\n      <td>Baker Island</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>Kingman Reef</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>Palmyra Atoll</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some countries need to be renamed so that we find the matching country-code later\n",
    "new_country_names = {\n",
    "    \"Denmark (Europe)\": \"Denmark\",\n",
    "    \"France (Europe)\": \"France\",\n",
    "    \"Netherlands (Europe)\": \"Netherlands\",\n",
    "    \"United Kingdom (Europe)\": \"United Kingdom\",\n",
    "    \"Åland\": \"Åland Islands\",\n",
    "    \"Czech Republic\": \"Czechia\",\n",
    "    \"Turkey\": \"Türkiye\",\n",
    "    \"Svalbard and Jan Mayen\": \"Svalbard and Jan Mayen Islands\",\n",
    "    \"Cape Verde\": \"Cabo Verde\",\n",
    "    \"Turks and Caicas Islands\": \"Turks and Caicos Islands\",\n",
    "    \"Swaziland\": \"Eswatini\",\n",
    "    \"Macedonia\": \"North Macedonia\",\n",
    "    \"Côte d'Ivoire\": \"Côte d’Ivoire\",\n",
    "    \"Federated States of Micronesia\": \"Micronesia (Federated States of)\",\n",
    "    \"South Georgia and the South Sandwich Isla\": \"South Georgia and the South Sandwich Islands\",\n",
    "    \"Bonaire, Saint Eustatius and Saba\": \"Bonaire, Sint Eustatius and Saba\",\n",
    "    \"Congo (Democratic Republic of the)\": \"Democratic Republic of the Congo\",\n",
    "    \"South Korea\": \"Korea, South\",\n",
    "    \"North Korea\": \"Korea, North\",\n",
    "    \"Palestina\": \"State of Palestine\"\n",
    "}\n",
    "\n",
    "temperature_countries_cleaned = temperature_countries_cleaned.replace({\"Country\": new_country_names}, inplace=False)\n",
    "\n",
    "# left-join cia-country-codes and un-country-codes\n",
    "temperature_countries_with_iso = temperature_countries_cleaned.merge(cia_country_codes,how=\"left\",left_on='Country', right_on='Entity')[[\"Country\",\"ISO-alpha3\"]]\n",
    "temperature_countries_with_iso = temperature_countries_with_iso.merge(un_country_codes,how=\"left\",left_on='Country', right_on='Country or Area')[[\"Country\",\"ISO-alpha3\", \"ISO-alpha3 Code\"]]\n",
    "\n",
    "# fill missing cia-country codes with un-country-codes\n",
    "temperature_countries_with_iso[\"ISO-alpha3\"].fillna(temperature_countries_with_iso[\"ISO-alpha3 Code\"], inplace=True)\n",
    "temperature_countries_with_iso.drop(columns=[\"ISO-alpha3 Code\"], inplace=True)\n",
    "\n",
    "# show countries for which we could not find an ISO code\n",
    "temperature_countries_with_iso[temperature_countries_with_iso[\"ISO-alpha3\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "These 3 countries/areas do not have any country codes in general, and are quite small, so we just ignore them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Todo: When combining adjust to use variable without saving temperature_countries_with_iso as csv file\n",
    "processed_countries_list_path = 'data/processed/temperature/temp-countries-list.csv'\n",
    "temperature_countries_with_iso.to_csv(processed_countries_list_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Which countries are in which datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Disaster vs. Temperature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countries in emdat & berkely: 209\n",
      "countries in emdat but not berkely (22):\n",
      "['ANT', 'AZO', 'BMU', 'BRN', 'COK', 'CSK', 'DDR', 'DFR', 'MDV', 'MHL', 'SCG', 'SHN', 'SPI', 'SSD', 'SUN', 'TKL', 'TUV', 'VUT', 'WLF', 'YMD', 'YMN', 'YUG']\n",
      "countries in berkely but not emdat (20):\n",
      "['ABW', 'ALA', 'AND', 'ATA', 'ATF', 'BES', 'CXR', 'ESH', 'FLK', 'FRO', 'GGY', 'GRL', 'HMD', 'JEY', 'LIE', 'MCO', 'SGS', 'SJM', 'SMR', 'SPM']\n"
     ]
    }
   ],
   "source": [
    "berkely_iso_codes = set(temperature_countries_with_iso[\"ISO-alpha3\"].dropna().tolist())\n",
    "emdat_iso_codes = set(disasters[\"ISO\"].unique().tolist())\n",
    "\n",
    "emdat_and_bekely = emdat_iso_codes.intersection(berkely_iso_codes)\n",
    "emdat_without_berkely = emdat_iso_codes-emdat_and_bekely\n",
    "berkely_without_emdat = berkely_iso_codes-emdat_and_bekely\n",
    "\n",
    "print(f\"countries in emdat & berkely: {len(emdat_and_bekely)}\")\n",
    "print(f\"countries in emdat but not berkely ({len(emdat_without_berkely)}):\")\n",
    "print(sorted(emdat_without_berkely))\n",
    "print(f\"countries in berkely but not emdat ({len(berkely_without_emdat)}):\")\n",
    "print(sorted(berkely_without_emdat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The Countries for which we have disaster data, but no temperature data are as follows:\n",
    "- Existing Countries (usually very small countries/islands):\n",
    "    - AZO Azores Islands\n",
    "    - BMU Bermuda\n",
    "    - BRN Brunei Darussalam\n",
    "    - COK Cook Islands (the)\n",
    "    - MDV Maldives\n",
    "    - MHL Marshall Islands (the)\n",
    "    - SHN Saint Helena, Ascension and Tristan da Cunha\n",
    "    - SSD South Sudan\n",
    "    - TKL Tokelau\n",
    "    - TUV Tuvalu\n",
    "    - VUT Vanuatu\n",
    "    - WLF Wallis and Futuna\n",
    "- Existing Countries (but invalid country code):\n",
    "    - SPI Canary Islands\n",
    "- Former Countries:\n",
    "    - ANT Netherlands Antilles\n",
    "    - CSK Czechoslovakia\n",
    "    - DDR Germany Dem Rep\n",
    "    - DFR Germany Fed Rep\n",
    "    - SCG Serbia Montenegro\n",
    "    - SUN Soviet Union\n",
    "    - YMD Yemen P Dem Rep\n",
    "    - YMN Yemen Arab Rep\n",
    "    - YUG Yugoslavia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Disaster vs Population Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gapminder_iso_codes = set(population_by_country[\"geo\"].str.upper().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countries in emdat & gapminder: 191\n",
      "countries in emdat but not gapminder (40):\n",
      "['AIA', 'ANT', 'ASM', 'AZO', 'BLM', 'BMU', 'COK', 'CSK', 'CUW', 'CYM', 'DDR', 'DFR', 'GLP', 'GUF', 'GUM', 'IMN', 'MAC', 'MAF', 'MNP', 'MSR', 'MTQ', 'MYT', 'NCL', 'NIU', 'PRI', 'PYF', 'REU', 'SCG', 'SHN', 'SPI', 'SUN', 'SXM', 'TCA', 'TKL', 'VGB', 'VIR', 'WLF', 'YMD', 'YMN', 'YUG']\n",
      "countries in gapminder but not emdat (6):\n",
      "['AND', 'HOS', 'LIE', 'MCO', 'NRU', 'SMR']\n"
     ]
    }
   ],
   "source": [
    "emdat_and_gapminder = emdat_iso_codes.intersection(gapminder_iso_codes)\n",
    "emdat_without_gapminder = emdat_iso_codes-emdat_and_gapminder\n",
    "gapminder_without_emdat = gapminder_iso_codes-emdat_and_gapminder\n",
    "\n",
    "print(f\"countries in emdat & gapminder: {len(emdat_and_gapminder)}\")\n",
    "print(f\"countries in emdat but not gapminder ({len(emdat_without_gapminder)}):\")\n",
    "print(sorted(emdat_without_gapminder))\n",
    "print(f\"countries in gapminder but not emdat ({len(gapminder_without_emdat)}):\")\n",
    "print(sorted(gapminder_without_emdat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The Countries for which we have disaster data, but no population data are as follows:\n",
    "- Existing Countries (independent)\n",
    "    - COK Cook Islands (the)\n",
    "    - NIU Niue\n",
    "- Existing Countries (dependent e.g .oversea territories)\n",
    "    - AIA Anguilla\n",
    "    - ASM American Samoa\n",
    "    - AZO Azores Islands\n",
    "    - BLM Saint Barthélemy\n",
    "    - BMU Bermuda\n",
    "    - CUW Curaçao\n",
    "    - CYM Cayman Islands (the)\n",
    "    - GLP Guadeloupe\n",
    "    - GUF French Guiana\n",
    "    - GUM Guam\n",
    "    - IMN Isle of Man\n",
    "    - MAC Macao\n",
    "    - MAF Saint Martin (French Part)\n",
    "    - MNP Northern Mariana Islands (the)\n",
    "    - MSR Montserrat\n",
    "    - MTQ Martinique\n",
    "    - MYT Mayotte\n",
    "    - NCL New Caledonia\n",
    "    - PRI Puerto Rico\n",
    "    - PYF French Polynesia\n",
    "    - REU Réunion\n",
    "    - SHN Saint Helena, Ascension and Tristan da Cunha\n",
    "    - SPI Canary Islands\n",
    "    - SXM Sint Maarten (Dutch part)\n",
    "    - TCA Turks and Caicos Islands (the)\n",
    "    - TKL Tokelau\n",
    "    - VGB Virgin Island (British)\n",
    "    - VIR Virgin Island (U.S.)\n",
    "    - WLF Wallis and Futuna\n",
    "- Former Countries\n",
    "    - ANT Netherlands Antilles\n",
    "    - CSK Czechoslovakia\n",
    "    - DDR Germany Dem Rep\n",
    "    - DFR Germany Fed Rep\n",
    "    - SCG Serbia Montenegro\n",
    "    - SUN Soviet Union\n",
    "    - YMD Yemen P Dem Rep\n",
    "    - YMN Yemen Arab Rep\n",
    "    - YUG Yugoslavia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Which regions are in which datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Africa', 'Asia', 'Europe', 'Americas', 'Oceania']"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disasters[\"Continent\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['africa', 'asia', 'europe', 'americas']"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population_by_region[\"geo\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Asia', 'Europe', 'Africa', 'South America', 'Oceania', 'North America']"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_countries[\"Region\"].dropna().unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As we can see, each dataset has a different number of regions. Additionally, we don't know which countries belong to each region. For example the region Europe could consist of different countries the disaster dataset than in the temperature dataset. That's why we decided to compute the regional data ourselves, by aggregating the countries according to UN Regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Which countries are need to be manually assigned a region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Check which countries are not in the UN Dataset, and thus need to be manually assigned a region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countries in emdat, but not un (12): \n",
      "{'DDR', 'TWN', 'DFR', 'CSK', 'ANT', 'SCG', 'YMN', 'YMD', 'SPI', 'SUN', 'AZO', 'YUG'}\n",
      "countries in gapminder, but not un (2): \n",
      "{'HOS', 'TWN'}\n",
      "countries in berkely, but not un (1): \n",
      "{'TWN'}\n"
     ]
    }
   ],
   "source": [
    "un_iso_codes = set(un_country_codes[\"ISO-alpha3 Code\"].tolist())\n",
    "\n",
    "emdat_without_un = emdat_iso_codes-un_iso_codes\n",
    "gapminder_without_un = gapminder_iso_codes-un_iso_codes\n",
    "berkely_without_un = berkely_iso_codes-un_iso_codes\n",
    "\n",
    "print(f\"countries in emdat, but not un ({len(emdat_without_un)}): \\n{emdat_without_un}\")\n",
    "print(f\"countries in gapminder, but not un ({len(gapminder_without_un)}): \\n{gapminder_without_un}\")\n",
    "print(f\"countries in berkely, but not un ({len(berkely_without_un)}): \\n{berkely_without_un}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Prepare Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1 Disaster Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Task:\n",
    "\n",
    "The goal is to convert the data into the following formats for later use.\n",
    "Along the way, this notebook does some data-preparation\n",
    "\n",
    "\n",
    "Disaster-All\n",
    "disaster/disaster-all:\n",
    "Columns: disaster_no, year, subgroup, type, total_deaths, dis_mag_value, dis_mag_scale, start_year, end_year\n",
    "Other interesting columns?\n",
    "\n",
    "Prefix: dis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Publisher: Centre for Research on the Epidemiology of Disasters (CRED)\n",
    "\n",
    "CRED defines a disaster as “a situation or event that overwhelms local capacity, necessitating a\n",
    "request at the national or international level for external assistance; an unforeseen and often sudden\n",
    "event that causes great damage, destruction and human suffering”\n",
    "\n",
    "For a disaster to be entered into the database at least one of the following criteria must be fulfilled:\n",
    "\n",
    "- 10 or more people reported killed\n",
    "- 100 or more people reported affected\n",
    "- Declaration of a state of emergency\n",
    "- Call for international assistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First look at the disaster data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "          Dis No  Year   Seq Glide Disaster Group Disaster Subgroup  \\\n0  1900-9002-CPV  1900  9002   NaN        Natural    Climatological   \n1  1900-9001-IND  1900  9001   NaN        Natural    Climatological   \n2  1901-0003-BEL  1901     3   NaN  Technological     Technological   \n3  1902-0012-GTM  1902    12   NaN        Natural       Geophysical   \n4  1902-0003-GTM  1902     3   NaN        Natural       Geophysical   \n\n         Disaster Type Disaster Subtype Disaster Subsubtype   Event Name  ...  \\\n0              Drought          Drought                 NaN          NaN  ...   \n1              Drought          Drought                 NaN          NaN  ...   \n2  Industrial accident        Explosion                 NaN    Coal mine  ...   \n3           Earthquake  Ground movement                 NaN          NaN  ...   \n4    Volcanic activity         Ash fall                 NaN  Santa Maria  ...   \n\n  Reconstruction Costs, Adjusted ('000 US$) Insured Damages ('000 US$)  \\\n0                                       NaN                        NaN   \n1                                       NaN                        NaN   \n2                                       NaN                        NaN   \n3                                       NaN                        NaN   \n4                                       NaN                        NaN   \n\n  Insured Damages, Adjusted ('000 US$) Total Damages ('000 US$)  \\\n0                                  NaN                      NaN   \n1                                  NaN                      NaN   \n2                                  NaN                      NaN   \n3                                  NaN                  25000.0   \n4                                  NaN                      NaN   \n\n  Total Damages, Adjusted ('000 US$)       CPI Adm Level Admin1 Code  \\\n0                                NaN  3.077091       NaN         NaN   \n1                                NaN  3.077091       NaN         NaN   \n2                                NaN  3.077091       NaN         NaN   \n3                           781207.0  3.200175       NaN         NaN   \n4                                NaN  3.200175       NaN         NaN   \n\n  Admin2 Code Geo Locations  \n0         NaN           NaN  \n1         NaN           NaN  \n2         NaN           NaN  \n3         NaN           NaN  \n4         NaN           NaN  \n\n[5 rows x 50 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dis No</th>\n      <th>Year</th>\n      <th>Seq</th>\n      <th>Glide</th>\n      <th>Disaster Group</th>\n      <th>Disaster Subgroup</th>\n      <th>Disaster Type</th>\n      <th>Disaster Subtype</th>\n      <th>Disaster Subsubtype</th>\n      <th>Event Name</th>\n      <th>...</th>\n      <th>Reconstruction Costs, Adjusted ('000 US$)</th>\n      <th>Insured Damages ('000 US$)</th>\n      <th>Insured Damages, Adjusted ('000 US$)</th>\n      <th>Total Damages ('000 US$)</th>\n      <th>Total Damages, Adjusted ('000 US$)</th>\n      <th>CPI</th>\n      <th>Adm Level</th>\n      <th>Admin1 Code</th>\n      <th>Admin2 Code</th>\n      <th>Geo Locations</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1900-9002-CPV</td>\n      <td>1900</td>\n      <td>9002</td>\n      <td>NaN</td>\n      <td>Natural</td>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>Drought</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.077091</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1900-9001-IND</td>\n      <td>1900</td>\n      <td>9001</td>\n      <td>NaN</td>\n      <td>Natural</td>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>Drought</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.077091</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1901-0003-BEL</td>\n      <td>1901</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>Technological</td>\n      <td>Technological</td>\n      <td>Industrial accident</td>\n      <td>Explosion</td>\n      <td>NaN</td>\n      <td>Coal mine</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.077091</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1902-0012-GTM</td>\n      <td>1902</td>\n      <td>12</td>\n      <td>NaN</td>\n      <td>Natural</td>\n      <td>Geophysical</td>\n      <td>Earthquake</td>\n      <td>Ground movement</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>25000.0</td>\n      <td>781207.0</td>\n      <td>3.200175</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1902-0003-GTM</td>\n      <td>1902</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>Natural</td>\n      <td>Geophysical</td>\n      <td>Volcanic activity</td>\n      <td>Ash fall</td>\n      <td>NaN</td>\n      <td>Santa Maria</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.200175</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 50 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Select & Rename Attributes\n",
    "\n",
    "1. Replace whitespaces with underscores\n",
    "2. Convert every character to lowercase\n",
    "3. Rename specific columns to ensure uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove whitespaces from all col-names and convert them to lower-case\n",
    "dis.columns = [c.replace(' ', '_').lower() for c in dis.columns]\n",
    "dis.rename(columns={'country':'country_name', 'iso':'country_code', 'disaster_group':'group','disaster_subgroup':'subgroup', 'disaster_subtype':'subtype', 'disaster_type':'type', 'total_deaths':'deaths'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the most interesting columns\n",
    "dis_all_col_names = [\"year\", \"dis_no\", \"country_name\", \"country_code\", \"location\",\"group\",\"subgroup\", \"type\", \"subtype\", \"deaths\", \"dis_mag_value\", \"dis_mag_scale\", \"start_year\", \"end_year\"]\n",
    "dis_all = dis.filter(items=dis_all_col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Which disaster groups are present in the dataset ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There are Natural disasters, technological disasters as well as complex disasters that represent specific events (e.g. famine) which are not directly linked to a natural hazard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['Natural', 'Technological', 'Complex Disasters'], dtype=object)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all[\"group\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We only focus on disasters which have a natural causation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis_all = dis[dis_all[\"group\"] == \"Natural\"].copy()\n",
    "dis_all.drop(columns=\"group\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Which types of natural disasters are there ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             subgroup                   type      deaths\n0          Biological        Animal accident        12.0\n1          Biological               Epidemic   9618804.0\n2          Biological     Insect infestation         0.0\n3      Climatological                Drought  11733889.0\n4      Climatological  Glacial lake outburst       262.0\n5      Climatological               Wildfire      4653.0\n6   Extra-terrestrial                 Impact         0.0\n7         Geophysical             Earthquake   2343912.0\n8         Geophysical    Mass movement (dry)      4644.0\n9         Geophysical      Volcanic activity     86893.0\n10       Hydrological                  Flood   7002992.0\n11       Hydrological              Landslide     67477.0\n12     Meteorological   Extreme temperature     194056.0\n13     Meteorological                    Fog      4000.0\n14     Meteorological                  Storm   1403609.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subgroup</th>\n      <th>type</th>\n      <th>deaths</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Biological</td>\n      <td>Animal accident</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Biological</td>\n      <td>Epidemic</td>\n      <td>9618804.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Biological</td>\n      <td>Insect infestation</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>11733889.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Climatological</td>\n      <td>Glacial lake outburst</td>\n      <td>262.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Climatological</td>\n      <td>Wildfire</td>\n      <td>4653.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Extra-terrestrial</td>\n      <td>Impact</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Geophysical</td>\n      <td>Earthquake</td>\n      <td>2343912.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Geophysical</td>\n      <td>Mass movement (dry)</td>\n      <td>4644.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Geophysical</td>\n      <td>Volcanic activity</td>\n      <td>86893.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Hydrological</td>\n      <td>Flood</td>\n      <td>7002992.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Hydrological</td>\n      <td>Landslide</td>\n      <td>67477.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Meteorological</td>\n      <td>Extreme temperature</td>\n      <td>194056.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Meteorological</td>\n      <td>Fog</td>\n      <td>4000.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Meteorological</td>\n      <td>Storm</td>\n      <td>1403609.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all.groupby([\"subgroup\",\"type\"]).agg({\"deaths\":\"sum\"}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The types of disasters are mostly the ones a normal person would expect when thinking about natural disasters. But there are some strange types like insect-infestations or animal-accident which are not that obvious to understand, they also have basically no deaths. Also for our research we want to exclude Epidemics since it would go beyond the scope of this task.\n",
    "\n",
    "Therefore also decided to omit disasters of the subgroups `Biological` and `Extra-terrestrial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis_all = dis_all[(dis_all[\"subgroup\"] != \"Biological\") & (dis_all[\"subgroup\"] != \"Extra-terrestrial\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We only consider the following types of disasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['Drought', 'Earthquake', 'Volcanic activity',\n       'Mass movement (dry)', 'Storm', 'Flood', 'Landslide', 'Wildfire',\n       'Extreme temperature ', 'Fog', 'Glacial lake outburst'],\n      dtype=object)"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all[\"type\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Handle Missing Values\n",
    "\n",
    "Fill missing Values for the number of deaths\n",
    "\n",
    "We can assume that missing values for the number of deaths of a particular disaster means that the deathtoll was 0.\n",
    "\n",
    "For the subtype we take a look for which type of natural disasters a subtype is not provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "dis_no                                           0\nyear                                             0\nseq                                              0\nglide                                        13353\nsubgroup                                         0\ntype                                             0\nsubtype                                       3116\ndisaster_subsubtype                          13768\nevent_name                                   12191\ncountry_name                                     0\ncountry_code                                     0\nregion                                           0\ncontinent                                        0\nlocation                                      1449\norigin                                       10901\nassociated_dis                               11341\nassociated_dis2                              14123\nofda_response                                13343\nappeal                                       13025\ndeclaration                                  12297\naid_contribution_('000_us$)                  14103\ndis_mag_value                                 9903\ndis_mag_scale                                 1114\nlatitude                                     12110\nlongitude                                    12110\nlocal_time                                   13732\nriver_basin                                  13549\nstart_year                                       0\nstart_month                                    295\nstart_day                                     2709\nend_year                                         0\nend_month                                      623\nend_day                                       2730\ndeaths                                        4412\nno_injured                                   10949\nno_affected                                   6478\nno_homeless                                  12412\ntotal_affected                                4182\nreconstruction_costs_('000_us$)              14838\nreconstruction_costs,_adjusted_('000_us$)    14838\ninsured_damages_('000_us$)                   13765\ninsured_damages,_adjusted_('000_us$)         13770\ntotal_damages_('000_us$)                      9559\ntotal_damages,_adjusted_('000_us$)            9611\ncpi                                            340\nadm_level                                     6528\nadmin1_code                                   9937\nadmin2_code                                  10683\ngeo_locations                                 6528\ndtype: int64"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['Flood', 'Storm', 'Landslide', 'Wildfire', 'Fog',\n       'Mass movement (dry)', 'Volcanic activity', 'Drought',\n       'Earthquake', 'Glacial lake outburst'], dtype=object)"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all[dis_all[\"subtype\"].isna()][\"type\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Unfortunately the missing values in the subtype column do not correspond to specific types of disasters.\n",
    "We can not conclude that easily what caused the values to be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['Drought', nan], dtype=object)"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all[dis_all[\"type\"] == \"Drought\"].subtype.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Disasters of type drought have no further more specific subtype.\n",
    "Consequently, we assign all droughts with missing values for subtype, the general type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis_all.loc[dis_all[\"type\"] == \"Drought\", \"subtype\"] = dis_all.loc[dis_all[\"type\"] == \"Drought\", \"subtype\"].fillna(\"Drought\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For all other types, we unfortunately can not guess the subtype.\n",
    "Therefore, we assign a custom label \"Uncategorized\" + the type of the disaster for each missing subtype\n",
    "We do not need to do this for disasters of type extreme temperature, since subtypes for all observations are specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis_all.loc[dis_all[\"type\"] == \"Earthquake\", \"subtype\"] = dis_all.loc[dis_all[\"type\"] == \"Earthquake\", \"subtype\"].fillna(\"Uncategorized_Earthquake\")\n",
    "dis_all.loc[dis_all[\"type\"] == \"Storm\", \"subtype\"] = dis_all.loc[dis_all[\"type\"] == \"Storm\", \"subtype\"].fillna(\"Uncategorized_Storm\")\n",
    "dis_all.loc[dis_all[\"type\"] == \"Flood\", \"subtype\"] = dis_all.loc[dis_all[\"type\"] == \"Flood\", \"subtype\"].fillna(\"Uncategorized_Flood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We assume that disasters with no death toll reported have a death toll of 0.\n",
    "This also aligns with the information we get from emdat (deaths < 10 are missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis_all[['deaths']] = dis_all[['deaths']].fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Determine Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As mentioned in `data integration` we want to compute the region of each disaster, by taking the UN Region that is assigned to each Country, in which the disaster occurred.\n",
    "Some country codes are not in the list of UN countries, thus we handle them specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countries in emdat, but not un (12): \n",
      "{'DDR', 'TWN', 'DFR', 'CSK', 'ANT', 'SCG', 'YMN', 'YMD', 'SPI', 'SUN', 'AZO', 'YUG'}\n"
     ]
    }
   ],
   "source": [
    "dis_iso_codes = set(dis_all[\"country_code\"].unique())\n",
    "un_iso_codes = set(un_country_codes[\"ISO-alpha3 Code\"].tolist())\n",
    "emdat_without_un = dis_iso_codes-un_iso_codes\n",
    "print(f\"countries in emdat, but not un ({len(emdat_without_un)}): \\n{emdat_without_un}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Automatically assign regions with UN Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For countries that were split in the past, but are now unified, we can just assign the unified country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Germany\n",
    "dis_all.loc[dis_all['country_code'] == \"DFR\",'country_code'] = \"DEU\"\n",
    "dis_all.loc[dis_all['country_code'] == \"DDR\",'country_code'] = \"DEU\"\n",
    "# Yemen\n",
    "dis_all.loc[dis_all['country_code'] == \"YMD\",'country_code'] = \"YEM\"\n",
    "dis_all.loc[dis_all['country_code'] == \"YMN\",'country_code'] = \"YEM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next we determine the region by using the un dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis_all = pd.merge(dis_all, un_country_codes[[\"ISO-alpha3 Code\",\"Region Name\", \"Region Code\"]], left_on='country_code', right_on='ISO-alpha3 Code', how='left')\n",
    "dis_all.rename(columns={\"Region Code\": \"region_code\", \"Region Name\": \"region_name\"}, inplace=True)\n",
    "dis_all.drop(columns=[\"ISO-alpha3 Code\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Manually Assign Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Some countries clearly belong to one region, so we can assign the disasters manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taiwan\n",
    "dis_all.loc[dis_all['country_code']=='TWN','region_name'] = 'Asia'\n",
    "dis_all.loc[dis_all['country_code']=='TWN','region_code'] = 142\n",
    "# Czechoslovakia\n",
    "dis_all.loc[dis_all['country_code']=='CSK','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='CSK','region_code'] = 150\n",
    "# Yugoslavia\n",
    "dis_all.loc[dis_all['country_code']=='YUG','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='YUG','region_code'] = 150\n",
    "# Serbia Montenegro\n",
    "dis_all.loc[dis_all['country_code']=='SCG','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='SCG','region_code'] = 150\n",
    "# Netherlands Antilles\n",
    "dis_all.loc[dis_all['country_code']=='ANT','region_name'] = 'Americas'\n",
    "dis_all.loc[dis_all['country_code']=='ANT','region_code'] = 19\n",
    "# Azores Islands\n",
    "dis_all.loc[dis_all['country_code']=='AZO','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='AZO','region_code'] = 150\n",
    "# Canary Islands\n",
    "dis_all.loc[dis_all['country_code']=='SPI','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='SPI','region_code'] = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Manually Assign Regions of Disasters Soviet Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For the Soviet Union disasters can occur in the european and/or asian parts.\n",
    "Thus we also take the \"location\" attribute into account and try to derive if the disaster was in europe or asia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask_europe = dis_all.loc[dis_all['country_code']=='SUN'][\"location\"].str.contains(\"Russian Federation|Ukraine|Moldavia|Siberia\").fillna(False)\n",
    "mask_asia = dis_all.loc[dis_all['country_code']=='SUN'][\"location\"].str.contains(\"Kazakhstan|Azerbaijan|Uzbekistan|Turkmenistan|Georgia|Armenia|Kyrgystan|Tajikistan|Tajiskistan|Tadzhikistan|Tadjikistan|Caucasus region|Dushanbe\", case=False).fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            dis_no  year   seq glide        subgroup     type  subtype  \\\n849  1921-9001-SUN  1921  9001   NaN  Climatological  Drought  Drought   \n\n    disaster_subsubtype event_name  country_name  ...  \\\n849                 NaN        NaN  Soviet Union  ...   \n\n    insured_damages,_adjusted_('000_us$) total_damages_('000_us$)  \\\n849                                  NaN                      NaN   \n\n    total_damages,_adjusted_('000_us$)       cpi adm_level admin1_code  \\\n849                                NaN  6.597283       NaN         NaN   \n\n    admin2_code geo_locations region_name region_code  \n849         NaN           NaN         NaN         NaN  \n\n[1 rows x 51 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dis_no</th>\n      <th>year</th>\n      <th>seq</th>\n      <th>glide</th>\n      <th>subgroup</th>\n      <th>type</th>\n      <th>subtype</th>\n      <th>disaster_subsubtype</th>\n      <th>event_name</th>\n      <th>country_name</th>\n      <th>...</th>\n      <th>insured_damages,_adjusted_('000_us$)</th>\n      <th>total_damages_('000_us$)</th>\n      <th>total_damages,_adjusted_('000_us$)</th>\n      <th>cpi</th>\n      <th>adm_level</th>\n      <th>admin1_code</th>\n      <th>admin2_code</th>\n      <th>geo_locations</th>\n      <th>region_name</th>\n      <th>region_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>849</th>\n      <td>1921-9001-SUN</td>\n      <td>1921</td>\n      <td>9001</td>\n      <td>NaN</td>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>Drought</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Soviet Union</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.597283</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 51 columns</p>\n</div>"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all[dis_all['country_code']=='SUN'][mask_europe & mask_asia]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Only one Event happened in both the asian as well as the european part of the soviet union.\n",
    "It is also a major event since it is a drought which caused the death of 1.2 million people.\n",
    "\n",
    "Researching the details of this event one can conclude that this observation can only be the Russian famine of 1921–1922.\n",
    "It mostly affected people living in europe, hence we assign this single observation the region europe.\n",
    "(https://en.wikipedia.org/wiki/Russian_famine_of_1921%E2%80%931922)\n",
    "\n",
    "For all other observations, the region should be unambiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis_all.loc[(dis_all['country_code']=='SUN') & mask_europe, \"region_name\"] = \"Europe\"\n",
    "dis_all.loc[(dis_all['country_code']=='SUN') & mask_europe, \"region_code\"] = 150\n",
    "\n",
    "dis_all.loc[(dis_all['country_code']=='SUN') & mask_asia, \"region_name\"] = \"Asia\"\n",
    "dis_all.loc[(dis_all['country_code']=='SUN') & mask_asia, \"region_code\"] = 142\n",
    "\n",
    "dis_all.loc[dis_all['dis_no']=='1921-9001-SUN', \"region_name\"] = \"Europe\"\n",
    "dis_all.loc[dis_all['dis_no']=='1921-9001-SUN', \"region_code\"] = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The only disasters without a region are now these 3 in the soviet Union. However, since they have no death count we can safely ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "              dis_no  year   seq glide        subgroup               type  \\\n0      1900-9002-CPV  1900  9002   NaN  Climatological            Drought   \n1      1900-9001-IND  1900  9001   NaN  Climatological            Drought   \n2      1902-0012-GTM  1902    12   NaN     Geophysical         Earthquake   \n3      1902-0003-GTM  1902     3   NaN     Geophysical  Volcanic activity   \n4      1902-0010-GTM  1902    10   NaN     Geophysical  Volcanic activity   \n...              ...   ...   ...   ...             ...                ...   \n14866  2022-0788-ZAF  2022   788   NaN    Hydrological              Flood   \n14867  2022-0313-ZAF  2022   313   NaN    Hydrological              Flood   \n14868  2022-0356-ZAF  2022   356   NaN    Hydrological              Flood   \n14869  2022-0209-COD  2022   209   NaN  Meteorological              Storm   \n14870  2022-0650-SSD  2022   650   NaN    Hydrological              Flood   \n\n                   subtype disaster_subsubtype   event_name  \\\n0                  Drought                 NaN          NaN   \n1                  Drought                 NaN          NaN   \n2          Ground movement                 NaN          NaN   \n3                 Ash fall                 NaN  Santa Maria   \n4                 Ash fall                 NaN  Santa Maria   \n...                    ...                 ...          ...   \n14866          Flash flood                 NaN          NaN   \n14867  Uncategorized_Flood                 NaN          NaN   \n14868  Uncategorized_Flood                 NaN          NaN   \n14869     Convective storm                 NaN          NaN   \n14870  Uncategorized_Flood                 NaN          NaN   \n\n                                 country_name  ...  \\\n0                                  Cabo Verde  ...   \n1                                       India  ...   \n2                                   Guatemala  ...   \n3                                   Guatemala  ...   \n4                                   Guatemala  ...   \n...                                       ...  ...   \n14866                            South Africa  ...   \n14867                            South Africa  ...   \n14868                            South Africa  ...   \n14869  Congo (the Democratic Republic of the)  ...   \n14870                             South Sudan  ...   \n\n      insured_damages,_adjusted_('000_us$) total_damages_('000_us$)  \\\n0                                      NaN                      NaN   \n1                                      NaN                      NaN   \n2                                      NaN                  25000.0   \n3                                      NaN                      NaN   \n4                                      NaN                      NaN   \n...                                    ...                      ...   \n14866                                  NaN                      NaN   \n14867                                  NaN                      NaN   \n14868                                  NaN                      NaN   \n14869                                  NaN                      NaN   \n14870                                  NaN                      NaN   \n\n      total_damages,_adjusted_('000_us$)       cpi adm_level  \\\n0                                    NaN  3.077091       NaN   \n1                                    NaN  3.077091       NaN   \n2                               781207.0  3.200175       NaN   \n3                                    NaN  3.200175       NaN   \n4                                    NaN  3.200175       NaN   \n...                                  ...       ...       ...   \n14866                                NaN       NaN       NaN   \n14867                                NaN       NaN         1   \n14868                                NaN       NaN       NaN   \n14869                                NaN       NaN       NaN   \n14870                                NaN       NaN         1   \n\n                          admin1_code admin2_code  \\\n0                                 NaN         NaN   \n1                                 NaN         NaN   \n2                                 NaN         NaN   \n3                                 NaN         NaN   \n4                                 NaN         NaN   \n...                               ...         ...   \n14866                             NaN         NaN   \n14867                           77311         NaN   \n14868                             NaN         NaN   \n14869                             NaN         NaN   \n14870  2747;2751;2754;2765;2768;37021         NaN   \n\n                                           geo_locations region_name  \\\n0                                                    NaN      Africa   \n1                                                    NaN        Asia   \n2                                                    NaN    Americas   \n3                                                    NaN    Americas   \n4                                                    NaN    Americas   \n...                                                  ...         ...   \n14866                                                NaN      Africa   \n14867                             KwaZulu-Natal (Adm1).       Africa   \n14868                                                NaN      Africa   \n14869                                                NaN      Africa   \n14870  Jonglei, Northern Bahr El Ghazal, Unity, Upper...      Africa   \n\n      region_code  \n0             2.0  \n1           142.0  \n2            19.0  \n3            19.0  \n4            19.0  \n...           ...  \n14866         2.0  \n14867         2.0  \n14868         2.0  \n14869         2.0  \n14870         2.0  \n\n[14871 rows x 51 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dis_no</th>\n      <th>year</th>\n      <th>seq</th>\n      <th>glide</th>\n      <th>subgroup</th>\n      <th>type</th>\n      <th>subtype</th>\n      <th>disaster_subsubtype</th>\n      <th>event_name</th>\n      <th>country_name</th>\n      <th>...</th>\n      <th>insured_damages,_adjusted_('000_us$)</th>\n      <th>total_damages_('000_us$)</th>\n      <th>total_damages,_adjusted_('000_us$)</th>\n      <th>cpi</th>\n      <th>adm_level</th>\n      <th>admin1_code</th>\n      <th>admin2_code</th>\n      <th>geo_locations</th>\n      <th>region_name</th>\n      <th>region_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1900-9002-CPV</td>\n      <td>1900</td>\n      <td>9002</td>\n      <td>NaN</td>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>Drought</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Cabo Verde</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.077091</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Africa</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1900-9001-IND</td>\n      <td>1900</td>\n      <td>9001</td>\n      <td>NaN</td>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>Drought</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>India</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.077091</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Asia</td>\n      <td>142.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1902-0012-GTM</td>\n      <td>1902</td>\n      <td>12</td>\n      <td>NaN</td>\n      <td>Geophysical</td>\n      <td>Earthquake</td>\n      <td>Ground movement</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Guatemala</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>25000.0</td>\n      <td>781207.0</td>\n      <td>3.200175</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Americas</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1902-0003-GTM</td>\n      <td>1902</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>Geophysical</td>\n      <td>Volcanic activity</td>\n      <td>Ash fall</td>\n      <td>NaN</td>\n      <td>Santa Maria</td>\n      <td>Guatemala</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.200175</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Americas</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1902-0010-GTM</td>\n      <td>1902</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>Geophysical</td>\n      <td>Volcanic activity</td>\n      <td>Ash fall</td>\n      <td>NaN</td>\n      <td>Santa Maria</td>\n      <td>Guatemala</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.200175</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Americas</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14866</th>\n      <td>2022-0788-ZAF</td>\n      <td>2022</td>\n      <td>788</td>\n      <td>NaN</td>\n      <td>Hydrological</td>\n      <td>Flood</td>\n      <td>Flash flood</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>South Africa</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Africa</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>14867</th>\n      <td>2022-0313-ZAF</td>\n      <td>2022</td>\n      <td>313</td>\n      <td>NaN</td>\n      <td>Hydrological</td>\n      <td>Flood</td>\n      <td>Uncategorized_Flood</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>South Africa</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>77311</td>\n      <td>NaN</td>\n      <td>KwaZulu-Natal (Adm1).</td>\n      <td>Africa</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>14868</th>\n      <td>2022-0356-ZAF</td>\n      <td>2022</td>\n      <td>356</td>\n      <td>NaN</td>\n      <td>Hydrological</td>\n      <td>Flood</td>\n      <td>Uncategorized_Flood</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>South Africa</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Africa</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>14869</th>\n      <td>2022-0209-COD</td>\n      <td>2022</td>\n      <td>209</td>\n      <td>NaN</td>\n      <td>Meteorological</td>\n      <td>Storm</td>\n      <td>Convective storm</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Congo (the Democratic Republic of the)</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Africa</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>14870</th>\n      <td>2022-0650-SSD</td>\n      <td>2022</td>\n      <td>650</td>\n      <td>NaN</td>\n      <td>Hydrological</td>\n      <td>Flood</td>\n      <td>Uncategorized_Flood</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>South Sudan</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>2747;2751;2754;2765;2768;37021</td>\n      <td>NaN</td>\n      <td>Jonglei, Northern Bahr El Ghazal, Unity, Upper...</td>\n      <td>Africa</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>14871 rows × 51 columns</p>\n</div>"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Save File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis_all.to_csv(DIS_PROCESSED_ALL_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2 Population Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nPopulation data\\n\\nThe population dataset by gapminder is a dataset containing information about the worlds population from 1800 until 2100.\\nThe data is a composition from different sources. There a two main sources - a dataset by Angus Maddison and the CLIO Infra Project -  and the World Population Prospects (WPP) provided by the UN.\\nThe dataset by Angus Maddison provides the data for the years 1800 - 1950. Population data after 1950 was taken from the WPP dataset.\\nAdditional sources were used, to fill missing values for years or regions. A details list of sources can be found in the documentation. (https://www.gapminder.org/data/documentation/gd003/)\\nThe primary data from the sources originates from census, informal census, indirect estimate and arbitrary guesses.\\n\\nModifications and Estimations:\\n- Summations of parts\\n- Larger area minus non-included parts\\n- Geographical interpolation\\n- Geographical extrapolation\\n- Temporal interpolation\\n- Temporal extrapolation\\n- Adjustments for under-enumeration\\n- Recalculated to fit present borders\\n\\nSince the data for the year 1950 of the two main datasets did not match for every country, small adjustments and smoothing were applied.\\n\\nPreprocessing:\\nFor our purposes, the subdatasets \"data-for-world-by-year\", \"data-for-regions-by-year\" and \"data-for-countries-by-year\" are relevant.\\nWe consider the period from 1900 until 2021 and remove redundant rows. We do this because the data after 2021 are based on estimated values.\\n\\nThere are no missing values.\\nSince the regions of the dataset did not match with the regions of our other datasets, we chose to compute the regions-population according to the regions from the un-country-codes.csv file.\\n\\nSubdataset:\\n    population-global.csv\\n        Columns: year, population\\n\\n    population-region.csv\\n        Columns: region_code, region_name, year, population\\n\\n    population-country.csv\\n        Columns: country_code, country_name, year, population\\n'"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Population data\n",
    "\n",
    "The population dataset by gapminder is a dataset containing information about the worlds population from 1800 until 2100.\n",
    "The data is a composition from different sources. There a two main sources - a dataset by Angus Maddison and the CLIO Infra Project -  and the World Population Prospects (WPP) provided by the UN.\n",
    "The dataset by Angus Maddison provides the data for the years 1800 - 1950. Population data after 1950 was taken from the WPP dataset.\n",
    "Additional sources were used, to fill missing values for years or regions. A details list of sources can be found in the documentation. (https://www.gapminder.org/data/documentation/gd003/)\n",
    "The primary data from the sources originates from census, informal census, indirect estimate and arbitrary guesses.\n",
    "\n",
    "Modifications and Estimations:\n",
    "- Summations of parts\n",
    "- Larger area minus non-included parts\n",
    "- Geographical interpolation\n",
    "- Geographical extrapolation\n",
    "- Temporal interpolation\n",
    "- Temporal extrapolation\n",
    "- Adjustments for under-enumeration\n",
    "- Recalculated to fit present borders\n",
    "\n",
    "Since the data for the year 1950 of the two main datasets did not match for every country, small adjustments and smoothing were applied.\n",
    "\n",
    "Preprocessing:\n",
    "For our purposes, the subdatasets \"data-for-world-by-year\", \"data-for-regions-by-year\" and \"data-for-countries-by-year\" are relevant.\n",
    "We consider the period from 1900 until 2021 and remove redundant rows. We do this because the data after 2021 are based on estimated values.\n",
    "\n",
    "There are no missing values.\n",
    "Since the regions of the dataset did not match with the regions of our other datasets, we chose to compute the regions-population according to the regions from the un-country-codes.csv file.\n",
    "\n",
    "Subdataset:\n",
    "    population-global.csv\n",
    "        Columns: year, population\n",
    "\n",
    "    population-region.csv\n",
    "        Columns: region_code, region_name, year, population\n",
    "\n",
    "    population-country.csv\n",
    "        Columns: country_code, country_name, year, population\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pop_dict = pd.read_excel('data/raw/population/gapminder-population-v7.xlsx', sheet_name=['data-for-world-by-year', 'data-for-regions-by-year', 'data-for-countries-etc-by-year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Preprocess global data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data for world population\n",
      "     geo   name    time   Population\n",
      "0  world  world  1800.0  985083734.9\n",
      "1  world  world  1801.0  988518009.0\n",
      "2  world  world  1802.0  991993182.0\n",
      "extract population data from 1900 until now\n",
      "       year  population\n",
      "100  1900.0  1627123965\n",
      "101  1901.0  1639684222\n",
      "102  1902.0  1652898195\n",
      "103  1903.0  1666760587\n",
      "104  1904.0  1680799825\n",
      "..      ...         ...\n",
      "217  2017.0 -2147483648\n",
      "218  2018.0 -2147483648\n",
      "219  2019.0 -2147483648\n",
      "220  2020.0 -2147483648\n",
      "221  2021.0 -2147483648\n",
      "\n",
      "[122 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukas\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:5039: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    }
   ],
   "source": [
    "pop_global_df = pop_dict.get('data-for-world-by-year')\n",
    "\n",
    "print('raw data for world population')\n",
    "print(pop_global_df.head(3))\n",
    "\n",
    "# remove unnecessary columns\n",
    "pop_global_df = pop_global_df[['time', 'Population']]\n",
    "pop_global_df.rename(columns={'Population': 'population', 'time': 'year'}, inplace=True)\n",
    "pop_global_df.set_index('year')\n",
    "\n",
    "print('extract population data from 1900 until now')\n",
    "pop_global_df = pop_global_df[pop_global_df['year'] >= 1900]\n",
    "pop_global_df = pop_global_df[pop_global_df['year'] <= 2021]\n",
    "pop_global_df['population'] = pop_global_df['population'].astype(int)\n",
    "print(pop_global_df)\n",
    "\n",
    "pop_global_df.to_csv('data/processed/population/population-global.csv', sep=';', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Prepare dataset for countries and regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     country_code country_name    year  population  region_code region_name\n",
      "8885          HOS     Holy See  2001.0         NaN        150.0      Europe\n",
      "8886          HOS     Holy See  2002.0         NaN        150.0      Europe\n",
      "8887          HOS     Holy See  2003.0         NaN        150.0      Europe\n",
      "8888          HOS     Holy See  2004.0         NaN        150.0      Europe\n",
      "8889          HOS     Holy See  2005.0         NaN        150.0      Europe\n",
      "8890          HOS     Holy See  2006.0         NaN        150.0      Europe\n",
      "8891          HOS     Holy See  2007.0         NaN        150.0      Europe\n",
      "8892          HOS     Holy See  2008.0         NaN        150.0      Europe\n",
      "8893          HOS     Holy See  2009.0         NaN        150.0      Europe\n",
      "8894          HOS     Holy See  2010.0         NaN        150.0      Europe\n",
      "8895          HOS     Holy See  2011.0         NaN        150.0      Europe\n",
      "8896          HOS     Holy See  2012.0         NaN        150.0      Europe\n",
      "8897          HOS     Holy See  2013.0         NaN        150.0      Europe\n",
      "8898          HOS     Holy See  2014.0         NaN        150.0      Europe\n",
      "8899          HOS     Holy See  2015.0         NaN        150.0      Europe\n",
      "8900          HOS     Holy See  2016.0         NaN        150.0      Europe\n",
      "8901          HOS     Holy See  2017.0         NaN        150.0      Europe\n",
      "8902          HOS     Holy See  2018.0         NaN        150.0      Europe\n",
      "8903          HOS     Holy See  2019.0         NaN        150.0      Europe\n",
      "8904          HOS     Holy See  2020.0         NaN        150.0      Europe\n",
      "8905          HOS     Holy See  2021.0         NaN        150.0      Europe\n"
     ]
    }
   ],
   "source": [
    "pop_country_df = pop_dict.get('data-for-countries-etc-by-year')\n",
    "un_country_codes = pd.read_csv(\"data/raw/country-codes/un-country-codes.csv\", sep=\";\")\n",
    "un_country_codes = un_country_codes[['Region Code', 'Region Name', 'ISO-alpha3 Code']]\n",
    "\n",
    "# extract population data from 1900 until now\n",
    "pop_country_df = pop_country_df[pop_country_df['time'] >= 1900]\n",
    "pop_country_df = pop_country_df[pop_country_df['time'] <= 2021]\n",
    "\n",
    "# format column that will be joined\n",
    "pop_country_df['geo'] = pop_country_df['geo'].str.upper()\n",
    "\n",
    "# merge country-population and un-country-codes\n",
    "pop_country_region_df = pd.merge(pop_country_df, un_country_codes, how='left', left_on='geo', right_on='ISO-alpha3 Code')\n",
    "\n",
    "pop_country_region_df.rename(columns={'Region Code': 'region_code', 'Region Name': 'region_name', 'time': 'year', 'Population': 'population', 'geo': 'country_code', 'name': 'country_name'}, inplace=True)\n",
    "\n",
    "# add missing region code to holy see and taiwan\n",
    "pop_country_region_df.loc[pop_country_region_df['country_code'] == 'HOS', 'region_code'] = 150\n",
    "pop_country_region_df.loc[pop_country_region_df['country_code'] == 'HOS', 'region_name'] = 'Europe'\n",
    "\n",
    "pop_country_region_df.loc[pop_country_region_df['country_code'] == 'TWN', 'region_code'] = 142\n",
    "pop_country_region_df.loc[pop_country_region_df['country_code'] == 'TWN', 'region_name'] = 'Asia'\n",
    "\n",
    "# remove unused column 'ISO-alpha3 Code'\n",
    "pop_country_region_df.drop('ISO-alpha3 Code', axis=1, inplace=True)\n",
    "\n",
    "# check if we worked correctly\n",
    "print(pop_country_region_df[pop_country_region_df.isnull().any(axis=1)])\n",
    "\n",
    "# the remaining missing values are population data for the country 'holy see' since the missing population is in the range < 1000 we, accept the missing values and calculate with the latest documented value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Save population regions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns_regions = ['region_code', 'region_name', 'country_code', 'country_name', 'year', 'population']\n",
    "\n",
    "# select columns\n",
    "pop_regions_df = pop_country_region_df[columns_regions]\n",
    "pop_regions_df = pop_regions_df.groupby(['region_code', 'region_name', 'year'], as_index=False)['population'].sum()\n",
    "pop_regions_df['population'] = pop_regions_df['population'].astype(int)\n",
    "\n",
    "#store in csv file\n",
    "pop_regions_df.to_csv('data/processed/population/population-region.csv', sep=';', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Save population countries dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns_country = ['country_code', 'country_name', 'year', 'population']\n",
    "\n",
    "# select columns\n",
    "pop_country_df = pop_country_region_df[columns_country]\n",
    "\n",
    "#store in csv file\n",
    "pop_country_df.to_csv('data/processed/population/population-country.csv', sep=';', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Compare calculated region population to gapminder dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total population from the year 2021\n",
      "Gapminder 7898737625.0\n",
      "Calculated 1057364421\n",
      "\n",
      "\n",
      "Gapminder region population\n",
      "          name    Population\n",
      "221     africa  1.391823e+09\n",
      "522       asia  4.634610e+09\n",
      "823     europe  8.460503e+08\n",
      "1124  americas  1.026254e+09\n",
      "\n",
      "\n",
      "Calculated region population\n",
      "    region_name  population\n",
      "121      Africa  1391823318\n",
      "243     Oceania    43602426\n",
      "365    Americas  1026253579\n",
      "487        Asia -2147483648\n",
      "609      Europe   743168746\n"
     ]
    }
   ],
   "source": [
    "regions_population_gapminder = pop_dict.get('data-for-regions-by-year')\n",
    "regions_population_2021_gapminder = regions_population_gapminder[regions_population_gapminder['time'] == 2021]\n",
    "total_population_gapminder = regions_population_2021_gapminder['Population'].sum()\n",
    "\n",
    "regions_population_2021_calc = pop_regions_df[pop_regions_df['year'] == 2021]\n",
    "total_population_calc = regions_population_2021_calc['population'].sum()\n",
    "\n",
    "print('Total population from the year 2021')\n",
    "print('Gapminder ' + str(total_population_gapminder))\n",
    "print('Calculated ' + str(total_population_calc))\n",
    "\n",
    "print('\\n')\n",
    "print('Gapminder region population')\n",
    "print(regions_population_2021_gapminder[['name', 'Population']])\n",
    "\n",
    "print('\\n')\n",
    "print('Calculated region population')\n",
    "print(regions_population_2021_calc[['region_name', 'population']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As we can see, we get the same results for the total populations in the year 2021.\n",
    "If we take a look at the region population, we see that the calculated population of america and africa shows no difference to the gapminder-dataset population. For the regions asia and europe, we get different results. Furthermore, the calculated region oceania does not exist in the gapminder dataset. Since the documentation of the gapminder dataset does not include the country-to-region assignment, we cannot compare the differences between our calculation and gapminder. However, since the overall score is equal, we can argue that those differences occur tue to different country-to-region assignments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
