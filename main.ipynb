{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Deaths by Natural Disasters and Climate Change\n",
    "\n",
    "*Sebastian FÃ¼rndraht, Hannes Rokitte, Paul Schmitt, Lukas Wieser*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Overview\n",
    "- Introduction\n",
    "    - Research Questions\n",
    "    - Used Datasets\n",
    "    - Requirements & Dependencies\n",
    "    - Constants\n",
    "    - Download Temperature Data\n",
    "    - Load Raw Datasets\n",
    "- Data Integration\n",
    "    - Preprocess CIA dataset\n",
    "    - Determine ISO codes for temperature data\n",
    "    - Which countries are in which datasets?\n",
    "    - Which regions are in which datasets?\n",
    "    - Which countries need to be manually assigned a region\n",
    "- Prepare Datasets\n",
    "    - ...\n",
    "- Data Exploration\n",
    "    - ...\n",
    "- Conclusion\n",
    "    - ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.1. Research Questions\n",
    "\n",
    "- How did the number of deaths per year from natural disasters change over the last hundred years?\n",
    "- How does this vary by country?\n",
    "- How does this vary by type of natural disaster?\n",
    "- Are there trends visible that could be due to climate change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.2. Used Datasets\n",
    "\n",
    "**Why we choose the datasets:**\n",
    "\n",
    "We have 3 main datasets. For disaster data we have EM-DAT. For determining the deaths per population of a natural disaster we also have the Gapminder Population Dataset. And finally for analysing trends of natural disasters with climate change we have temperature data by Berkeley Earth Study.\n",
    "\n",
    "Additionally, we have 2 auxiliary datasets for countries (UN Country Codes and CIA Country Codes). The temperature dataset only contains country names, and these datasets help us to derive the country codes. We need both of them since the CIA dataset is a bit outdated, and the UN dataset does only contain official UN countries, e.g. it excludes Taiwan. Additionally we also use the UN Dataset to derive the region of each country.\n",
    "\n",
    "**List of the datasets:**\n",
    "\n",
    "- EM-DAT:\n",
    "    - content: Yearly deaths by disasters\n",
    "    - timespan: 1900-2022\n",
    "    - countries: 235\n",
    "    - source: https://public.emdat.be/\n",
    "    - license: this is the only dataset, which is not allowed to be published.\n",
    "- Gapminder Population:\n",
    "    - content: Yearly population. Globally, by country, and by region\n",
    "    - timespan: 1800-2100\n",
    "    - source: https://www.gapminder.org/data/documentation/gd003/\n",
    "- Berkeley Earth Study:\n",
    "    - content: Monthly temperature anomalies. Globally, by country, and by region\n",
    "    - timespan: 1750-2020\n",
    "    - source: https://berkeleyearth.org/data/\n",
    "- UN Country Codes:\n",
    "    - content: Country Names, ISO Codes, and Regions.\n",
    "    - countries: 250\n",
    "    - source: https://unstats.un.org/unsd/methodology/m49/\n",
    "- CIA Country Codes:\n",
    "    - content: Country Names and Country Codes\n",
    "    - countries: 278\n",
    "    - source: https://www.cia.gov/the-world-factbook/references/country-data-codes/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.3. Requirements & Dependencies\n",
    "\n",
    "This project was created using Python 3.9.7.\n",
    "The exact versions of the dependencies can be installed with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import requests\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.4. Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# RAW\n",
    "DIS_RAW_FILE = Path('data/raw/disaster/emdat_public_2022_12_22_full.xlsx')\n",
    "TEMP_RAW_FOLDER = 'data/raw/temperature/'\n",
    "PATH_COUNTRIES_LAND_FOLDER = 'countries-land/'\n",
    "PATH_REGIONS_LAND_FOLDER = 'regions-land/'\n",
    "TEMP_GLOBAL_LAND_OCEAN_FILE = 'global-land-ocean.txt'\n",
    "TEMP_RAW_COUNTRIES_LIST_FILE = \"data/raw/temperature/countries-list.csv\"\n",
    "\n",
    "UN_COUNTRY_CODES_FILE = 'data/raw/country-codes/un-country-codes.csv'\n",
    "CIA_COUNTRY_CODES_FILE = \"data/raw/country-codes/cia-country-codes.csv\"\n",
    "\n",
    "# Preprocessed (to be deleted)\n",
    "DIS_PROCESSED_FOLDER = \"data/processed/disaster\"\n",
    "DIS_PROCESSED_ALL_FILE = Path(\"data/processed/disaster/disaster-all.csv\")\n",
    "\n",
    "PATH_COUNTRIES_LIST_FILE = 'temp-countries-list.csv'\n",
    "COUNTRIES_LIST_FILE = \"temp-countries-list.csv\"\n",
    "\n",
    "TEMP_PROCESSED_FOLDER = \"data/processed/temperature/\"\n",
    "TEMP_PROCESSED_COUNTRIES_LIST_FILE = \"data/processed/temperature/temp-countries-list.csv\"\n",
    "\n",
    "POP_PROCESSED_FOLDER = \"data/processed/population\"\n",
    "POP_PROCESSED_GLOBAL_FILE = 'data/processed/population/population-global.csv'\n",
    "POP_PROCESSED_REGION_FILE = 'data/processed/population/population-region.csv'\n",
    "POP_PROCESSED_COUNTRY_FILE = 'data/processed/population/population-country.csv'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Directories"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "Path(DIS_PROCESSED_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "Path(TEMP_PROCESSED_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "Path(POP_PROCESSED_FOLDER).mkdir(parents=True, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5. Download Temperature Data\n",
    "Automatically download the regional and country temperature data, so we don't have to download each file by ourselves."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "countries = pd.read_csv(TEMP_RAW_COUNTRIES_LIST_FILE, sep=\";\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "temp_regions = countries[\"Region\"].dropna().unique().tolist()\n",
    "temp_countries = countries[\"Country\"].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def download_temperature_countries(country_names: list[str]):\n",
    "    for country in country_names:\n",
    "        print(f\"downloading {country}\")\n",
    "        country_encoded = urllib.parse.quote(country.lower().replace(\" \", \"-\"),encoding='cp1252')\n",
    "        url = f\"http://berkeleyearth.lbl.gov/auto/Regional/TAVG/Text/{country_encoded}-TAVG-Trend.txt\"\n",
    "        response = requests.get(url)\n",
    "        data = response.text\n",
    "        with open(f'data/raw/temperature/countries-land/{country}.txt', 'w', encoding=\"utf-8\") as file:\n",
    "            file.write(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def download_temperature_regions(region_names: list[str]):\n",
    "    for region in region_names:\n",
    "        print(f\"downloading {region}\")\n",
    "        region_encoded = urllib.parse.quote(region.lower().replace(\" \", \"-\"),encoding='cp1252')\n",
    "        url = f\"http://berkeleyearth.lbl.gov/auto/Regional/TAVG/Text/{region_encoded}-TAVG-Trend.txt\"\n",
    "        response = requests.get(url)\n",
    "        data = response.text\n",
    "        with open(f'data/raw/temperature/regions-land/{region}.txt', 'w', encoding=\"utf-8\") as file:\n",
    "            file.write(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Change the variable `should_download_temperature_data` to `True`, to download the temperature data of countries & regions. (This should not be necessary, since the data should already be downloaded)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "should_download_temperature_data = False\n",
    "if should_download_temperature_data:\n",
    "    download_temperature_countries(temp_countries)\n",
    "    download_temperature_regions(temp_regions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.6. Load Raw Datasets\n",
    "\n",
    "Next we load the raw datasets. With exception to the temperature dataset, since it is divided into multiple files (e.g. one file per country) and will be loaded later on."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukas\\anaconda3\\envs\\dopp-a2\\lib\\site-packages\\openpyxl\\styles\\stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    }
   ],
   "source": [
    "dis = pd.read_excel(DIS_RAW_FILE, skiprows=6, sheet_name=\"emdat data\")\n",
    "\n",
    "temperature_countries = pd.read_csv(\"data/raw/temperature/countries-list.csv\", sep=\";\")\n",
    "\n",
    "pop_dict = pd.read_excel('data/raw/population/gapminder-population-v7.xlsx', sheet_name=['data-for-world-by-year', 'data-for-regions-by-year', 'data-for-countries-etc-by-year'])\n",
    "pop_global_df = pop_dict.get('data-for-world-by-year')\n",
    "pop_country_df = pop_dict.get('data-for-countries-etc-by-year')\n",
    "pop_region_gapminder = pop_dict.get('data-for-regions-by-year')\n",
    "\n",
    "un_country_codes = pd.read_csv(UN_COUNTRY_CODES_FILE, sep=\";\")\n",
    "cia_country_codes = pd.read_csv(CIA_COUNTRY_CODES_FILE, sep=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Data Integration\n",
    "\n",
    "In this section we want to ensure, that we can later combine our datasets without many problems.\n",
    "For this we first add country codes to the temperature dataset, and then take a closer look which countries and regions occur in each dataset.\n",
    "\n",
    "### 2.1 Preprocess CIA dataset\n",
    "\n",
    "First do some preprocessing on the CIA dataset, since it is a bit messy as we can see below. For example different kind of country codes are stored in the same column."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "        Entity GENC        ISO 3166 Stanag Internet Comment\n0  Afghanistan  AFG  AF | AFG | 004    AFG      .af       -\n1     Akrotiri  XQZ       - | - | -      -        -       -\n2      Albania  ALB  AL | ALB | 008    ALB      .al       -",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Entity</th>\n      <th>GENC</th>\n      <th>ISO 3166</th>\n      <th>Stanag</th>\n      <th>Internet</th>\n      <th>Comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>AFG</td>\n      <td>AF | AFG | 004</td>\n      <td>AFG</td>\n      <td>.af</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Akrotiri</td>\n      <td>XQZ</td>\n      <td>- | - | -</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Albania</td>\n      <td>ALB</td>\n      <td>AL | ALB | 008</td>\n      <td>ALB</td>\n      <td>.al</td>\n      <td>-</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cia_country_codes.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "        Entity GENC Stanag Internet Comment ISO-alpha2 ISO-alpha3 ISO-numeric\n0  Afghanistan  AFG    AFG      .af       -         AF        AFG         004\n1     Akrotiri  XQZ      -        -       -        NaN        NaN         NaN\n2      Albania  ALB    ALB      .al       -         AL        ALB         008",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Entity</th>\n      <th>GENC</th>\n      <th>Stanag</th>\n      <th>Internet</th>\n      <th>Comment</th>\n      <th>ISO-alpha2</th>\n      <th>ISO-alpha3</th>\n      <th>ISO-numeric</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>AFG</td>\n      <td>AFG</td>\n      <td>.af</td>\n      <td>-</td>\n      <td>AF</td>\n      <td>AFG</td>\n      <td>004</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Akrotiri</td>\n      <td>XQZ</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Albania</td>\n      <td>ALB</td>\n      <td>ALB</td>\n      <td>.al</td>\n      <td>-</td>\n      <td>AL</td>\n      <td>ALB</td>\n      <td>008</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split iso-codes in separate columns\n",
    "cia_country_codes[[\"ISO-alpha2\",\"ISO-alpha3\",\"ISO-numeric\"]] = cia_country_codes[\"ISO 3166\"].str.split(\"|\",2,expand=True)\n",
    "cia_country_codes.drop(columns=[\"ISO 3166\"], inplace=True)\n",
    "# strip whitespaces from iso-codes\n",
    "cia_country_codes[[\"ISO-alpha2\",\"ISO-alpha3\",\"ISO-numeric\"]] = cia_country_codes[[\"ISO-alpha2\",\"ISO-alpha3\",\"ISO-numeric\"]].apply(lambda x: x.str.strip())\n",
    "# replace not existing iso-codes with NaN for more clarity\n",
    "cia_country_codes[\"ISO-alpha2\"].replace(\"-\", np.nan, inplace=True)\n",
    "cia_country_codes[\"ISO-alpha3\"].replace(\"-\", np.nan, inplace=True)\n",
    "cia_country_codes[\"ISO-numeric\"].replace(\"-\", np.nan, inplace=True)\n",
    "# show preprocessed cia data\n",
    "cia_country_codes.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Determine ISO codes for temperature data\n",
    "\n",
    "Next we want to add ISO codes to the temperature dataset, since the temperature dataset only contains country names. This will help us later, when we want to relate countries in the disaster dataset to countries in the temperature dataset.\n",
    "\n",
    "#### Remove aggregated countries.\n",
    "\n",
    "We can see, that e.g. Denmark appears twice. This issue happens multiple times, and is due to the reason that some countries are aggregates of other countries e.g. `Denmark` consists of `Denmark (Europe)` also known as `Denmark Mainland`, and `Greenland`. The bearkley earth website has a worldmap on which the country is highlighted, this helped us to better understand what each of the conflicting countries is."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "             Country         Region\n55            Cyprus           Asia\n56    Czech Republic         Europe\n57           Denmark  North America\n58  Denmark (Europe)         Europe\n59          Djibouti         Africa",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Country</th>\n      <th>Region</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>55</th>\n      <td>Cyprus</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>Czech Republic</td>\n      <td>Europe</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>Denmark</td>\n      <td>North America</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>Denmark (Europe)</td>\n      <td>Europe</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>Djibouti</td>\n      <td>Africa</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_countries.iloc[55: 55+5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We decided to remove the \"aggregated\" country. Here is a list of the aggregate countries we removed, their individual parts still exists in the dataset:\n",
    "- Denmark (Denmark Mainland, Greenland)\n",
    "- France (France Mainland, French Guiana, French Polynesia, French Southern and Antarctic Lands)\n",
    "- Netherlands (Netherlands Mainland, Sint Maarten, CuraÃ§ao, Aruba)\n",
    "- United Kingdom (United Kingdom + Oversea territories such as Montserrat, Bermuda)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "temperature_countries_remove = pd.DataFrame({\n",
    "    \"Country\": [\"Denmark\",\"France\", \"Netherlands\", \"United Kingdom\"],\n",
    "    \"Region\": [\"North America\", np.nan, \"Europe\", \"Europe\"]\n",
    "})\n",
    "temperature_countries_cleaned = pd.concat([temperature_countries, temperature_countries_remove]).drop_duplicates(keep=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Rename Countries & Match ISO Codes\n",
    "\n",
    "Some countries need to be renamed so that we find the matching country-code later. After renaming the countries we can join the country codes to the temperature data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "           Country ISO-alpha3\n18    Baker Island        NaN\n113   Kingman Reef        NaN\n161  Palmyra Atoll        NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Country</th>\n      <th>ISO-alpha3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18</th>\n      <td>Baker Island</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>Kingman Reef</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>Palmyra Atoll</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_country_names = {\n",
    "    \"Denmark (Europe)\": \"Denmark\",\n",
    "    \"France (Europe)\": \"France\",\n",
    "    \"Netherlands (Europe)\": \"Netherlands\",\n",
    "    \"United Kingdom (Europe)\": \"United Kingdom\",\n",
    "    \"Ãland\": \"Ãland Islands\",\n",
    "    \"Czech Republic\": \"Czechia\",\n",
    "    \"Turkey\": \"TÃ¼rkiye\",\n",
    "    \"Svalbard and Jan Mayen\": \"Svalbard and Jan Mayen Islands\",\n",
    "    \"Cape Verde\": \"Cabo Verde\",\n",
    "    \"Turks and Caicas Islands\": \"Turks and Caicos Islands\",\n",
    "    \"Swaziland\": \"Eswatini\",\n",
    "    \"Macedonia\": \"North Macedonia\",\n",
    "    \"CÃ´te d'Ivoire\": \"CÃ´te dâIvoire\",\n",
    "    \"Federated States of Micronesia\": \"Micronesia (Federated States of)\",\n",
    "    \"South Georgia and the South Sandwich Isla\": \"South Georgia and the South Sandwich Islands\",\n",
    "    \"Bonaire, Saint Eustatius and Saba\": \"Bonaire, Sint Eustatius and Saba\",\n",
    "    \"Congo (Democratic Republic of the)\": \"Democratic Republic of the Congo\",\n",
    "    \"South Korea\": \"Korea, South\",\n",
    "    \"North Korea\": \"Korea, North\",\n",
    "    \"Palestina\": \"State of Palestine\"\n",
    "}\n",
    "\n",
    "temperature_countries_cleaned = temperature_countries_cleaned.replace({\"Country\": new_country_names}, inplace=False)\n",
    "\n",
    "# left-join cia-country-codes and un-country-codes\n",
    "temperature_countries_with_iso = temperature_countries_cleaned.merge(cia_country_codes,how=\"left\",left_on='Country', right_on='Entity')[[\"Country\",\"ISO-alpha3\"]]\n",
    "temperature_countries_with_iso = temperature_countries_with_iso.merge(un_country_codes,how=\"left\",left_on='Country', right_on='Country or Area')[[\"Country\",\"ISO-alpha3\", \"ISO-alpha3 Code\"]]\n",
    "\n",
    "# fill missing cia-country codes with un-country-codes\n",
    "temperature_countries_with_iso[\"ISO-alpha3\"].fillna(temperature_countries_with_iso[\"ISO-alpha3 Code\"], inplace=True)\n",
    "temperature_countries_with_iso.drop(columns=[\"ISO-alpha3 Code\"], inplace=True)\n",
    "\n",
    "# show countries for which we could not find an ISO code\n",
    "temperature_countries_with_iso[temperature_countries_with_iso[\"ISO-alpha3\"].isna()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These 3 countries/areas above do not have any country codes in general, and are quite small, so we just ignore them later on."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Which countries are in which datasets?\n",
    "\n",
    "Next we take a look, if the datasets all have the same countries, or if there are countries that only exist in one dataset.\n",
    "\n",
    "#### Disaster vs. Temperature Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countries in disaster & temperature dataset: 209\n",
      "countries in disaster but not temperature dataset (22):\n",
      "['ANT', 'AZO', 'BMU', 'BRN', 'COK', 'CSK', 'DDR', 'DFR', 'MDV', 'MHL', 'SCG', 'SHN', 'SPI', 'SSD', 'SUN', 'TKL', 'TUV', 'VUT', 'WLF', 'YMD', 'YMN', 'YUG']\n",
      "countries in temperature but not disaster dataset (20):\n",
      "['ABW', 'ALA', 'AND', 'ATA', 'ATF', 'BES', 'CXR', 'ESH', 'FLK', 'FRO', 'GGY', 'GRL', 'HMD', 'JEY', 'LIE', 'MCO', 'SGS', 'SJM', 'SMR', 'SPM']\n"
     ]
    }
   ],
   "source": [
    "berkely_iso_codes = set(temperature_countries_with_iso[\"ISO-alpha3\"].dropna().tolist())\n",
    "emdat_iso_codes = set(dis[\"ISO\"].unique().tolist())\n",
    "\n",
    "emdat_and_bekely = emdat_iso_codes.intersection(berkely_iso_codes)\n",
    "emdat_without_berkely = emdat_iso_codes-emdat_and_bekely\n",
    "berkely_without_emdat = berkely_iso_codes-emdat_and_bekely\n",
    "\n",
    "print(f\"countries in disaster & temperature dataset: {len(emdat_and_bekely)}\")\n",
    "print(f\"countries in disaster but not temperature dataset ({len(emdat_without_berkely)}):\")\n",
    "print(sorted(emdat_without_berkely))\n",
    "print(f\"countries in temperature but not disaster dataset ({len(berkely_without_emdat)}):\")\n",
    "print(sorted(berkely_without_emdat))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The composition of countries for which we have disaster data, but no temperature data is as follows:\n",
    "- Existing Countries (usually very small countries/islands):\n",
    "     - `AZO` Azores Islands, `BMU` Bermuda, `BRN` Brunei Darussalam, `COK` Cook Islands (the), `MDV` Maldives, `MHL` Marshall Islands (the), `SHN` Saint Helena, Ascension and Tristan da Cunha, `SSD` South Sudan, `TKL` Tokelau, `TUV` Tuvalu, `VUT` Vanuatu, `WLF` Wallis and Futuna\n",
    "- Existing Countries (but invalid country code):\n",
    "    - `SPI` Canary Islands\n",
    "- Former Countries:\n",
    "    - `ANT` Netherlands Antilles,`CSK` Czechoslovakia,`DDR` Germany Dem Rep,`DFR` Germany Fed Rep,`SCG` Serbia Montenegro,`SUN` Soviet Union,`YMD` Yemen P Dem Rep,`YMN` Yemen Arab Rep,`YUG` Yugoslavia\n",
    "\n",
    "#### Disaster vs Population Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countries in disaster & population dataset: 191\n",
      "countries in disaster but not population dataset (40):\n",
      "['AIA', 'ANT', 'ASM', 'AZO', 'BLM', 'BMU', 'COK', 'CSK', 'CUW', 'CYM', 'DDR', 'DFR', 'GLP', 'GUF', 'GUM', 'IMN', 'MAC', 'MAF', 'MNP', 'MSR', 'MTQ', 'MYT', 'NCL', 'NIU', 'PRI', 'PYF', 'REU', 'SCG', 'SHN', 'SPI', 'SUN', 'SXM', 'TCA', 'TKL', 'VGB', 'VIR', 'WLF', 'YMD', 'YMN', 'YUG']\n",
      "countries in population but not disaster dataset (6):\n",
      "['AND', 'HOS', 'LIE', 'MCO', 'NRU', 'SMR']\n"
     ]
    }
   ],
   "source": [
    "gapminder_iso_codes = set(pop_country_df[\"geo\"].str.upper().unique())\n",
    "\n",
    "emdat_and_gapminder = emdat_iso_codes.intersection(gapminder_iso_codes)\n",
    "emdat_without_gapminder = emdat_iso_codes-emdat_and_gapminder\n",
    "gapminder_without_emdat = gapminder_iso_codes-emdat_and_gapminder\n",
    "\n",
    "print(f\"countries in disaster & population dataset: {len(emdat_and_gapminder)}\")\n",
    "print(f\"countries in disaster but not population dataset ({len(emdat_without_gapminder)}):\")\n",
    "print(sorted(emdat_without_gapminder))\n",
    "print(f\"countries in population but not disaster dataset ({len(gapminder_without_emdat)}):\")\n",
    "print(sorted(gapminder_without_emdat))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Countries for which we have disaster data, but no population data are as follows:\n",
    "- Existing Countries (independent)\n",
    "    - `COK` Cook Islands (the), `NIU` Niue\n",
    "- Existing Countries (dependent e.g .oversea territories)\n",
    "\t- `AIA` Anguilla, `ASM` American Samoa, `AZO` Azores Islands, `BLM` Saint BarthÃ©lemy, `BMU` Bermuda, `CUW` CuraÃ§ao, `CYM` Cayman Islands (the), `GLP` Guadeloupe, `GUF` French Guiana, `GUM` Guam, `IMN` Isle of Man, `MAC` Macao, `MAF` Saint Martin (French Part), `MNP` Northern Mariana Islands (the), `MSR` Montserrat, `MTQ` Martinique, `MYT` Mayotte, `NCL` New Caledonia, `PRI` Puerto Rico, `PYF` French Polynesia, `REU` RÃ©union, `SHN` Saint Helena, Ascension and Tristan da Cunha, `SPI` Canary Islands, `SXM` Sint Maarten (Dutch part), `TCA` Turks and Caicos Islands (the), `TKL` Tokelau, `VGB` Virgin Island (British), `VIR` Virgin Island (U.S.), `WLF` Wallis and Futuna\n",
    "- Former Countries\n",
    "\t- `ANT` Netherlands Antilles, `CSK` Czechoslovakia, `DDR` Germany Dem Rep, `DFR` Germany Fed Rep, `SCG` Serbia Montenegro, `SUN` Soviet Union, `YMD` Yemen P Dem Rep, `YMN` Yemen Arab Rep, `YUG` Yugoslavia\n",
    "\n",
    "#### Conclusion of comparing countries\n",
    "\n",
    "When comparing data on the country level, we should keep in mind, that (1) countries can change over time, and (2) that for some countries should only occur in one dataset and thus should probably be ignored when trying to relate disasters with climate change.\n",
    "\n",
    "We can later also handle former countries e.g. merge `DDR` and `DFR` into `DEU`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Which regions are in which datasets?\n",
    "\n",
    "The regions have different names for each dataset (\"continent\", \"geo\", \"Region\")."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "['Africa', 'Asia', 'Europe', 'Americas', 'Oceania']"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis[\"Continent\"].unique().tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "['africa', 'asia', 'europe', 'americas']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_region_gapminder[\"geo\"].unique().tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "['Asia', 'Europe', 'Africa', 'South America', 'Oceania', 'North America']"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_countries[\"Region\"].dropna().unique().tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, each dataset has a different number of regions. Additionally, we don't know which countries belong to each region. For example the region Europe could consist of different countries the disaster dataset than in the temperature dataset.\n",
    "\n",
    "That's why we decided to **compute the regional data ourselves**, by aggregating the countries according to UN Regions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5 Which countries need to be manually assigned a region\n",
    "\n",
    "Check which countries are not in the UN dataset, and thus need to be manually assigned a region."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countries in disaster, but not UN dataset (12): \n",
      "['ANT', 'AZO', 'CSK', 'DDR', 'DFR', 'SCG', 'SPI', 'SUN', 'TWN', 'YMD', 'YMN', 'YUG']\n",
      "countries in population, but not UN dataset (2): \n",
      "['HOS', 'TWN']\n",
      "countries in temperature, but not UN dataset (1): \n",
      "['TWN']\n"
     ]
    }
   ],
   "source": [
    "un_iso_codes = set(un_country_codes[\"ISO-alpha3 Code\"].tolist())\n",
    "\n",
    "emdat_without_un = emdat_iso_codes-un_iso_codes\n",
    "gapminder_without_un = gapminder_iso_codes-un_iso_codes\n",
    "berkely_without_un = berkely_iso_codes-un_iso_codes\n",
    "\n",
    "print(f\"countries in disaster, but not UN dataset ({len(emdat_without_un)}): \\n{sorted(emdat_without_un)}\")\n",
    "print(f\"countries in population, but not UN dataset ({len(gapminder_without_un)}): \\n{sorted(gapminder_without_un)}\")\n",
    "print(f\"countries in temperature, but not UN dataset ({len(berkely_without_un)}): \\n{sorted(berkely_without_un)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Prepare Datasets\n",
    "\n",
    "In this section we apply preprocessing to our 3 datasets. This includes standardizing the column names, removing unnecessary columns, removing rows, handling missing values, deriving the region of each country, aggregating data, etc."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Disaster Data\n",
    "\n",
    "The goal is to preprocess the disaster data for later use.\n",
    "\n",
    "**What is considered as a disaster?**\n",
    "\n",
    "The publisher of the dataset *Centre for Research on the Epidemiology of Disasters* (CRED) defines a disaster as âa situation or event that overwhelms local capacity, necessitating a request at the national or international level for external assistance; an unforeseen and often sudden event that causes great damage, destruction and human sufferingâ\n",
    "\n",
    "For a disaster to be entered into the database at least one of the following criteria must be fulfilled:\n",
    "\n",
    "- 10 or more people reported killed\n",
    "- 100 or more people reported affected\n",
    "- Declaration of a state of emergency\n",
    "- Call for international assistance\n",
    "\n",
    "First look at the disaster data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "          Dis No  Year   Seq Glide Disaster Group Disaster Subgroup  \\\n0  1900-9002-CPV  1900  9002   NaN        Natural    Climatological   \n1  1900-9001-IND  1900  9001   NaN        Natural    Climatological   \n2  1901-0003-BEL  1901     3   NaN  Technological     Technological   \n3  1902-0012-GTM  1902    12   NaN        Natural       Geophysical   \n4  1902-0003-GTM  1902     3   NaN        Natural       Geophysical   \n\n         Disaster Type Disaster Subtype Disaster Subsubtype   Event Name  ...  \\\n0              Drought          Drought                 NaN          NaN  ...   \n1              Drought          Drought                 NaN          NaN  ...   \n2  Industrial accident        Explosion                 NaN    Coal mine  ...   \n3           Earthquake  Ground movement                 NaN          NaN  ...   \n4    Volcanic activity         Ash fall                 NaN  Santa Maria  ...   \n\n  Reconstruction Costs, Adjusted ('000 US$) Insured Damages ('000 US$)  \\\n0                                       NaN                        NaN   \n1                                       NaN                        NaN   \n2                                       NaN                        NaN   \n3                                       NaN                        NaN   \n4                                       NaN                        NaN   \n\n  Insured Damages, Adjusted ('000 US$) Total Damages ('000 US$)  \\\n0                                  NaN                      NaN   \n1                                  NaN                      NaN   \n2                                  NaN                      NaN   \n3                                  NaN                  25000.0   \n4                                  NaN                      NaN   \n\n  Total Damages, Adjusted ('000 US$)       CPI Adm Level Admin1 Code  \\\n0                                NaN  3.077091       NaN         NaN   \n1                                NaN  3.077091       NaN         NaN   \n2                                NaN  3.077091       NaN         NaN   \n3                           781207.0  3.200175       NaN         NaN   \n4                                NaN  3.200175       NaN         NaN   \n\n  Admin2 Code Geo Locations  \n0         NaN           NaN  \n1         NaN           NaN  \n2         NaN           NaN  \n3         NaN           NaN  \n4         NaN           NaN  \n\n[5 rows x 50 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dis No</th>\n      <th>Year</th>\n      <th>Seq</th>\n      <th>Glide</th>\n      <th>Disaster Group</th>\n      <th>Disaster Subgroup</th>\n      <th>Disaster Type</th>\n      <th>Disaster Subtype</th>\n      <th>Disaster Subsubtype</th>\n      <th>Event Name</th>\n      <th>...</th>\n      <th>Reconstruction Costs, Adjusted ('000 US$)</th>\n      <th>Insured Damages ('000 US$)</th>\n      <th>Insured Damages, Adjusted ('000 US$)</th>\n      <th>Total Damages ('000 US$)</th>\n      <th>Total Damages, Adjusted ('000 US$)</th>\n      <th>CPI</th>\n      <th>Adm Level</th>\n      <th>Admin1 Code</th>\n      <th>Admin2 Code</th>\n      <th>Geo Locations</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1900-9002-CPV</td>\n      <td>1900</td>\n      <td>9002</td>\n      <td>NaN</td>\n      <td>Natural</td>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>Drought</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.077091</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1900-9001-IND</td>\n      <td>1900</td>\n      <td>9001</td>\n      <td>NaN</td>\n      <td>Natural</td>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>Drought</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.077091</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1901-0003-BEL</td>\n      <td>1901</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>Technological</td>\n      <td>Technological</td>\n      <td>Industrial accident</td>\n      <td>Explosion</td>\n      <td>NaN</td>\n      <td>Coal mine</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.077091</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1902-0012-GTM</td>\n      <td>1902</td>\n      <td>12</td>\n      <td>NaN</td>\n      <td>Natural</td>\n      <td>Geophysical</td>\n      <td>Earthquake</td>\n      <td>Ground movement</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>25000.0</td>\n      <td>781207.0</td>\n      <td>3.200175</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1902-0003-GTM</td>\n      <td>1902</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>Natural</td>\n      <td>Geophysical</td>\n      <td>Volcanic activity</td>\n      <td>Ash fall</td>\n      <td>NaN</td>\n      <td>Santa Maria</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.200175</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã 50 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that each row contains a single disaster, and corresponding information."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Select & Rename Attributes\n",
    "\n",
    "Replace whitespaces with underscores, convert every character to lowercase, rename specific columns to ensure uniformity\n",
    "and finally select only the interesting columns. Other columns like \"economic\" impact are removed, since they are out of scope for this task."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Remove whitespaces from all col-names and convert them to lower-case\n",
    "dis.columns = [c.replace(' ', '_').lower() for c in dis.columns]\n",
    "dis.rename(columns={'country':'country_name', 'iso':'country_code', 'disaster_group':'group','disaster_subgroup':'subgroup', 'disaster_subtype':'subtype', 'disaster_type':'type', 'total_deaths':'deaths'}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Select the most interesting columns\n",
    "dis_all_col_names = [\"year\", \"dis_no\", \"country_name\", \"country_code\", \"location\",\"group\",\"subgroup\", \"type\", \"subtype\", \"deaths\", \"dis_mag_value\", \"dis_mag_scale\", \"start_year\", \"end_year\"]\n",
    "dis_all = dis.filter(items=dis_all_col_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Which disaster groups are present in the dataset ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are Natural disasters, technological disasters as well as complex disasters that represent specific events (e.g. famine) which are not directly linked to a natural hazard."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "['Natural', 'Technological', 'Complex Disasters']"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all[\"group\"].unique().tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We only focus on disasters which have a natural causation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "dis_all = dis_all[dis_all[\"group\"] == \"Natural\"].copy()\n",
    "dis_all.drop(columns=\"group\", inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Which types of natural disasters are there ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "             subgroup                   type      deaths\n0          Biological        Animal accident        12.0\n1          Biological               Epidemic   9618804.0\n2          Biological     Insect infestation         0.0\n3      Climatological                Drought  11733889.0\n4      Climatological  Glacial lake outburst       262.0\n5      Climatological               Wildfire      4653.0\n6   Extra-terrestrial                 Impact         0.0\n7         Geophysical             Earthquake   2343912.0\n8         Geophysical    Mass movement (dry)      4644.0\n9         Geophysical      Volcanic activity     86893.0\n10       Hydrological                  Flood   7002992.0\n11       Hydrological              Landslide     67477.0\n12     Meteorological   Extreme temperature     194056.0\n13     Meteorological                    Fog      4000.0\n14     Meteorological                  Storm   1403609.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subgroup</th>\n      <th>type</th>\n      <th>deaths</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Biological</td>\n      <td>Animal accident</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Biological</td>\n      <td>Epidemic</td>\n      <td>9618804.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Biological</td>\n      <td>Insect infestation</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>11733889.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Climatological</td>\n      <td>Glacial lake outburst</td>\n      <td>262.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Climatological</td>\n      <td>Wildfire</td>\n      <td>4653.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Extra-terrestrial</td>\n      <td>Impact</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Geophysical</td>\n      <td>Earthquake</td>\n      <td>2343912.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Geophysical</td>\n      <td>Mass movement (dry)</td>\n      <td>4644.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Geophysical</td>\n      <td>Volcanic activity</td>\n      <td>86893.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Hydrological</td>\n      <td>Flood</td>\n      <td>7002992.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Hydrological</td>\n      <td>Landslide</td>\n      <td>67477.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Meteorological</td>\n      <td>Extreme temperature</td>\n      <td>194056.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Meteorological</td>\n      <td>Fog</td>\n      <td>4000.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Meteorological</td>\n      <td>Storm</td>\n      <td>1403609.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all.groupby([\"subgroup\",\"type\"]).agg({\"deaths\":\"sum\"}).reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The types of disasters are mostly the ones a normal person would expect when thinking about natural disasters. But there are some strange types like `Insect Infestations` or `Animal Accident` which are not that obvious to understand, they also have basically no deaths. Also, for our research we want to exclude `Epidemics` since it would go beyond the scope of this task.\n",
    "\n",
    "Therefore also decided to omit disasters of the subgroups `Biological` and `Extra-terrestrial`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "dis_all = dis_all[(dis_all[\"subgroup\"] != \"Biological\") & (dis_all[\"subgroup\"] != \"Extra-terrestrial\")]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We only consider the following types of disasters:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "['Drought',\n 'Earthquake',\n 'Extreme temperature ',\n 'Flood',\n 'Fog',\n 'Glacial lake outburst',\n 'Landslide',\n 'Mass movement (dry)',\n 'Storm',\n 'Volcanic activity',\n 'Wildfire']"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dis_all[\"type\"].unique())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Handle Missing Values\n",
    "\n",
    "Fill missing Values for the number of deaths\n",
    "\n",
    "We can assume that missing values for the number of deaths of a particular disaster means that the deathtoll was 0.\n",
    "\n",
    "For the subtype we take a look for which type of natural disasters a subtype is not provided."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "year                0\ndis_no              0\ncountry_name        0\ncountry_code        0\nlocation         1449\nsubgroup            0\ntype                0\nsubtype          3116\ndeaths           4412\ndis_mag_value    9903\ndis_mag_scale    1114\nstart_year          0\nend_year            0\ndtype: int64"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all.isna().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that all disasters have a year, country, and type. However, especially for subtype and deaths it would be interesting to take a closer look, since the \"type\" of disasters and the \"deaths\" are integral parts of our analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "['Drought',\n 'Earthquake',\n 'Flood',\n 'Fog',\n 'Glacial lake outburst',\n 'Landslide',\n 'Mass movement (dry)',\n 'Storm',\n 'Volcanic activity',\n 'Wildfire']"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dis_all[dis_all[\"subtype\"].isna()][\"type\"].unique())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unfortunately the missing values in the subtype column do not correspond to specific types of disasters, but basically all diaster types have missing subtypes. We can not conclude that easily what caused the values to be missing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['Drought', nan], dtype=object)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all[dis_all[\"type\"] == \"Drought\"].subtype.unique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Disasters of type drought have no further more specific subtype. Consequently, we assign all droughts with missing values for subtype, the general type"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "dis_all.loc[dis_all[\"type\"] == \"Drought\", \"subtype\"] = dis_all.loc[dis_all[\"type\"] == \"Drought\", \"subtype\"].fillna(\"Drought\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For all other types, we unfortunately can not guess the subtype. Therefore, we assign a custom label \"Uncategorized\" + the type of the disaster for each missing subtype. We do not need to do this for disasters of type extreme temperature, since subtypes for all observations are specified"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "dis_all.loc[dis_all[\"type\"] == \"Earthquake\", \"subtype\"] = dis_all.loc[dis_all[\"type\"] == \"Earthquake\", \"subtype\"].fillna(\"Uncategorized_Earthquake\")\n",
    "dis_all.loc[dis_all[\"type\"] == \"Storm\", \"subtype\"] = dis_all.loc[dis_all[\"type\"] == \"Storm\", \"subtype\"].fillna(\"Uncategorized_Storm\")\n",
    "dis_all.loc[dis_all[\"type\"] == \"Flood\", \"subtype\"] = dis_all.loc[dis_all[\"type\"] == \"Flood\", \"subtype\"].fillna(\"Uncategorized_Flood\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We assume that disasters with no death toll reported have a death toll of 0. This also aligns with the information we get from the source of our dataset (deaths tolls smaller than 10 are not registered)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "dis_all[['deaths']] = dis_all[['deaths']].fillna(value=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Determine Regions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As mentioned in `data integration` we want to compute the region of each disaster, by taking the UN Region that is assigned to each Country, in which the disaster occurred. Some country codes are not in the list of UN countries, thus we handle them specifically later.\n",
    "\n",
    "Once more, here are the countries that need to be handled specifically:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countries in disaster, but not un dataset (12): \n",
      "['ANT', 'AZO', 'CSK', 'DDR', 'DFR', 'SCG', 'SPI', 'SUN', 'TWN', 'YMD', 'YMN', 'YUG']\n"
     ]
    }
   ],
   "source": [
    "dis_iso_codes = set(dis_all[\"country_code\"].unique())\n",
    "un_iso_codes = set(un_country_codes[\"ISO-alpha3 Code\"].tolist())\n",
    "emdat_without_un = dis_iso_codes-un_iso_codes\n",
    "print(f\"countries in disaster, but not un dataset ({len(emdat_without_un)}): \\n{sorted(emdat_without_un)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Automatically assign regions with UN Dataset**\n",
    "\n",
    "For countries that were split in the past, but are now unified, we can just assign the unified country."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Germany\n",
    "dis_all.loc[dis_all['country_code'] == \"DFR\",'country_code'] = \"DEU\"\n",
    "dis_all.loc[dis_all['country_code'] == \"DDR\",'country_code'] = \"DEU\"\n",
    "# Yemen\n",
    "dis_all.loc[dis_all['country_code'] == \"YMD\",'country_code'] = \"YEM\"\n",
    "dis_all.loc[dis_all['country_code'] == \"YMN\",'country_code'] = \"YEM\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we derive the region by using the UN dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "dis_all = pd.merge(dis_all, un_country_codes[[\"ISO-alpha3 Code\",\"Region Name\", \"Region Code\"]], left_on='country_code', right_on='ISO-alpha3 Code', how='left')\n",
    "dis_all.rename(columns={\"Region Code\": \"region_code\", \"Region Name\": \"region_name\"}, inplace=True)\n",
    "dis_all.drop(columns=[\"ISO-alpha3 Code\"],inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Manually Assign Regions**\n",
    "\n",
    "Some countries clearly belong to one region, so we can assign the disasters manually"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# Taiwan\n",
    "dis_all.loc[dis_all['country_code']=='TWN','region_name'] = 'Asia'\n",
    "dis_all.loc[dis_all['country_code']=='TWN','region_code'] = 142\n",
    "# Czechoslovakia\n",
    "dis_all.loc[dis_all['country_code']=='CSK','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='CSK','region_code'] = 150\n",
    "# Yugoslavia\n",
    "dis_all.loc[dis_all['country_code']=='YUG','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='YUG','region_code'] = 150\n",
    "# Serbia Montenegro\n",
    "dis_all.loc[dis_all['country_code']=='SCG','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='SCG','region_code'] = 150\n",
    "# Netherlands Antilles\n",
    "dis_all.loc[dis_all['country_code']=='ANT','region_name'] = 'Americas'\n",
    "dis_all.loc[dis_all['country_code']=='ANT','region_code'] = 19\n",
    "# Azores Islands\n",
    "dis_all.loc[dis_all['country_code']=='AZO','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='AZO','region_code'] = 150\n",
    "# Canary Islands\n",
    "dis_all.loc[dis_all['country_code']=='SPI','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='SPI','region_code'] = 150"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Manually Assign Regions of Soviet Union**\n",
    "\n",
    "For the Soviet Union disasters can occur in the european and/or asian parts. Thus we also take the \"location\" attribute into account and try to derive if the disaster was in europe or asia."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "mask_europe = dis_all.loc[dis_all['country_code']=='SUN'][\"location\"].str.contains(\"Russian Federation|Ukraine|Moldavia|Siberia\").fillna(False)\n",
    "mask_asia = dis_all.loc[dis_all['country_code']=='SUN'][\"location\"].str.contains(\"Kazakhstan|Azerbaijan|Uzbekistan|Turkmenistan|Georgia|Armenia|Kyrgystan|Tajikistan|Tajiskistan|Tadzhikistan|Tadjikistan|Caucasus region|Dushanbe\", case=False).fillna(False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "     year         dis_no  country_name country_code  \\\n849  1921  1921-9001-SUN  Soviet Union          SUN   \n\n                                              location        subgroup  \\\n849  South Ukraine, Volga, Ural (Kazakhstan,Russian...  Climatological   \n\n        type  subtype     deaths  dis_mag_value dis_mag_scale  start_year  \\\n849  Drought  Drought  1200000.0            NaN           Km2        1921   \n\n     end_year region_name  region_code  \n849      1921         NaN          NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>dis_no</th>\n      <th>country_name</th>\n      <th>country_code</th>\n      <th>location</th>\n      <th>subgroup</th>\n      <th>type</th>\n      <th>subtype</th>\n      <th>deaths</th>\n      <th>dis_mag_value</th>\n      <th>dis_mag_scale</th>\n      <th>start_year</th>\n      <th>end_year</th>\n      <th>region_name</th>\n      <th>region_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>849</th>\n      <td>1921</td>\n      <td>1921-9001-SUN</td>\n      <td>Soviet Union</td>\n      <td>SUN</td>\n      <td>South Ukraine, Volga, Ural (Kazakhstan,Russian...</td>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>Drought</td>\n      <td>1200000.0</td>\n      <td>NaN</td>\n      <td>Km2</td>\n      <td>1921</td>\n      <td>1921</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all[dis_all['country_code']=='SUN'][mask_europe & mask_asia]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Only one Event happened in both the asian and the european part of the soviet union. It is also a major event since it is a drought which caused the death of 1.2 million people.\n",
    "\n",
    "Researching the details of this event one can conclude that this observation can only be the [Russian famine of 1921â1922](https://en.wikipedia.org/wiki/Russian_famine_of_1921%E2%80%931922). It mostly affected people living in europe, hence we assign this single observation the region europe.\n",
    "\n",
    "For all other observations, the region should be unambiguous."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "dis_all.loc[(dis_all['country_code']=='SUN') & mask_europe, \"region_name\"] = \"Europe\"\n",
    "dis_all.loc[(dis_all['country_code']=='SUN') & mask_europe, \"region_code\"] = 150\n",
    "\n",
    "dis_all.loc[(dis_all['country_code']=='SUN') & mask_asia, \"region_name\"] = \"Asia\"\n",
    "dis_all.loc[(dis_all['country_code']=='SUN') & mask_asia, \"region_code\"] = 142\n",
    "\n",
    "dis_all.loc[dis_all['dis_no']=='1921-9001-SUN', \"region_name\"] = \"Europe\"\n",
    "dis_all.loc[dis_all['dis_no']=='1921-9001-SUN', \"region_code\"] = 150"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The only disasters without a region are now these 3 in the soviet Union. However, since they have no death count and are just 3 disaster, we can safely ignore them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "      year         dis_no  country_name country_code location        subgroup  \\\n3187  1981  1981-0280-SUN  Soviet Union          SUN     East  Meteorological   \n3188  1981  1981-0301-SUN  Soviet Union          SUN      NaN    Hydrological   \n3213  1982  1982-0346-SUN  Soviet Union          SUN     East    Hydrological   \n\n       type              subtype  deaths  dis_mag_value dis_mag_scale  \\\n3187  Storm  Uncategorized_Storm     0.0            NaN           Kph   \n3188  Flood  Uncategorized_Flood     0.0            NaN           Km2   \n3213  Flood  Uncategorized_Flood     0.0          150.0           Km2   \n\n      start_year  end_year region_name  region_code  \n3187        1981      1981         NaN          NaN  \n3188        1981      1981         NaN          NaN  \n3213        1982      1982         NaN          NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>dis_no</th>\n      <th>country_name</th>\n      <th>country_code</th>\n      <th>location</th>\n      <th>subgroup</th>\n      <th>type</th>\n      <th>subtype</th>\n      <th>deaths</th>\n      <th>dis_mag_value</th>\n      <th>dis_mag_scale</th>\n      <th>start_year</th>\n      <th>end_year</th>\n      <th>region_name</th>\n      <th>region_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3187</th>\n      <td>1981</td>\n      <td>1981-0280-SUN</td>\n      <td>Soviet Union</td>\n      <td>SUN</td>\n      <td>East</td>\n      <td>Meteorological</td>\n      <td>Storm</td>\n      <td>Uncategorized_Storm</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>Kph</td>\n      <td>1981</td>\n      <td>1981</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3188</th>\n      <td>1981</td>\n      <td>1981-0301-SUN</td>\n      <td>Soviet Union</td>\n      <td>SUN</td>\n      <td>NaN</td>\n      <td>Hydrological</td>\n      <td>Flood</td>\n      <td>Uncategorized_Flood</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>Km2</td>\n      <td>1981</td>\n      <td>1981</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3213</th>\n      <td>1982</td>\n      <td>1982-0346-SUN</td>\n      <td>Soviet Union</td>\n      <td>SUN</td>\n      <td>East</td>\n      <td>Hydrological</td>\n      <td>Flood</td>\n      <td>Uncategorized_Flood</td>\n      <td>0.0</td>\n      <td>150.0</td>\n      <td>Km2</td>\n      <td>1982</td>\n      <td>1982</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all[dis_all[\"region_code\"].isna()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Look at the preprocessed data\n",
    "\n",
    "The preprocessed disaster data now looks like this:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# Todo: Remove later\n",
    "dis_all.to_csv(DIS_PROCESSED_ALL_FILE, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "   year         dis_no country_name country_code                   location  \\\n0  1900  1900-9002-CPV   Cabo Verde          CPV                Countrywide   \n1  1900  1900-9001-IND        India          IND                     Bengal   \n2  1902  1902-0012-GTM    Guatemala          GTM  Quezaltenango, San Marcos   \n\n         subgroup        type          subtype     deaths  dis_mag_value  \\\n0  Climatological     Drought          Drought    11000.0            NaN   \n1  Climatological     Drought          Drought  1250000.0            NaN   \n2     Geophysical  Earthquake  Ground movement     2000.0            8.0   \n\n  dis_mag_scale  start_year  end_year region_name  region_code  \n0           Km2        1900      1900      Africa          2.0  \n1           Km2        1900      1900        Asia        142.0  \n2       Richter        1902      1902    Americas         19.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>dis_no</th>\n      <th>country_name</th>\n      <th>country_code</th>\n      <th>location</th>\n      <th>subgroup</th>\n      <th>type</th>\n      <th>subtype</th>\n      <th>deaths</th>\n      <th>dis_mag_value</th>\n      <th>dis_mag_scale</th>\n      <th>start_year</th>\n      <th>end_year</th>\n      <th>region_name</th>\n      <th>region_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1900</td>\n      <td>1900-9002-CPV</td>\n      <td>Cabo Verde</td>\n      <td>CPV</td>\n      <td>Countrywide</td>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>Drought</td>\n      <td>11000.0</td>\n      <td>NaN</td>\n      <td>Km2</td>\n      <td>1900</td>\n      <td>1900</td>\n      <td>Africa</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1900</td>\n      <td>1900-9001-IND</td>\n      <td>India</td>\n      <td>IND</td>\n      <td>Bengal</td>\n      <td>Climatological</td>\n      <td>Drought</td>\n      <td>Drought</td>\n      <td>1250000.0</td>\n      <td>NaN</td>\n      <td>Km2</td>\n      <td>1900</td>\n      <td>1900</td>\n      <td>Asia</td>\n      <td>142.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1902</td>\n      <td>1902-0012-GTM</td>\n      <td>Guatemala</td>\n      <td>GTM</td>\n      <td>Quezaltenango, San Marcos</td>\n      <td>Geophysical</td>\n      <td>Earthquake</td>\n      <td>Ground movement</td>\n      <td>2000.0</td>\n      <td>8.0</td>\n      <td>Richter</td>\n      <td>1902</td>\n      <td>1902</td>\n      <td>Americas</td>\n      <td>19.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_all.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Population Data\n",
    "\n",
    "The goal is to preprocess the population dataset. Specifically we want to create 3 datasets, one for the global level, one for the regional level, and one for the country level.\n",
    "\n",
    "We consider the period from 1900 until 2021 and remove redundant rows. We do this because the data after 2021 are based on estimated values, and are basically predictions into the future.\n",
    "\n",
    "Since the regions of the dataset did not match with the regions of our other datasets, we chose to compute the regions-population according to the regions from the un-country-codes.csv file.\n",
    "\n",
    "**How exactly is the population determined?**\n",
    "The primary data from the sources originates from census, informal census, indirect estimate and arbitrary guesses.\n",
    "For the years 1800-1950 the underlying data comes from the `Angus Maddison` dataset, and from 1950 onwards the data is provided by the World Population Prospects (WPP) by the UN. Therefore, we suspect that the data from 1950 onwards is more reliable, since it was collected in a more standardized manner by the UN.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Check for missing values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "geo           0\nname          0\ntime          0\nPopulation    0\ndtype: int64"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_global_df.isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "geo             0\nname            0\ntime            0\nPopulation    100\ndtype: int64"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_country_df.isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['hos'], dtype=object)"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_country_df[pop_country_df[\"Population\"].isna()][\"geo\"].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that only `Holy Sea` has missing values, we will keep this in mind, and come back later."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Does the regional data have missing values?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "geo           0\nname          0\ntime          0\nPopulation    0\ndtype: int64"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_region_gapminder.isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Preprocess global data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "     geo   name    time   Population\n0  world  world  1800.0  985083734.9\n1  world  world  1801.0  988518009.0\n2  world  world  1802.0  991993182.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>geo</th>\n      <th>name</th>\n      <th>time</th>\n      <th>Population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>world</td>\n      <td>world</td>\n      <td>1800.0</td>\n      <td>985083734.9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>world</td>\n      <td>world</td>\n      <td>1801.0</td>\n      <td>988518009.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>world</td>\n      <td>world</td>\n      <td>1802.0</td>\n      <td>991993182.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_global_df.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove & rename columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukas\\anaconda3\\envs\\dopp-a2\\lib\\site-packages\\pandas\\core\\frame.py:5039: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    },
    {
     "data": {
      "text/plain": "          population\nyear                \n1800.0  9.850837e+08\n1801.0  9.885180e+08\n1802.0  9.919932e+08\n1803.0  9.955096e+08\n1804.0  9.990675e+08\n...              ...\n2096.0  1.038852e+10\n2097.0  1.037989e+10\n2098.0  1.037045e+10\n2099.0  1.036027e+10\n2100.0  1.034932e+10\n\n[301 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>population</th>\n    </tr>\n    <tr>\n      <th>year</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1800.0</th>\n      <td>9.850837e+08</td>\n    </tr>\n    <tr>\n      <th>1801.0</th>\n      <td>9.885180e+08</td>\n    </tr>\n    <tr>\n      <th>1802.0</th>\n      <td>9.919932e+08</td>\n    </tr>\n    <tr>\n      <th>1803.0</th>\n      <td>9.955096e+08</td>\n    </tr>\n    <tr>\n      <th>1804.0</th>\n      <td>9.990675e+08</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2096.0</th>\n      <td>1.038852e+10</td>\n    </tr>\n    <tr>\n      <th>2097.0</th>\n      <td>1.037989e+10</td>\n    </tr>\n    <tr>\n      <th>2098.0</th>\n      <td>1.037045e+10</td>\n    </tr>\n    <tr>\n      <th>2099.0</th>\n      <td>1.036027e+10</td>\n    </tr>\n    <tr>\n      <th>2100.0</th>\n      <td>1.034932e+10</td>\n    </tr>\n  </tbody>\n</table>\n<p>301 rows Ã 1 columns</p>\n</div>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_global_df = pop_global_df[['time', 'Population']]\n",
    "pop_global_df.rename(columns={'Population': 'population', 'time': 'year'}, inplace=True)\n",
    "pop_global_df.set_index('year')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract population data from 1900 until 2021"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "pop_global_df = pop_global_df[pop_global_df['year'] >= 1900]\n",
    "pop_global_df = pop_global_df[pop_global_df['year'] <= 2021]\n",
    "pop_global_df['population'] = pop_global_df['population'].astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "       year  population\n100  1900.0  1627123965\n101  1901.0  1639684222\n102  1902.0  1652898195\n103  1903.0  1666760587\n104  1904.0  1680799825",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>100</th>\n      <td>1900.0</td>\n      <td>1627123965</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>1901.0</td>\n      <td>1639684222</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>1902.0</td>\n      <td>1652898195</td>\n    </tr>\n    <tr>\n      <th>103</th>\n      <td>1903.0</td>\n      <td>1666760587</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>1904.0</td>\n      <td>1680799825</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_global_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Prepare dataset for countries and regions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract population data from 1900 until 2021"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "pop_country_df = pop_country_df[pop_country_df['time'] >= 1900]\n",
    "pop_country_df = pop_country_df[pop_country_df['time'] <= 2021]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Derive the region of each country by using the UN dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "un_country_codes = un_country_codes[['Region Code', 'Region Name', 'ISO-alpha3 Code']]\n",
    "\n",
    "# format column that will be joined\n",
    "pop_country_df['geo'] = pop_country_df['geo'].str.upper()\n",
    "\n",
    "# merge country-population and un-country-codes\n",
    "pop_country_region_df = pd.merge(pop_country_df, un_country_codes, how='left', left_on='geo', right_on='ISO-alpha3 Code')\n",
    "pop_country_region_df.rename(columns={'Region Code': 'region_code', 'Region Name': 'region_name', 'time': 'year', 'Population': 'population', 'geo': 'country_code', 'name': 'country_name'}, inplace=True)\n",
    "\n",
    "# remove unused column 'ISO-alpha3 Code'\n",
    "pop_country_region_df.drop('ISO-alpha3 Code', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Manually add the region to the countries which are not included in the UN dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# holy see\n",
    "pop_country_region_df.loc[pop_country_region_df['country_code'] == 'HOS', 'region_code'] = 150\n",
    "pop_country_region_df.loc[pop_country_region_df['country_code'] == 'HOS', 'region_name'] = 'Europe'\n",
    "# taiwan\n",
    "pop_country_region_df.loc[pop_country_region_df['country_code'] == 'TWN', 'region_code'] = 142\n",
    "pop_country_region_df.loc[pop_country_region_df['country_code'] == 'TWN', 'region_name'] = 'Asia'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check if we worked correctly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_country_region_df[\"country_code\"].isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However `holy see` still has missing values as we saw earlier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "911.0"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_country_region_df[pop_country_region_df[\"country_code\"]==\"HOS\"][\"population\"].max()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The remaining missing values are population data for the country 'holy see' since the missing population is at maximum smaller than 1000 decided to exclude holy see from our analysis. Having no missing values helps us later, since can convert the datatype of the columns from float to integers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "pop_country_region_df = pop_country_region_df[pop_country_region_df[\"country_code\"] != \"HOS\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Adjust Datatypes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "pop_country_region_df[\"population\"] = pop_country_region_df[\"population\"].astype(np.int64)\n",
    "pop_country_region_df[\"year\"] = pop_country_region_df[\"year\"].astype(int)\n",
    "\n",
    "pop_global_df[\"year\"] = pop_global_df[\"year\"].astype(int)\n",
    "\n",
    "pop_region_gapminder[\"Population\"] = pop_region_gapminder[\"Population\"].astype(np.int64)\n",
    "pop_region_gapminder[\"time\"] = pop_region_gapminder[\"time\"].astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Derive regional & country data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "columns_regions = ['region_code', 'region_name', 'country_code', 'country_name', 'year', 'population']\n",
    "pop_regions_df = pop_country_region_df[columns_regions]\n",
    "pop_regions_df = pop_regions_df.groupby(['region_code', 'region_name', 'year'], as_index=False)['population'].sum()\n",
    "#pop_regions_df['population'] = pop_regions_df['population'].astype(np.int64)\n",
    "asia = pop_regions_df[pop_regions_df['region_name'] == 'Asia']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "columns_country = ['country_code', 'country_name', 'year', 'population']\n",
    "pop_country_df = pop_country_region_df[columns_country]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compare our calculated regional data to the gapminder\n",
    "\n",
    "To check if we aggregated our regional data correctly from the country data, we comapre it to the data from original regional data from gapminder."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "regions_population_2021_gapminder = pop_region_gapminder[pop_region_gapminder['time'] == 2021]\n",
    "total_population_gapminder = regions_population_2021_gapminder['Population'].sum()\n",
    "\n",
    "regions_population_2021_calc = pop_regions_df[pop_regions_df['year'] == 2021]\n",
    "total_population_calc = regions_population_2021_calc['population'].sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Total population from the year 2021"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gapminder:  7898737625\n",
      "Calculated: 7898737625\n"
     ]
    }
   ],
   "source": [
    "print(f'Gapminder:  {total_population_gapminder:.0f}')\n",
    "print(f'Calculated: {total_population_calc:.0f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gapminder population by region for 2021:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "          name  Population\n221     africa  1391823318\n522       asia  4634610444\n823     europe   846050284\n1124  americas  1026253579",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>Population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>221</th>\n      <td>africa</td>\n      <td>1391823318</td>\n    </tr>\n    <tr>\n      <th>522</th>\n      <td>asia</td>\n      <td>4634610444</td>\n    </tr>\n    <tr>\n      <th>823</th>\n      <td>europe</td>\n      <td>846050284</td>\n    </tr>\n    <tr>\n      <th>1124</th>\n      <td>americas</td>\n      <td>1026253579</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions_population_2021_gapminder[['name', 'Population']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our population by region for 2021:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "    region_name  population\n121      Africa  1391823318\n243     Oceania    43602426\n365    Americas  1026253579\n487        Asia  4693889556\n609      Europe   743168746",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>region_name</th>\n      <th>population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>121</th>\n      <td>Africa</td>\n      <td>1391823318</td>\n    </tr>\n    <tr>\n      <th>243</th>\n      <td>Oceania</td>\n      <td>43602426</td>\n    </tr>\n    <tr>\n      <th>365</th>\n      <td>Americas</td>\n      <td>1026253579</td>\n    </tr>\n    <tr>\n      <th>487</th>\n      <td>Asia</td>\n      <td>4693889556</td>\n    </tr>\n    <tr>\n      <th>609</th>\n      <td>Europe</td>\n      <td>743168746</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions_population_2021_calc[['region_name', 'population']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, we get the same results for the total populations in the year 2021.\n",
    "\n",
    "If we take a look at the region population, we see that the calculated population of america and africa shows no difference to the gapminder-dataset population. For the regions asia and europe, we get different results. Furthermore, the calculated region oceania does not exist in the gapminder dataset. Since the documentation of the gapminder dataset does not include the country-to-region assignment, we cannot compare the differences between our calculation and gapminder. However, since the overall score is equal, we can argue that those differences occur tue to different country-to-region assignments."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# Todo: Remove later\n",
    "pop_global_df.to_csv(POP_PROCESSED_GLOBAL_FILE, sep=';', index=False, header=True)\n",
    "#store in csv file\n",
    "pop_country_df.to_csv(POP_PROCESSED_COUNTRY_FILE, sep=';', index=False, header=True)\n",
    "#store in csv file\n",
    "pop_regions_df.to_csv(POP_PROCESSED_REGION_FILE, sep=';', index=False, header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Look at the preprocessed data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "     year  population\n100  1900  1627123965\n101  1901  1639684222\n102  1902  1652898195",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>100</th>\n      <td>1900</td>\n      <td>1627123965</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>1901</td>\n      <td>1639684222</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>1902</td>\n      <td>1652898195</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_global_df.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "  country_code country_name  year  population\n0          AFG  Afghanistan  1900     4707744\n1          AFG  Afghanistan  1901     4751177\n2          AFG  Afghanistan  1902     4802500",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country_code</th>\n      <th>country_name</th>\n      <th>year</th>\n      <th>population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1900</td>\n      <td>4707744</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1901</td>\n      <td>4751177</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1902</td>\n      <td>4802500</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_country_df.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "   region_code region_name  year  population\n0          2.0      Africa  1900   138578556\n1          2.0      Africa  1901   139018147\n2          2.0      Africa  1902   139489077",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>region_code</th>\n      <th>region_name</th>\n      <th>year</th>\n      <th>population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1900</td>\n      <td>138578556</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1901</td>\n      <td>139018147</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1902</td>\n      <td>139489077</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_regions_df.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Temperature Data\n",
    "\n",
    "The goal is to preprocess the temperature dataset. Specifically we want to create 3 datasets, one for the global level, one for the regional level, and one for the country level.\n",
    "\n",
    "Since the regions of the dataset did not match with the regions of our other datasets, we chose to derive the regions ourselves according to the UN dataset.\n",
    "\n",
    "We also need to do combine multiple files, since the data for each country and each region is located in a desperate file.\n",
    "\n",
    "Additionally, we aggregate the data by the year, since the other datasets also operate on a yearly basis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load country level data\n",
    "\n",
    "In this section we load Berkleys land-temperature data for each country from the corresponding .txt files and combine them into a single dataframe. We also change the naming of the countries according to the naming standard of the UN and add their country code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "temp_countries = temperature_countries_with_iso"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "  country_code country_name  year month  monthly_anomaly  monthly_unc  \\\n0          AFG  Afghanistan  1848     5           -0.297        2.037   \n1          AFG  Afghanistan  1848     6           -0.796        2.136   \n2          AFG  Afghanistan  1848     7           -0.113        1.937   \n3          AFG  Afghanistan  1848     8           -0.462        1.937   \n4          AFG  Afghanistan  1848     9           -1.272        1.865   \n\n   annual_anomaly  annual_unc  five_year_anomaly  five_year_unc  \\\n0             NaN         NaN                NaN            NaN   \n1             NaN         NaN                NaN            NaN   \n2          -0.777       0.639                NaN            NaN   \n3          -0.743       0.644                NaN            NaN   \n4          -0.676       0.669                NaN            NaN   \n\n   ten_year_anomaly  ten_year_unc  twenty_year_anomaly  twenty_year_unc  \n0               NaN           NaN                  NaN              NaN  \n1               NaN           NaN                  NaN              NaN  \n2               NaN           NaN                  NaN              NaN  \n3               NaN           NaN                  NaN              NaN  \n4               NaN           NaN                  NaN              NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country_code</th>\n      <th>country_name</th>\n      <th>year</th>\n      <th>month</th>\n      <th>monthly_anomaly</th>\n      <th>monthly_unc</th>\n      <th>annual_anomaly</th>\n      <th>annual_unc</th>\n      <th>five_year_anomaly</th>\n      <th>five_year_unc</th>\n      <th>ten_year_anomaly</th>\n      <th>ten_year_unc</th>\n      <th>twenty_year_anomaly</th>\n      <th>twenty_year_unc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1848</td>\n      <td>5</td>\n      <td>-0.297</td>\n      <td>2.037</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1848</td>\n      <td>6</td>\n      <td>-0.796</td>\n      <td>2.136</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1848</td>\n      <td>7</td>\n      <td>-0.113</td>\n      <td>1.937</td>\n      <td>-0.777</td>\n      <td>0.639</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1848</td>\n      <td>8</td>\n      <td>-0.462</td>\n      <td>1.937</td>\n      <td>-0.743</td>\n      <td>0.644</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1848</td>\n      <td>9</td>\n      <td>-1.272</td>\n      <td>1.865</td>\n      <td>-0.676</td>\n      <td>0.669</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually define the column names, since the raw data unfortunately no easy way to retrieve this information.\n",
    "temp_land_country_column_names = [\"country_code\", \"country_name\", \"year\", \"month\", \"monthly_anomaly\", \"monthly_unc\", \"annual_anomaly\", \"annual_unc\", \"five_year_anomaly\", \"five_year_unc\", \"ten_year_anomaly\", \"ten_year_unc\", \"twenty_year_anomaly\", \"twenty_year_unc\"]\n",
    "temp_country = pd.DataFrame(columns=temp_land_country_column_names)\n",
    "\n",
    "temp_country_names = temp_countries['Country']\n",
    "\n",
    "# Todo when combining use the dictionary in the data-integration notebook\n",
    "new_country_names = {\n",
    "    \"Denmark (Europe)\": \"Denmark\",\n",
    "    \"France (Europe)\": \"France\",\n",
    "    \"Netherlands (Europe)\": \"Netherlands\",\n",
    "    \"United Kingdom (Europe)\": \"United Kingdom\",\n",
    "    \"Ãland\": \"Ãland Islands\",\n",
    "    \"Czech Republic\": \"Czechia\",\n",
    "    \"Turkey\": \"TÃ¼rkiye\",\n",
    "    \"Svalbard and Jan Mayen\": \"Svalbard and Jan Mayen Islands\",\n",
    "    \"Cape Verde\": \"Cabo Verde\",\n",
    "    \"Turks and Caicas Islands\": \"Turks and Caicos Islands\",\n",
    "    \"Swaziland\": \"Eswatini\",\n",
    "    \"Macedonia\": \"North Macedonia\",\n",
    "    \"CÃ´te d'Ivoire\": \"CÃ´te dâIvoire\",\n",
    "    \"Federated States of Micronesia\": \"Micronesia (Federated States of)\",\n",
    "    \"South Georgia and the South Sandwich Isla\": \"South Georgia and the South Sandwich Islands\",\n",
    "    \"Bonaire, Saint Eustatius and Saba\": \"Bonaire, Sint Eustatius and Saba\",\n",
    "    \"Congo (Democratic Republic of the)\": \"Democratic Republic of the Congo\",\n",
    "    \"South Korea\": \"Korea, South\",\n",
    "    \"North Korea\": \"Korea, North\",\n",
    "    \"Palestina\": \"State of Palestine\"\n",
    "}\n",
    "\n",
    "def swap_keys_values(d):\n",
    "    return {v: k for k, v in d.items()}\n",
    "\n",
    "map_country_to_filename = swap_keys_values(new_country_names)\n",
    "\n",
    "for temp_country_name in temp_country_names:\n",
    "    temp_country_file_name = temp_country_name\n",
    "    if temp_country_name in map_country_to_filename:\n",
    "        temp_country_file_name = map_country_to_filename[temp_country_name]\n",
    "    path_temp_country_txt_file = TEMP_RAW_FOLDER + PATH_COUNTRIES_LAND_FOLDER + temp_country_file_name + '.txt'\n",
    "    temp_land_one_country = pd.read_csv(path_temp_country_txt_file, comment=\"%\", header=None, delim_whitespace=True)\n",
    "    country_iso_name = temp_countries.loc[temp_countries['Country'] == temp_country_name]['ISO-alpha3'].iloc[0]\n",
    "\n",
    "    temp_land_one_country.insert(0, 'country_code', country_iso_name)\n",
    "    temp_land_one_country.insert(1, 'country_name', temp_country_name)\n",
    "\n",
    "    temp_land_one_country.columns=temp_land_country_column_names\n",
    "\n",
    "    temp_country = pd.concat([temp_country, temp_land_one_country])\n",
    "\n",
    "temp_country.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Derive regional from country temperature\n",
    "\n",
    "**Why we are doing this:**\n",
    "Berkleys offers a dataset with regional temperature anomaly data, however because we also use datasets for natural disasters and population we have to ensure that regions in all datasets are including the same countries.\n",
    "As we saw in `data integration`, the regions defined for Berkleys regional temp anomaly datasets differ from the definition of regions in the UN dataset we use, therefore we decided to calculate regional temp anomalies based on the berkleys temp anomaly data for individual countries for regions based on the region data of the UN dataset.\n",
    "\n",
    "**How to aggregate temperature anomalies:**\n",
    "Temperature Anomalies tend to be similar across large regions geographical regions, even if the absolute temperature of two different measuring points differs for the same time period, their anomalies tend to be quite similar. To create regional temperature anomaly data we calculate the mean of temperature anomaly measures for all countries in a given region on a monthly basis. Source: https://data.giss.nasa.gov/gistemp/faq/#q101\n",
    "\n",
    "When computing a region it is possible that some countries are not included, because it is possible that countries started reporting earlier than others or stopped reporting for some time periods."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def calc_region_anomaly(temp_land_country_regions):\n",
    "    temp_return_land_region = temp_land_country_regions.groupby(['region_code', 'region_name', 'year', 'month'], as_index=False)['monthly_anomaly'].mean()\n",
    "    temp_return_land_region.rename({ 'monthly_anomaly': 'temperature_anomaly'}, axis=1,inplace=True)\n",
    "    return temp_return_land_region"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "   region_code region_name  year  month  temperature_anomaly\n0          2.0      Africa  1787      1               0.1520\n1          2.0      Africa  1787      2              -0.3142\n2          2.0      Africa  1787      3              -0.7876\n3          2.0      Africa  1787      4              -0.9146\n4          2.0      Africa  1787      5              -0.5570",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>region_code</th>\n      <th>region_name</th>\n      <th>year</th>\n      <th>month</th>\n      <th>temperature_anomaly</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1787</td>\n      <td>1</td>\n      <td>0.1520</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1787</td>\n      <td>2</td>\n      <td>-0.3142</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1787</td>\n      <td>3</td>\n      <td>-0.7876</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1787</td>\n      <td>4</td>\n      <td>-0.9146</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1787</td>\n      <td>5</td>\n      <td>-0.5570</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Todo: only load UN country codes once after combining notebook\n",
    "# load and format un-country-codes data\n",
    "un_country_codes = un_country_codes[['Region Code', 'Region Name', 'ISO-alpha3 Code']]\n",
    "un_country_codes.columns=['region_code', 'region_name', 'country_code']\n",
    "\n",
    "# Join UN country codes to temp data\n",
    "# exclude temp data for antarctica when calculating regional temp data\n",
    "temp_land_country_no_antarctica = temp_country[temp_country.country_name != 'Antarctica']\n",
    "temp_land_country_regions = pd.merge(temp_land_country_no_antarctica, un_country_codes, on='country_code', how='left')\n",
    "temp_land_country_regions.loc[temp_land_country_regions['country_name']=='Taiwan','region_name'] = 'Asia'\n",
    "temp_land_country_regions.loc[temp_land_country_regions['country_name']=='Taiwan','region_code'] = 142\n",
    "\n",
    "# calculate anomaly data for regions\n",
    "temp_region = calc_region_anomaly(temp_land_country_regions)\n",
    "\n",
    "temp_region.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load berkleys regional data\n",
    "\n",
    "We load Berkleys regional temperature anomaly data to compare it to the regional temperature anomaly data we calculated from the country temperature anomaly data and UN regions combined.\n",
    "\n",
    "We combine Berkleys North and South America region to one Region 'Americas' to be able to compare it to the combined 'Americas' region of the UN data later."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "  region_name  year  month  temperature_anomaly\n0      Africa  1880      8               -0.181\n1      Africa  1880      9               -0.389\n2      Africa  1880     10               -0.274",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>region_name</th>\n      <th>year</th>\n      <th>month</th>\n      <th>temperature_anomaly</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Africa</td>\n      <td>1880</td>\n      <td>8</td>\n      <td>-0.181</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Africa</td>\n      <td>1880</td>\n      <td>9</td>\n      <td>-0.389</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Africa</td>\n      <td>1880</td>\n      <td>10</td>\n      <td>-0.274</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Todo: make this code segment prettier; reuse other code segments\n",
    "\n",
    "temp_land_country_column_names = [\"region_name\", \"year\", \"month\", \"monthly_anomaly\", \"monthly_unc\", \"annual_anomaly\", \"annual_unc\", \"five_year_anomaly\", \"five_year_unc\", \"ten_year_anomaly\", \"ten_year_unc\", \"twenty_year_anomaly\", \"twenty_year_unc\"]\n",
    "temp_berkley_regions = ['Africa', 'Asia', 'Europe', 'North America', 'Oceania', 'South America']\n",
    "temp_berkleys_region = pd.DataFrame(columns=temp_land_country_column_names)\n",
    "\n",
    "for temp_berkley_region in temp_berkley_regions:\n",
    "    region_file_name = temp_berkley_region\n",
    "    path = TEMP_RAW_FOLDER + PATH_REGIONS_LAND_FOLDER + region_file_name + '.txt'\n",
    "    one_region = pd.read_csv(path, comment=\"%\", header=None, delim_whitespace=True)\n",
    "    one_region.insert(0, 'region_name', temp_berkley_region)\n",
    "    one_region.columns=temp_land_country_column_names\n",
    "    temp_berkleys_region = pd.concat([temp_berkleys_region, one_region])\n",
    "\n",
    "temp_berkleys_region.loc[(temp_berkleys_region['region_name'] == 'South America') | (temp_berkleys_region['region_name'] == 'North America'), 'region_name'] = 'Americas'\n",
    "temp_berkleys_region = temp_berkleys_region[['region_name', 'year', 'month', 'monthly_anomaly']]\n",
    "temp_berkleys_region = temp_berkleys_region.groupby(by=['region_name', 'year', 'month'])['monthly_anomaly'].mean()\n",
    "temp_berkleys_region = temp_berkleys_region.reset_index()\n",
    "temp_berkleys_region = temp_berkleys_region.rename(columns={'monthly_anomaly': 'temperature_anomaly'})\n",
    "\n",
    "temp_berkleys_region.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load global temperature data\n",
    "\n",
    "Two versions exist that treat temperature anomalies at locations with sea ice:\n",
    "1. Anomalies are extrapolated from land-surface air temperature anomalies.\n",
    "2. Anomalies are extrapolated from sea-surface water temperature anomalies (usually collected from open water areas on the periphery of the sea ice).\n",
    "\n",
    "We choose to use the air temperature version based on Berkleys remark:\n",
    "\"We believe that the use of air temperatures above sea ice provides a more natural means of describing changes in Earth's surface temperature.\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "   year  month  monthly_anomaly  monthly_unc  annual_anomaly  annual_unc  \\\n0  1850      1           -0.736        0.389             NaN         NaN   \n1  1850      2           -0.202        0.526             NaN         NaN   \n2  1850      3           -0.363        0.333             NaN         NaN   \n3  1850      4           -0.589        0.334             NaN         NaN   \n4  1850      5           -0.614        0.217             NaN         NaN   \n\n   five_year_anomaly  five_year_unc  ten_year_anomaly  ten_year_unc  \\\n0                NaN            NaN               NaN           NaN   \n1                NaN            NaN               NaN           NaN   \n2                NaN            NaN               NaN           NaN   \n3                NaN            NaN               NaN           NaN   \n4                NaN            NaN               NaN           NaN   \n\n   twenty_year_anomaly  twenty_year_unc  \n0                  NaN              NaN  \n1                  NaN              NaN  \n2                  NaN              NaN  \n3                  NaN              NaN  \n4                  NaN              NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>month</th>\n      <th>monthly_anomaly</th>\n      <th>monthly_unc</th>\n      <th>annual_anomaly</th>\n      <th>annual_unc</th>\n      <th>five_year_anomaly</th>\n      <th>five_year_unc</th>\n      <th>ten_year_anomaly</th>\n      <th>ten_year_unc</th>\n      <th>twenty_year_anomaly</th>\n      <th>twenty_year_unc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1850</td>\n      <td>1</td>\n      <td>-0.736</td>\n      <td>0.389</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1850</td>\n      <td>2</td>\n      <td>-0.202</td>\n      <td>0.526</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1850</td>\n      <td>3</td>\n      <td>-0.363</td>\n      <td>0.333</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1850</td>\n      <td>4</td>\n      <td>-0.589</td>\n      <td>0.334</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1850</td>\n      <td>5</td>\n      <td>-0.614</td>\n      <td>0.217</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only include the dataset where anomalies are extrapolated from land-surface air temperature anomalies.\n",
    "temp_global = pd.read_csv(TEMP_RAW_FOLDER + TEMP_GLOBAL_LAND_OCEAN_FILE, comment=\"%\", header=None, delim_whitespace=True, engine='python', skipfooter=2079)\n",
    "temp_global.columns = [\"year\", \"month\", \"monthly_anomaly\", \"monthly_unc\", \"annual_anomaly\", \"annual_unc\", \"five_year_anomaly\", \"five_year_unc\", \"ten_year_anomaly\", \"ten_year_unc\", \"twenty_year_anomaly\", \"twenty_year_unc\"]\n",
    "temp_global.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Remove unnecessary columns & rows"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We only keep entries after the year 1900 because our research questions focus on the past 100 years."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def cut_before_1900(temp_data):\n",
    "    return temp_data[temp_data['year'] >= 1900]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We only keep monthly anomaly data and rename the column to temperature_anomaly"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "def keep_monthly_anomalies(temp_data):\n",
    "    drop_labels = [\"monthly_unc\", \"annual_anomaly\", \"annual_unc\", \"five_year_anomaly\", \"five_year_unc\", \"ten_year_anomaly\", \"ten_year_unc\", \"twenty_year_anomaly\", \"twenty_year_unc\"]\n",
    "    temp_data = temp_data.drop(labels=drop_labels, axis=1)\n",
    "    return temp_data.rename(columns={'monthly_anomaly': 'temperature_anomaly'})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apply formatting functions for only keeping monthly anomalies and cut values before 1900."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# country temperature data\n",
    "temp_country = cut_before_1900(temp_country)\n",
    "temp_country = keep_monthly_anomalies(temp_country)\n",
    "temp_country = temp_country.groupby([\"year\",\"country_code\"]).agg({\"temperature_anomaly\":\"mean\",\"country_name\":\"first\"})\n",
    "temp_country.reset_index(inplace=True)\n",
    "# regional temperature data (ours & berkely)\n",
    "temp_region = cut_before_1900(temp_region)\n",
    "temp_berkleys_region = cut_before_1900(temp_berkleys_region)\n",
    "temp_region = temp_region.groupby([\"year\",\"region_code\"]).agg({\"temperature_anomaly\":\"mean\",\"region_name\":\"first\"})\n",
    "temp_region.reset_index(inplace=True)\n",
    "temp_berkleys_region.reset_index(inplace=True)\n",
    "temp_berkleys_region = temp_berkleys_region.groupby([\"year\", \"region_name\"]).agg({\"temperature_anomaly\":\"mean\"})\n",
    "temp_berkleys_region.reset_index(inplace=True)\n",
    "# global temperature data\n",
    "temp_global = cut_before_1900(temp_global)\n",
    "temp_global = keep_monthly_anomalies(temp_global)\n",
    "temp_global = temp_global.groupby([\"year\"])[\"temperature_anomaly\"].mean()\n",
    "temp_global = temp_global.reset_index()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compare berkleys and our regional data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "def temp_calc_region_corr(temp_single_region):\n",
    "    temp_land_single_region = temp_region[(temp_region['region_name'] == temp_single_region)].set_index(['year'])\n",
    "    temp_berkleys_land_single_region = temp_berkleys_region[(temp_berkleys_region['region_name'] == temp_single_region)].set_index(['year'])\n",
    "\n",
    "    comp_temp_land_africa_joined = temp_berkleys_land_single_region.join(temp_land_single_region, on=['year'], lsuffix='_berkleys', rsuffix='_self')\n",
    "\n",
    "    temp_land_region_corr = comp_temp_land_africa_joined['temperature_anomaly_berkleys'].corr(comp_temp_land_africa_joined['temperature_anomaly_self'])\n",
    "\n",
    "    print(f'Correlation for region {temp_single_region:<10}: {temp_land_region_corr:0.2f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation for region Africa    : 0.99\n",
      "Correlation for region Asia      : 0.87\n",
      "Correlation for region Europe    : 0.99\n",
      "Correlation for region Americas  : 0.94\n",
      "Correlation for region Oceania   : 0.85\n"
     ]
    }
   ],
   "source": [
    "temp_berkley_regions = ['Africa', 'Asia', 'Europe', 'Americas', 'Oceania']\n",
    "for temp_berkley_region in temp_berkley_regions:\n",
    "    temp_calc_region_corr(temp_berkley_region)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that there are some sligth differences, which is expectable since the region differ, as we saw in `data integration`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Handle Missing Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: 5\n",
      "Region:  0\n",
      "Global:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Country: \" + str(temp_country['temperature_anomaly'].isna().sum()))\n",
    "print(\"Region:  \" + str(temp_region['temperature_anomaly'].isna().sum()))\n",
    "print(\"Global:  \" + str(temp_global['temperature_anomaly'].isna().sum()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets look in which countries the missing values occur"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "country_code\nGUM    2\nPNG    2\nSLB    1\ndtype: int64"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_country[temp_country['temperature_anomaly'].isna()].groupby('country_code').size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We do simple linear interpolation to for these 5 missing data values. The reson for this is, that we assume that the temperature for a given year is related to its previous and following year. Also since we only have 5 instances, there wont be a large effect on the data as a whole."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "temp_country['temperature_anomaly'] = temp_country['temperature_anomaly'].interpolate(method='linear')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check if we have removed all missing values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_country[temp_country['temperature_anomaly'].isna()==True].size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Look at the preprocessed data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "# Todo: Remove later\n",
    "temp_country.to_csv(TEMP_PROCESSED_FOLDER + 'temp-land-country.csv', index=False)\n",
    "temp_region.to_csv(TEMP_PROCESSED_FOLDER + 'temp-land-region.csv', index=False)\n",
    "temp_global.to_csv(TEMP_PROCESSED_FOLDER + 'temp-land-ocean-global.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "   year country_code  temperature_anomaly country_name\n0  1900          ABW            -0.061750        Aruba\n1  1900          AFG            -0.427250  Afghanistan\n2  1900          AGO             0.146167       Angola",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>country_code</th>\n      <th>temperature_anomaly</th>\n      <th>country_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1900</td>\n      <td>ABW</td>\n      <td>-0.061750</td>\n      <td>Aruba</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1900</td>\n      <td>AFG</td>\n      <td>-0.427250</td>\n      <td>Afghanistan</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1900</td>\n      <td>AGO</td>\n      <td>0.146167</td>\n      <td>Angola</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_country.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "   year  region_code  temperature_anomaly region_name\n0  1900          2.0            -0.016705      Africa\n1  1900          9.0            -0.307362     Oceania\n2  1900         19.0            -0.160803    Americas",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>region_code</th>\n      <th>temperature_anomaly</th>\n      <th>region_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1900</td>\n      <td>2.0</td>\n      <td>-0.016705</td>\n      <td>Africa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1900</td>\n      <td>9.0</td>\n      <td>-0.307362</td>\n      <td>Oceania</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1900</td>\n      <td>19.0</td>\n      <td>-0.160803</td>\n      <td>Americas</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_region.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "   year  temperature_anomaly\n0  1900            -0.125167\n1  1901            -0.199333\n2  1902            -0.362167",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>temperature_anomaly</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1900</td>\n      <td>-0.125167</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1901</td>\n      <td>-0.199333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1902</td>\n      <td>-0.362167</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_global.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
