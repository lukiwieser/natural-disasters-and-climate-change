{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Climate Change and Deaths by Natural Disasters\n",
    "\n",
    "*Sebastian FÃ¼rndraht, Hannes Rokitte, Paul Schmitt, Lukas Wieser*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Overview\n",
    "- Introduction\n",
    "    - Research Questions\n",
    "    - Used Datasets\n",
    "    - Requirements & Dependencies\n",
    "    - Constants\n",
    "    - Download Temperature Data\n",
    "- Data Integration\n",
    "    - Preprocess CIA dataset\n",
    "    - Determine ISO codes for temperature data\n",
    "    - Which countries are in which datasets?\n",
    "    - Which regions are in which datasets?\n",
    "    - Which countries need to be manually assigned a region\n",
    "- Prepare Datasets\n",
    "    - ...\n",
    "- Data Exploration\n",
    "    - ...\n",
    "- Conclusion\n",
    "    - ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Used Datasets\n",
    "\n",
    "We have 3 main datasets. For disaster data we have EM-DAT. For determining the deaths per population of a natural disaster we also have the Gapminder Pupulation Dataset. And finally for analysing trends of natural disasters with climate change we have temperature data by Berkeley Earth Study.\n",
    "\n",
    "Additionally, we have 2 auxiliary datasets for countries (UN Country Codes and CIA Country Codes). The temperature dataset only contains country names, and these datasets help us to derive the country codes. We need both of them since the CIA dataset is a bit outdated, and the UN dataset does only contain official UN countries, e.g. it excludes Taiwan. We also use the UN Dataset to derive the region of each country.\n",
    "\n",
    "Overview:\n",
    "- EM-DAT:\n",
    "    - Yearly deaths by disasters\n",
    "    - from 1900-2022\n",
    "    - for 235 countries\n",
    "    - source: https://public.emdat.be/\n",
    "    - license: this is the only dataset, which is not allowed to be published.\n",
    "- Gapminder Population\n",
    "    - Yearly population\n",
    "    - Globally, by country, and by region\n",
    "    - from 1800-2100\n",
    "    - source: https://www.gapminder.org/data/documentation/gd003/\n",
    "- Berkeley Earth Study\n",
    "    - Monthly temperature anomalies\n",
    "    - Globally, by country, and by region\n",
    "    - from 1750-2020\n",
    "    - source: https://berkeleyearth.org/data/\n",
    "- UN Country Codes\n",
    "    - Country Names, ISO Codes, and Regions.\n",
    "    - 250 Countries/Regions\n",
    "    - source: https://unstats.un.org/unsd/methodology/m49/\n",
    "- CIA Country Codes\n",
    "    - Country Names and Country Codes\n",
    "    - 278 Countries/Regions\n",
    "    - source: https://www.cia.gov/the-world-factbook/references/country-data-codes/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Requirements & Dependencies\n",
    "\n",
    "This project was created using Python 3.9.\n",
    "The exact versions of the dependencies can be installed with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import requests\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# RAW\n",
    "DIS_RAW_FILE = Path('data/raw/disaster/emdat_public_2022_12_22_full.xlsx')\n",
    "TEMP_RAW_FOLDER = 'data/raw/temperature/'\n",
    "PATH_COUNTRIES_LAND_FOLDER = 'countries-land/'\n",
    "PATH_REGIONS_LAND_FOLDER = 'regions-land/'\n",
    "TEMP_GLOBAL_LAND_OCEAN_FILE = 'global-land-ocean.txt'\n",
    "TEMP_RAW_COUNTRIES_LIST_FILE = \"data/raw/temperature/countries-list.csv\"\n",
    "\n",
    "UN_COUNTRY_CODES_FILE = 'data/raw/country-codes/un-country-codes.csv'\n",
    "CIA_COUNTRY_CODES_FILE = \"data/raw/country-codes/cia-country-codes.csv\"\n",
    "\n",
    "# Preprocessed (to be deleted)\n",
    "DIS_PROCESSED_FOLDER = \"data/processed/disaster\"\n",
    "DIS_PROCESSED_ALL_FILE = Path(\"data/processed/disaster/disaster-all.csv\")\n",
    "\n",
    "\n",
    "PATH_COUNTRIES_LIST_FILE = 'temp-countries-list.csv'\n",
    "COUNTRIES_LIST_FILE = \"temp-countries-list.csv\"\n",
    "\n",
    "TEMP_PROCESSED_FOLDER = \"data/processed/temperature/\"\n",
    "TEMP_PROCESSED_COUNTRIES_LIST_FILE = \"data/processed/temperature/temp-countries-list.csv\"\n",
    "\n",
    "POP_PROCESSED_FOLDER = \"data/processed/population\"\n",
    "POP_PROCESSED_GLOBAL_FILE = 'data/processed/population/population-global.csv'\n",
    "POP_PROCESSED_REGION_FILE = 'data/processed/population/population-region.csv'\n",
    "POP_PROCESSED_COUNTRY_FILE = 'data/processed/population/population-country.csv'\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Directories"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Path(DIS_PROCESSED_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "Path(TEMP_PROCESSED_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "Path(POP_PROCESSED_FOLDER).mkdir(parents=True, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download Temperature Data\n",
    "Automatically download the regional and country temperature data, so we don't have to download each file by ourselves."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "countries = pd.read_csv(TEMP_RAW_COUNTRIES_LIST_FILE, sep=\";\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_regions = countries[\"Region\"].dropna().unique().tolist()\n",
    "temp_countries = countries[\"Country\"].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def download_temperature_countries(country_names: list[str]):\n",
    "    for country in country_names:\n",
    "        print(f\"downloading {country}\")\n",
    "        country_encoded = urllib.parse.quote(country.lower().replace(\" \", \"-\"),encoding='cp1252')\n",
    "        url = f\"http://berkeleyearth.lbl.gov/auto/Regional/TAVG/Text/{country_encoded}-TAVG-Trend.txt\"\n",
    "        response = requests.get(url)\n",
    "        data = response.text\n",
    "        with open(f'data/raw/temperature/countries-land/{country}.txt', 'w', encoding=\"utf-8\") as file:\n",
    "            file.write(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def download_temperature_regions(region_names: list[str]):\n",
    "    for region in region_names:\n",
    "        print(f\"downloading {region}\")\n",
    "        region_encoded = urllib.parse.quote(region.lower().replace(\" \", \"-\"),encoding='cp1252')\n",
    "        url = f\"http://berkeleyearth.lbl.gov/auto/Regional/TAVG/Text/{region_encoded}-TAVG-Trend.txt\"\n",
    "        response = requests.get(url)\n",
    "        data = response.text\n",
    "        with open(f'data/raw/temperature/regions-land/{region}.txt', 'w', encoding=\"utf-8\") as file:\n",
    "            file.write(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Change the variable `should_download_temperature_data` to `True`, to download the temperature data of countries & regions. (This should not be necessary, since the data should already be downloaded)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "should_download_temperature_data = False\n",
    "if should_download_temperature_data:\n",
    "    download_temperature_countries(temp_countries)\n",
    "    download_temperature_regions(temp_regions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Raw Datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis = pd.read_excel(DIS_RAW_FILE, skiprows=6, sheet_name=\"emdat data\")\n",
    "\n",
    "temperature_countries = pd.read_csv(\"data/raw/temperature/countries-list.csv\", sep=\";\")\n",
    "\n",
    "pop_dict = pd.read_excel('data/raw/population/gapminder-population-v7.xlsx', sheet_name=['data-for-world-by-year', 'data-for-regions-by-year', 'data-for-countries-etc-by-year'])\n",
    "pop_global_df = pop_dict.get('data-for-world-by-year')\n",
    "pop_country_df = pop_dict.get('data-for-countries-etc-by-year')\n",
    "pop_region_gapminder = pop_dict.get('data-for-regions-by-year')\n",
    "\n",
    "un_country_codes = pd.read_csv(UN_COUNTRY_CODES_FILE, sep=\";\")\n",
    "cia_country_codes = pd.read_csv(CIA_COUNTRY_CODES_FILE, sep=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Data Integration\n",
    "\n",
    "In this section we want to ensure, that we can later combine our datasets without many problems.\n",
    "For this we first add country codes to the temperature dataset, and then take a closer look which countries and regions occur in each dataset.\n",
    "\n",
    "### 2.1 Preprocess CIA dataset\n",
    "\n",
    "First do some preprocessing on the CIA dataset, since it is a bit messy as we can se below.\n",
    "For example different kind of country codes are stored in the same column."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cia_country_codes.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# split iso-codes in separate columns\n",
    "cia_country_codes[[\"ISO-alpha2\",\"ISO-alpha3\",\"ISO-numeric\"]] = cia_country_codes[\"ISO 3166\"].str.split(\"|\",2,expand=True)\n",
    "cia_country_codes.drop(columns=[\"ISO 3166\"], inplace=True)\n",
    "# strip whitespaces from iso-codes\n",
    "cia_country_codes[[\"ISO-alpha2\",\"ISO-alpha3\",\"ISO-numeric\"]] = cia_country_codes[[\"ISO-alpha2\",\"ISO-alpha3\",\"ISO-numeric\"]].apply(lambda x: x.str.strip())\n",
    "# replace not existing iso-codes with NaN for more clarity\n",
    "cia_country_codes[\"ISO-alpha2\"].replace(\"-\", np.nan, inplace=True)\n",
    "cia_country_codes[\"ISO-alpha3\"].replace(\"-\", np.nan, inplace=True)\n",
    "cia_country_codes[\"ISO-numeric\"].replace(\"-\", np.nan, inplace=True)\n",
    "# show preprocessed cia data\n",
    "cia_country_codes.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Determine ISO codes for temperature data\n",
    "\n",
    "Next we want to add ISO codes to the temperature dataset, since the temperature dataset only contains country names.\n",
    "This will help us later, when we want to relate countries in the disaster dataset to countries in the temperature dataset.\n",
    "\n",
    "#### Remove aggregated countries.\n",
    "\n",
    "We can see, that e.g. Denmark appears twice. This issue happens multiple times, and is due to the reason that some countries are aggregates of other countries e.g. `Denmark` consists of `Denmark (Europe)` also known as `Denmark Mainland`, and `Greenland`. The bearkley earth website has a worldmap on which the country is highlighted, this helped us to better understand what each of the conflicting countries is."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temperature_countries.iloc[55: 55+5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We decided to remove the \"aggregated\" country. Here is a list of the aggregate countries we removed, their individual parts still exists in the dataset:\n",
    "- Denmark (Denmark Mainland, Greenland)\n",
    "- France (France Mainland, French Guiana, French Polynesia, French Southern and Antarctic Lands)\n",
    "- Netherlands (Netherlands Mainland, Sint Maarten, CuraÃ§ao, Aruba)\n",
    "- United Kingdom (United Kingdom + Oversea territories such as Montserrat, Bermuda)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temperature_countries_remove = pd.DataFrame({\n",
    "    \"Country\": [\"Denmark\",\"France\", \"Netherlands\", \"United Kingdom\"],\n",
    "    \"Region\": [\"North America\", np.nan, \"Europe\", \"Europe\"]\n",
    "})\n",
    "temperature_countries_cleaned = pd.concat([temperature_countries, temperature_countries_remove]).drop_duplicates(keep=False)\n",
    "temperature_countries_cleaned.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rename Countries & Match ISO Codes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# some countries need to be renamed so that we find the matching country-code later\n",
    "new_country_names = {\n",
    "    \"Denmark (Europe)\": \"Denmark\",\n",
    "    \"France (Europe)\": \"France\",\n",
    "    \"Netherlands (Europe)\": \"Netherlands\",\n",
    "    \"United Kingdom (Europe)\": \"United Kingdom\",\n",
    "    \"Ãland\": \"Ãland Islands\",\n",
    "    \"Czech Republic\": \"Czechia\",\n",
    "    \"Turkey\": \"TÃ¼rkiye\",\n",
    "    \"Svalbard and Jan Mayen\": \"Svalbard and Jan Mayen Islands\",\n",
    "    \"Cape Verde\": \"Cabo Verde\",\n",
    "    \"Turks and Caicas Islands\": \"Turks and Caicos Islands\",\n",
    "    \"Swaziland\": \"Eswatini\",\n",
    "    \"Macedonia\": \"North Macedonia\",\n",
    "    \"CÃ´te d'Ivoire\": \"CÃ´te dâIvoire\",\n",
    "    \"Federated States of Micronesia\": \"Micronesia (Federated States of)\",\n",
    "    \"South Georgia and the South Sandwich Isla\": \"South Georgia and the South Sandwich Islands\",\n",
    "    \"Bonaire, Saint Eustatius and Saba\": \"Bonaire, Sint Eustatius and Saba\",\n",
    "    \"Congo (Democratic Republic of the)\": \"Democratic Republic of the Congo\",\n",
    "    \"South Korea\": \"Korea, South\",\n",
    "    \"North Korea\": \"Korea, North\",\n",
    "    \"Palestina\": \"State of Palestine\"\n",
    "}\n",
    "\n",
    "temperature_countries_cleaned = temperature_countries_cleaned.replace({\"Country\": new_country_names}, inplace=False)\n",
    "\n",
    "# left-join cia-country-codes and un-country-codes\n",
    "temperature_countries_with_iso = temperature_countries_cleaned.merge(cia_country_codes,how=\"left\",left_on='Country', right_on='Entity')[[\"Country\",\"ISO-alpha3\"]]\n",
    "temperature_countries_with_iso = temperature_countries_with_iso.merge(un_country_codes,how=\"left\",left_on='Country', right_on='Country or Area')[[\"Country\",\"ISO-alpha3\", \"ISO-alpha3 Code\"]]\n",
    "\n",
    "# fill missing cia-country codes with un-country-codes\n",
    "temperature_countries_with_iso[\"ISO-alpha3\"].fillna(temperature_countries_with_iso[\"ISO-alpha3 Code\"], inplace=True)\n",
    "temperature_countries_with_iso.drop(columns=[\"ISO-alpha3 Code\"], inplace=True)\n",
    "\n",
    "# show countries for which we could not find an ISO code\n",
    "temperature_countries_with_iso[temperature_countries_with_iso[\"ISO-alpha3\"].isna()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These 3 countries/areas do not have any country codes in general, and are quite small, so we just ignore them later on."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Which countries are in which datasets?\n",
    "\n",
    "Next we take a look, if the datasets all have the same countries, or if there are countries that only exist in one dataset.\n",
    "\n",
    "#### Disaster vs. Temperature Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "berkely_iso_codes = set(temperature_countries_with_iso[\"ISO-alpha3\"].dropna().tolist())\n",
    "emdat_iso_codes = set(dis[\"ISO\"].unique().tolist())\n",
    "\n",
    "emdat_and_bekely = emdat_iso_codes.intersection(berkely_iso_codes)\n",
    "emdat_without_berkely = emdat_iso_codes-emdat_and_bekely\n",
    "berkely_without_emdat = berkely_iso_codes-emdat_and_bekely\n",
    "\n",
    "print(f\"countries in emdat & berkely: {len(emdat_and_bekely)}\")\n",
    "print(f\"countries in emdat but not berkely ({len(emdat_without_berkely)}):\")\n",
    "print(sorted(emdat_without_berkely))\n",
    "print(f\"countries in berkely but not emdat ({len(berkely_without_emdat)}):\")\n",
    "print(sorted(berkely_without_emdat))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Countries for which we have disaster data, but no temperature data are as follows:\n",
    "- Existing Countries (usually very small countries/islands):\n",
    "     - `AZO` Azores Islands, `BMU` Bermuda, `BRN` Brunei Darussalam, `COK` Cook Islands (the), `MDV` Maldives, `MHL` Marshall Islands (the), `SHN` Saint Helena, Ascension and Tristan da Cunha, `SSD` South Sudan, `TKL` Tokelau, `TUV` Tuvalu, `VUT` Vanuatu, `WLF` Wallis and Futuna\n",
    "- Existing Countries (but invalid country code):\n",
    "    - `SPI` Canary Islands\n",
    "- Former Countries:\n",
    "    - `ANT` Netherlands Antilles,`CSK` Czechoslovakia,`DDR` Germany Dem Rep,`DFR` Germany Fed Rep,`SCG` Serbia Montenegro,`SUN` Soviet Union,`YMD` Yemen P Dem Rep,`YMN` Yemen Arab Rep,`YUG` Yugoslavia"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Disaster vs Population Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gapminder_iso_codes = set(pop_country_df[\"geo\"].str.upper().unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "emdat_and_gapminder = emdat_iso_codes.intersection(gapminder_iso_codes)\n",
    "emdat_without_gapminder = emdat_iso_codes-emdat_and_gapminder\n",
    "gapminder_without_emdat = gapminder_iso_codes-emdat_and_gapminder\n",
    "\n",
    "print(f\"countries in emdat & gapminder: {len(emdat_and_gapminder)}\")\n",
    "print(f\"countries in emdat but not gapminder ({len(emdat_without_gapminder)}):\")\n",
    "print(sorted(emdat_without_gapminder))\n",
    "print(f\"countries in gapminder but not emdat ({len(gapminder_without_emdat)}):\")\n",
    "print(sorted(gapminder_without_emdat))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Countries for which we have disaster data, but no population data are as follows:\n",
    "- Existing Countries (independent)\n",
    "    - `COK` Cook Islands (the), `NIU` Niue\n",
    "- Existing Countries (dependent e.g .oversea territories)\n",
    "\t- `AIA` Anguilla, `ASM` American Samoa, `AZO` Azores Islands, `BLM` Saint BarthÃ©lemy, `BMU` Bermuda, `CUW` CuraÃ§ao, `CYM` Cayman Islands (the), `GLP` Guadeloupe, `GUF` French Guiana, `GUM` Guam, `IMN` Isle of Man, `MAC` Macao, `MAF` Saint Martin (French Part), `MNP` Northern Mariana Islands (the), `MSR` Montserrat, `MTQ` Martinique, `MYT` Mayotte, `NCL` New Caledonia, `PRI` Puerto Rico, `PYF` French Polynesia, `REU` RÃ©union, `SHN` Saint Helena, Ascension and Tristan da Cunha, `SPI` Canary Islands, `SXM` Sint Maarten (Dutch part), `TCA` Turks and Caicos Islands (the), `TKL` Tokelau, `VGB` Virgin Island (British), `VIR` Virgin Island (U.S.), `WLF` Wallis and Futuna\n",
    "- Former Countries\n",
    "\t- `ANT` Netherlands Antilles, `CSK` Czechoslovakia, `DDR` Germany Dem Rep, `DFR` Germany Fed Rep, `SCG` Serbia Montenegro, `SUN` Soviet Union, `YMD` Yemen P Dem Rep, `YMN` Yemen Arab Rep, `YUG` Yugoslavia"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When comparing data on the country level, we should keep in mind, that (1) countries can change over time, and (2) that for some countries should only occur in one dataset and thus should probably be ignored when trying to relate disasters with climate change.\n",
    "\n",
    "We can later also handle former countries e.g. merge `DDR` and `DFR` into `DEU`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Which regions are in which datasets?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mdis\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mContinent\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39munique()\u001B[38;5;241m.\u001B[39mtolist()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'dis' is not defined"
     ]
    }
   ],
   "source": [
    "dis[\"Continent\"].unique().tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pop_region_gapminder[\"geo\"].unique().tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temperature_countries[\"Region\"].dropna().unique().tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, each dataset has a different number of regions. Additionally, we don't know which countries belong to each region. For example the region Europe could consist of different countries the disaster dataset than in the temperature dataset.\n",
    "\n",
    "That's why we decided to compute the regional data ourselves, by aggregating the countries according to UN Regions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5 Which countries need to be manually assigned a region"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check which countries are not in the UN Dataset, and thus need to be manually assigned a region:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "un_iso_codes = set(un_country_codes[\"ISO-alpha3 Code\"].tolist())\n",
    "\n",
    "emdat_without_un = emdat_iso_codes-un_iso_codes\n",
    "gapminder_without_un = gapminder_iso_codes-un_iso_codes\n",
    "berkely_without_un = berkely_iso_codes-un_iso_codes\n",
    "\n",
    "print(f\"countries in emdat, but not un ({len(emdat_without_un)}): \\n{emdat_without_un}\")\n",
    "print(f\"countries in gapminder, but not un ({len(gapminder_without_un)}): \\n{gapminder_without_un}\")\n",
    "print(f\"countries in berkely, but not un ({len(berkely_without_un)}): \\n{berkely_without_un}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Prepare Datasets\n",
    "\n",
    "In this section we apply preprocessing to our 3 datasets.\n",
    "This includes standardizing the column names, removing unnecessary columns, removing rows, handling missing values, deriving the region of each country, etc.\n",
    "\n",
    "### 3.1 Disaster Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Task:\n",
    "\n",
    "The goal is to convert the data into the following formats for later use.\n",
    "Along the way, this notebook does some data-preparation\n",
    "\n",
    "\n",
    "Disaster-All\n",
    "disaster/disaster-all:\n",
    "Columns: disaster_no, year, subgroup, type, total_deaths, dis_mag_value, dis_mag_scale, start_year, end_year\n",
    "Other interesting columns?\n",
    "\n",
    "Prefix: dis\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Publisher: Centre for Research on the Epidemiology of Disasters (CRED)\n",
    "\n",
    "CRED defines a disaster as âa situation or event that overwhelms local capacity, necessitating a\n",
    "request at the national or international level for external assistance; an unforeseen and often sudden\n",
    "event that causes great damage, destruction and human sufferingâ\n",
    "\n",
    "For a disaster to be entered into the database at least one of the following criteria must be fulfilled:\n",
    "\n",
    "- 10 or more people reported killed\n",
    "- 100 or more people reported affected\n",
    "- Declaration of a state of emergency\n",
    "- Call for international assistance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First look at the disaster data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Select & Rename Attributes\n",
    "\n",
    "1. Replace whitespaces with underscores\n",
    "2. Convert every character to lowercase\n",
    "3. Rename specific columns to ensure uniformity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove whitespaces from all col-names and convert them to lower-case\n",
    "dis.columns = [c.replace(' ', '_').lower() for c in dis.columns]\n",
    "dis.rename(columns={'country':'country_name', 'iso':'country_code', 'disaster_group':'group','disaster_subgroup':'subgroup', 'disaster_subtype':'subtype', 'disaster_type':'type', 'total_deaths':'deaths'}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select the most interesting columns\n",
    "dis_all_col_names = [\"year\", \"dis_no\", \"country_name\", \"country_code\", \"location\",\"group\",\"subgroup\", \"type\", \"subtype\", \"deaths\", \"dis_mag_value\", \"dis_mag_scale\", \"start_year\", \"end_year\"]\n",
    "dis_all = dis.filter(items=dis_all_col_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Which disaster groups are present in the dataset ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are Natural disasters, technological disasters as well as complex disasters that represent specific events (e.g. famine) which are not directly linked to a natural hazard."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis_all[\"group\"].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We only focus on disasters which have a natural causation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis_all = dis_all[dis_all[\"group\"] == \"Natural\"].copy()\n",
    "dis_all.drop(columns=\"group\", inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Which types of natural disasters are there ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis_all.groupby([\"subgroup\",\"type\"]).agg({\"deaths\":\"sum\"}).reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The types of disasters are mostly the ones a normal person would expect when thinking about natural disasters. But there are some strange types like insect-infestations or animal-accident which are not that obvious to understand, they also have basically no deaths. Also for our research we want to exclude Epidemics since it would go beyond the scope of this task.\n",
    "\n",
    "Therefore also decided to omit disasters of the subgroups `Biological` and `Extra-terrestrial`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis_all = dis_all[(dis_all[\"subgroup\"] != \"Biological\") & (dis_all[\"subgroup\"] != \"Extra-terrestrial\")]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We only consider the following types of disasters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis_all[\"type\"].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Handle Missing Values\n",
    "\n",
    "Fill missing Values for the number of deaths\n",
    "\n",
    "We can assume that missing values for the number of deaths of a particular disaster means that the deathtoll was 0.\n",
    "\n",
    "For the subtype we take a look for which type of natural disasters a subtype is not provided."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis_all.isna().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis_all[dis_all[\"subtype\"].isna()][\"type\"].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unfortunately the missing values in the subtype column do not correspond to specific types of disasters.\n",
    "We can not conclude that easily what caused the values to be missing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis_all[dis_all[\"type\"] == \"Drought\"].subtype.unique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Disasters of type drought have no further more specific subtype.\n",
    "Consequently, we assign all droughts with missing values for subtype, the general type"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis_all.loc[dis_all[\"type\"] == \"Drought\", \"subtype\"] = dis_all.loc[dis_all[\"type\"] == \"Drought\", \"subtype\"].fillna(\"Drought\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For all other types, we unfortunately can not guess the subtype.\n",
    "Therefore, we assign a custom label \"Uncategorized\" + the type of the disaster for each missing subtype\n",
    "We do not need to do this for disasters of type extreme temperature, since subtypes for all observations are specified"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis_all.loc[dis_all[\"type\"] == \"Earthquake\", \"subtype\"] = dis_all.loc[dis_all[\"type\"] == \"Earthquake\", \"subtype\"].fillna(\"Uncategorized_Earthquake\")\n",
    "dis_all.loc[dis_all[\"type\"] == \"Storm\", \"subtype\"] = dis_all.loc[dis_all[\"type\"] == \"Storm\", \"subtype\"].fillna(\"Uncategorized_Storm\")\n",
    "dis_all.loc[dis_all[\"type\"] == \"Flood\", \"subtype\"] = dis_all.loc[dis_all[\"type\"] == \"Flood\", \"subtype\"].fillna(\"Uncategorized_Flood\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We assume that disasters with no death toll reported have a death toll of 0.\n",
    "This also aligns with the information we get from emdat (deaths < 10 are missing)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis_all[['deaths']] = dis_all[['deaths']].fillna(value=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Determine Regions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As mentioned in `data integration` we want to compute the region of each disaster, by taking the UN Region that is assigned to each Country, in which the disaster occurred.\n",
    "Some country codes are not in the list of UN countries, thus we handle them specifically."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis_iso_codes = set(dis_all[\"country_code\"].unique())\n",
    "un_iso_codes = set(un_country_codes[\"ISO-alpha3 Code\"].tolist())\n",
    "emdat_without_un = dis_iso_codes-un_iso_codes\n",
    "print(f\"countries in emdat, but not un ({len(emdat_without_un)}): \\n{emdat_without_un}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Automatically assign regions with UN Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For countries that were split in the past, but are now unified, we can just assign the unified country."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Germany\n",
    "dis_all.loc[dis_all['country_code'] == \"DFR\",'country_code'] = \"DEU\"\n",
    "dis_all.loc[dis_all['country_code'] == \"DDR\",'country_code'] = \"DEU\"\n",
    "# Yemen\n",
    "dis_all.loc[dis_all['country_code'] == \"YMD\",'country_code'] = \"YEM\"\n",
    "dis_all.loc[dis_all['country_code'] == \"YMN\",'country_code'] = \"YEM\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we determine the region by using the un dataset:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis_all = pd.merge(dis_all, un_country_codes[[\"ISO-alpha3 Code\",\"Region Name\", \"Region Code\"]], left_on='country_code', right_on='ISO-alpha3 Code', how='left')\n",
    "dis_all.rename(columns={\"Region Code\": \"region_code\", \"Region Name\": \"region_name\"}, inplace=True)\n",
    "dis_all.drop(columns=[\"ISO-alpha3 Code\"],inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Manually Assign Regions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some countries clearly belong to one region, so we can assign the disasters manually"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Taiwan\n",
    "dis_all.loc[dis_all['country_code']=='TWN','region_name'] = 'Asia'\n",
    "dis_all.loc[dis_all['country_code']=='TWN','region_code'] = 142\n",
    "# Czechoslovakia\n",
    "dis_all.loc[dis_all['country_code']=='CSK','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='CSK','region_code'] = 150\n",
    "# Yugoslavia\n",
    "dis_all.loc[dis_all['country_code']=='YUG','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='YUG','region_code'] = 150\n",
    "# Serbia Montenegro\n",
    "dis_all.loc[dis_all['country_code']=='SCG','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='SCG','region_code'] = 150\n",
    "# Netherlands Antilles\n",
    "dis_all.loc[dis_all['country_code']=='ANT','region_name'] = 'Americas'\n",
    "dis_all.loc[dis_all['country_code']=='ANT','region_code'] = 19\n",
    "# Azores Islands\n",
    "dis_all.loc[dis_all['country_code']=='AZO','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='AZO','region_code'] = 150\n",
    "# Canary Islands\n",
    "dis_all.loc[dis_all['country_code']=='SPI','region_name'] = 'Europe'\n",
    "dis_all.loc[dis_all['country_code']=='SPI','region_code'] = 150"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Manually Assign Regions of Disasters Soviet Union"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the Soviet Union disasters can occur in the european and/or asian parts.\n",
    "Thus we also take the \"location\" attribute into account and try to derive if the disaster was in europe or asia."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mask_europe = dis_all.loc[dis_all['country_code']=='SUN'][\"location\"].str.contains(\"Russian Federation|Ukraine|Moldavia|Siberia\").fillna(False)\n",
    "mask_asia = dis_all.loc[dis_all['country_code']=='SUN'][\"location\"].str.contains(\"Kazakhstan|Azerbaijan|Uzbekistan|Turkmenistan|Georgia|Armenia|Kyrgystan|Tajikistan|Tajiskistan|Tadzhikistan|Tadjikistan|Caucasus region|Dushanbe\", case=False).fillna(False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis_all[dis_all['country_code']=='SUN'][mask_europe & mask_asia]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Only one Event happened in both the asian as well as the european part of the soviet union.\n",
    "It is also a major event since it is a drought which caused the death of 1.2 million people.\n",
    "\n",
    "Researching the details of this event one can conclude that this observation can only be the Russian famine of 1921â1922.\n",
    "It mostly affected people living in europe, hence we assign this single observation the region europe.\n",
    "(https://en.wikipedia.org/wiki/Russian_famine_of_1921%E2%80%931922)\n",
    "\n",
    "For all other observations, the region should be unambiguous."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis_all.loc[(dis_all['country_code']=='SUN') & mask_europe, \"region_name\"] = \"Europe\"\n",
    "dis_all.loc[(dis_all['country_code']=='SUN') & mask_europe, \"region_code\"] = 150\n",
    "\n",
    "dis_all.loc[(dis_all['country_code']=='SUN') & mask_asia, \"region_name\"] = \"Asia\"\n",
    "dis_all.loc[(dis_all['country_code']=='SUN') & mask_asia, \"region_code\"] = 142\n",
    "\n",
    "dis_all.loc[dis_all['dis_no']=='1921-9001-SUN', \"region_name\"] = \"Europe\"\n",
    "dis_all.loc[dis_all['dis_no']=='1921-9001-SUN', \"region_code\"] = 150"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The only disasters without a region are now these 3 in the soviet Union. However, since they have no death count we can safely ignore them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis_all"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Save File"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Todo: Remove later\n",
    "dis_all.to_csv(DIS_PROCESSED_ALL_FILE, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis_all.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Population Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Population data\n",
    "\n",
    "The population dataset by gapminder is a dataset containing information about the worlds population from 1800 until 2100.\n",
    "The data is a composition from different sources. There a two main sources - a dataset by Angus Maddison and the CLIO Infra Project -  and the World Population Prospects (WPP) provided by the UN.\n",
    "The dataset by Angus Maddison provides the data for the years 1800 - 1950. Population data after 1950 was taken from the WPP dataset.\n",
    "Additional sources were used, to fill missing values for years or regions. A details list of sources can be found in the documentation. (https://www.gapminder.org/data/documentation/gd003/)\n",
    "The primary data from the sources originates from census, informal census, indirect estimate and arbitrary guesses.\n",
    "\n",
    "Modifications and Estimations:\n",
    "- Summations of parts\n",
    "- Larger area minus non-included parts\n",
    "- Geographical interpolation\n",
    "- Geographical extrapolation\n",
    "- Temporal interpolation\n",
    "- Temporal extrapolation\n",
    "- Adjustments for under-enumeration\n",
    "- Recalculated to fit present borders\n",
    "\n",
    "Since the data for the year 1950 of the two main datasets did not match for every country, small adjustments and smoothing were applied.\n",
    "\n",
    "Preprocessing:\n",
    "For our purposes, the subdatasets \"data-for-world-by-year\", \"data-for-regions-by-year\" and \"data-for-countries-by-year\" are relevant.\n",
    "We consider the period from 1900 until 2021 and remove redundant rows. We do this because the data after 2021 are based on estimated values.\n",
    "\n",
    "There are no missing values.\n",
    "Since the regions of the dataset did not match with the regions of our other datasets, we chose to compute the regions-population according to the regions from the un-country-codes.csv file.\n",
    "\n",
    "Subdataset:\n",
    "    population-global.csv\n",
    "        Columns: year, population\n",
    "\n",
    "    population-region.csv\n",
    "        Columns: region_code, region_name, year, population\n",
    "\n",
    "    population-country.csv\n",
    "        Columns: country_code, country_name, year, population\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Preprocess global data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('raw data for world population')\n",
    "print(pop_global_df.head(3))\n",
    "\n",
    "# remove unnecessary columns\n",
    "pop_global_df = pop_global_df[['time', 'Population']]\n",
    "pop_global_df.rename(columns={'Population': 'population', 'time': 'year'}, inplace=True)\n",
    "pop_global_df.set_index('year')\n",
    "\n",
    "print('extract population data from 1900 until now')\n",
    "pop_global_df = pop_global_df[pop_global_df['year'] >= 1900]\n",
    "pop_global_df = pop_global_df[pop_global_df['year'] <= 2021]\n",
    "pop_global_df['population'] = pop_global_df['population'].astype(int)\n",
    "print(pop_global_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Prepare dataset for countries and regions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "un_country_codes = un_country_codes[['Region Code', 'Region Name', 'ISO-alpha3 Code']]\n",
    "\n",
    "# extract population data from 1900 until now\n",
    "pop_country_df = pop_country_df[pop_country_df['time'] >= 1900]\n",
    "pop_country_df = pop_country_df[pop_country_df['time'] <= 2021]\n",
    "\n",
    "# format column that will be joined\n",
    "pop_country_df['geo'] = pop_country_df['geo'].str.upper()\n",
    "\n",
    "# merge country-population and un-country-codes\n",
    "pop_country_region_df = pd.merge(pop_country_df, un_country_codes, how='left', left_on='geo', right_on='ISO-alpha3 Code')\n",
    "\n",
    "pop_country_region_df.rename(columns={'Region Code': 'region_code', 'Region Name': 'region_name', 'time': 'year', 'Population': 'population', 'geo': 'country_code', 'name': 'country_name'}, inplace=True)\n",
    "\n",
    "# add missing region code to holy see and taiwan\n",
    "pop_country_region_df.loc[pop_country_region_df['country_code'] == 'HOS', 'region_code'] = 150\n",
    "pop_country_region_df.loc[pop_country_region_df['country_code'] == 'HOS', 'region_name'] = 'Europe'\n",
    "\n",
    "pop_country_region_df.loc[pop_country_region_df['country_code'] == 'TWN', 'region_code'] = 142\n",
    "pop_country_region_df.loc[pop_country_region_df['country_code'] == 'TWN', 'region_name'] = 'Asia'\n",
    "\n",
    "# remove unused column 'ISO-alpha3 Code'\n",
    "pop_country_region_df.drop('ISO-alpha3 Code', axis=1, inplace=True)\n",
    "\n",
    "# check if we worked correctly\n",
    "print(pop_country_region_df[pop_country_region_df.isnull().any(axis=1)])\n",
    "\n",
    "# the remaining missing values are population data for the country 'holy see' since the missing population is in the range < 1000 we, accept the missing values and calculate with the latest documented value."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Save population regions dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "columns_regions = ['region_code', 'region_name', 'country_code', 'country_name', 'year', 'population']\n",
    "\n",
    "# select columns\n",
    "pop_regions_df = pop_country_region_df[columns_regions]\n",
    "pop_regions_df = pop_regions_df.groupby(['region_code', 'region_name', 'year'], as_index=False)['population'].sum()\n",
    "pop_regions_df['population'] = pop_regions_df['population'].astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Save population countries dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "columns_country = ['country_code', 'country_name', 'year', 'population']\n",
    "\n",
    "# select columns\n",
    "pop_country_df = pop_country_region_df[columns_country]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compare calculated region population to gapminder dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "regions_population_2021_gapminder = pop_region_gapminder[pop_region_gapminder['time'] == 2021]\n",
    "total_population_gapminder = regions_population_2021_gapminder['Population'].sum()\n",
    "\n",
    "regions_population_2021_calc = pop_regions_df[pop_regions_df['year'] == 2021]\n",
    "total_population_calc = regions_population_2021_calc['population'].sum()\n",
    "\n",
    "print('Total population from the year 2021')\n",
    "print('Gapminder ' + str(total_population_gapminder))\n",
    "print('Calculated ' + str(total_population_calc))\n",
    "\n",
    "print('\\n')\n",
    "print('Gapminder region population')\n",
    "print(regions_population_2021_gapminder[['name', 'Population']])\n",
    "\n",
    "print('\\n')\n",
    "print('Calculated region population')\n",
    "print(regions_population_2021_calc[['region_name', 'population']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, we get the same results for the total populations in the year 2021.\n",
    "If we take a look at the region population, we see that the calculated population of america and africa shows no difference to the gapminder-dataset population. For the regions asia and europe, we get different results. Furthermore, the calculated region oceania does not exist in the gapminder dataset. Since the documentation of the gapminder dataset does not include the country-to-region assignment, we cannot compare the differences between our calculation and gapminder. However, since the overall score is equal, we can argue that those differences occur tue to different country-to-region assignments."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Todo: Remove later\n",
    "pop_global_df.to_csv(POP_PROCESSED_GLOBAL_FILE, sep=';', index=False, header=True)\n",
    "#store in csv file\n",
    "pop_country_df.to_csv(POP_PROCESSED_COUNTRY_FILE, sep=';', index=False, header=True)\n",
    "#store in csv file\n",
    "pop_regions_df.to_csv(POP_PROCESSED_REGION_FILE, sep=';', index=False, header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pop_global_df.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pop_country_df.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pop_regions_df.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Temperature Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load temp-land-country data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_countries = temperature_countries_with_iso"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section we load Berkleys land-temperature data for each country from the corresponding .txt files and combine them into a single dataframe. We also change the naming of the countries according to the naming standard of the UN and add their ISO3-Country code."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_land_country_column_names = [\"country_code\", \"country_name\", \"year\", \"month\", \"monthly_anomaly\", \"monthly_unc\", \"annual_anomaly\", \"annual_unc\", \"five_year_anomaly\", \"five_year_unc\", \"ten_year_anomaly\", \"ten_year_unc\", \"twenty_year_anomaly\", \"twenty_year_unc\"]\n",
    "temp_country = pd.DataFrame(columns=temp_land_country_column_names)\n",
    "\n",
    "temp_country_names = temp_countries['Country']\n",
    "\n",
    "# Todo when combining use the dictionary in the data-integration notebook\n",
    "new_country_names = {\n",
    "    \"Denmark (Europe)\": \"Denmark\",\n",
    "    \"France (Europe)\": \"France\",\n",
    "    \"Netherlands (Europe)\": \"Netherlands\",\n",
    "    \"United Kingdom (Europe)\": \"United Kingdom\",\n",
    "    \"Ãland\": \"Ãland Islands\",\n",
    "    \"Czech Republic\": \"Czechia\",\n",
    "    \"Turkey\": \"TÃ¼rkiye\",\n",
    "    \"Svalbard and Jan Mayen\": \"Svalbard and Jan Mayen Islands\",\n",
    "    \"Cape Verde\": \"Cabo Verde\",\n",
    "    \"Turks and Caicas Islands\": \"Turks and Caicos Islands\",\n",
    "    \"Swaziland\": \"Eswatini\",\n",
    "    \"Macedonia\": \"North Macedonia\",\n",
    "    \"CÃ´te d'Ivoire\": \"CÃ´te dâIvoire\",\n",
    "    \"Federated States of Micronesia\": \"Micronesia (Federated States of)\",\n",
    "    \"South Georgia and the South Sandwich Isla\": \"South Georgia and the South Sandwich Islands\",\n",
    "    \"Bonaire, Saint Eustatius and Saba\": \"Bonaire, Sint Eustatius and Saba\",\n",
    "    \"Congo (Democratic Republic of the)\": \"Democratic Republic of the Congo\",\n",
    "    \"South Korea\": \"Korea, South\",\n",
    "    \"North Korea\": \"Korea, North\",\n",
    "    \"Palestina\": \"State of Palestine\"\n",
    "}\n",
    "\n",
    "def swap_keys_values(d):\n",
    "    return {v: k for k, v in d.items()}\n",
    "\n",
    "map_country_to_filename = swap_keys_values(new_country_names)\n",
    "\n",
    "for temp_country_name in temp_country_names:\n",
    "    temp_country_file_name = temp_country_name\n",
    "    if temp_country_name in map_country_to_filename:\n",
    "        temp_country_file_name = map_country_to_filename[temp_country_name]\n",
    "    path_temp_country_txt_file = TEMP_RAW_FOLDER + PATH_COUNTRIES_LAND_FOLDER + temp_country_file_name + '.txt'\n",
    "    temp_land_one_country = pd.read_csv(path_temp_country_txt_file, comment=\"%\", header=None, delim_whitespace=True)\n",
    "    country_iso_name = temp_countries.loc[temp_countries['Country'] == temp_country_name]['ISO-alpha3'].iloc[0]\n",
    "\n",
    "    temp_land_one_country.insert(0, 'country_code', country_iso_name)\n",
    "    temp_land_one_country.insert(1, 'country_name', temp_country_name)\n",
    "\n",
    "    temp_land_one_country.columns=temp_land_country_column_names\n",
    "\n",
    "    temp_country = pd.concat([temp_country, temp_land_one_country])\n",
    "\n",
    "temp_country.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create temp-land-region data from temp-land-country data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Berkleys offers a dataset with regional temperature anomly data, however because we also use datasets for natural disasters and population we have to ensure that regions in all datasets are including the same countries. The regions defined for Berkleys regional temp anomaly datasets differ from the definition of regions in the UN dataset we use, therefore we decided to calculate regional temp anomalies based on the berkleys temp anomaly data for individual countries for regions based on the region data of the UN dataset.\n",
    "\n",
    "Temperature anomalies tend to be similar across large regions geographical regions, even if the absolute temperature of two different measuring points differs for the same time period, their anomalies tend to be quite similar. To create regional temperature anomaly data we calculate the mean of temperature anomaly measures for all countries in a given region on a monthly basis. Source: https://data.giss.nasa.gov/gistemp/faq/#q101\n",
    "\n",
    "When computing a region it is possible that some countries are not included, because it is possible that countries started reporting earlier than others or stopped reporting for some time periods."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_region_anomaly(temp_land_country_regions):\n",
    "    temp_return_land_region = temp_land_country_regions.groupby(['region_code', 'region_name', 'year', 'month'], as_index=False)['monthly_anomaly'].mean()\n",
    "    temp_return_land_region.rename({ 'monthly_anomaly': 'temperature_anomaly'}, axis=1,inplace=True)\n",
    "    return temp_return_land_region"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Todo: only load UN country codes once after combining notebook\n",
    "# load and format un-country-codes data\n",
    "\n",
    "un_country_codes = un_country_codes[['Region Code', 'Region Name', 'ISO-alpha3 Code']]\n",
    "un_country_codes.columns=['region_code', 'region_name', 'country_code']\n",
    "\n",
    "# Join UN country codes to temp data\n",
    "# exclude temp data for antarctica when calculating regional temp data\n",
    "temp_land_country_no_antarctica = temp_country[temp_country.country_name != 'Antarctica']\n",
    "temp_land_country_regions = pd.merge(temp_land_country_no_antarctica, un_country_codes, on='country_code', how='left')\n",
    "temp_land_country_regions.loc[temp_land_country_regions['country_name']=='Taiwan','region_name'] = 'Asia'\n",
    "temp_land_country_regions.loc[temp_land_country_regions['country_name']=='Taiwan','region_code'] = 142\n",
    "\n",
    "# calculate anomaly data for regions\n",
    "temp_region = calc_region_anomaly(temp_land_country_regions)\n",
    "\n",
    "temp_region.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load berkleys regional data\n",
    "\n",
    "We load Berkleys regional temperature anomaly data to compare it to the regional temperature anomaly data we calculated from the country temperature anomaly data and UN regions combined.\n",
    "\n",
    "We combine Berkleys North and South America region to one Region 'Americas' to be able to compare it to the combined 'Americas' region of the UN data later."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Todo: make this code segment prettier; reuse other code segments\n",
    "\n",
    "temp_land_country_column_names = [\"region_name\", \"year\", \"month\", \"monthly_anomaly\", \"monthly_unc\", \"annual_anomaly\", \"annual_unc\", \"five_year_anomaly\", \"five_year_unc\", \"ten_year_anomaly\", \"ten_year_unc\", \"twenty_year_anomaly\", \"twenty_year_unc\"]\n",
    "temp_berkley_regions = ['Africa', 'Asia', 'Europe', 'North America', 'Oceania', 'South America']\n",
    "temp_berkleys_region = pd.DataFrame(columns=temp_land_country_column_names)\n",
    "\n",
    "for temp_berkley_region in temp_berkley_regions:\n",
    "    region_file_name = temp_berkley_region\n",
    "    path = TEMP_RAW_FOLDER + PATH_REGIONS_LAND_FOLDER + region_file_name + '.txt'\n",
    "    one_region = pd.read_csv(path, comment=\"%\", header=None, delim_whitespace=True)\n",
    "    one_region.insert(0, 'region_name', temp_berkley_region)\n",
    "    one_region.columns=temp_land_country_column_names\n",
    "    temp_berkleys_region = pd.concat([temp_berkleys_region, one_region])\n",
    "\n",
    "temp_berkleys_region.loc[(temp_berkleys_region['region_name'] == 'South America') | (temp_berkleys_region['region_name'] == 'North America'), 'region_name'] = 'Americas'\n",
    "temp_berkleys_region = temp_berkleys_region[['region_name', 'year', 'month', 'monthly_anomaly']]\n",
    "temp_berkleys_region = temp_berkleys_region.groupby(by=['region_name', 'year', 'month'])['monthly_anomaly'].mean()\n",
    "temp_berkleys_region = temp_berkleys_region.reset_index()\n",
    "temp_berkleys_region = temp_berkleys_region.rename(columns={'monthly_anomaly': 'temperature_anomaly'})\n",
    "\n",
    "temp_berkleys_region.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load temp-land-ocean-global data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Two versions exist that treat temperature anomalies at locations with sea ice:\n",
    "1. Anomalies are extrapolated from land-surface air temperature anomalies.\n",
    "2. Anomalies are extrapolated from sea-surface water temperature anomalies (usually collected from open water areas on the periphery of the sea ice).\n",
    "\n",
    "We choose to use the air temperature version based on Berkleys remark:\n",
    "\"We believe that the use of air temperatures above sea ice provides a more natural means of describing changes in Earth's surface temperature.\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# only include the dataset where anomalies are extrapolated from land-surface air temperature anomalies.\n",
    "temp_global = pd.read_csv(TEMP_RAW_FOLDER + TEMP_GLOBAL_LAND_OCEAN_FILE, comment=\"%\", header=None, delim_whitespace=True, engine='python', skipfooter=2079)\n",
    "temp_global.columns = [\"year\", \"month\", \"monthly_anomaly\", \"monthly_unc\", \"annual_anomaly\", \"annual_unc\", \"five_year_anomaly\", \"five_year_unc\", \"ten_year_anomaly\", \"ten_year_unc\", \"twenty_year_anomaly\", \"twenty_year_unc\"]\n",
    "temp_global.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Format the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We only keep entries after the year 1900 because our research questions focus on the past 100 years."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cut_before_1900(temp_data):\n",
    "    return temp_data[temp_data['year'] >= 1900]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We only keep monthly anomaly data and rename the column to temperature_anomaly"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def keep_monthly_anomalies(temp_data):\n",
    "    drop_labels = [\"monthly_unc\", \"annual_anomaly\", \"annual_unc\", \"five_year_anomaly\", \"five_year_unc\", \"ten_year_anomaly\", \"ten_year_unc\", \"twenty_year_anomaly\", \"twenty_year_unc\"]\n",
    "    temp_data = temp_data.drop(labels=drop_labels, axis=1)\n",
    "    return temp_data.rename(columns={'monthly_anomaly': 'temperature_anomaly'})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Format temp-land-country"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apply formatting functions for only keeping monthly anomalies and cut values before 1900"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_country = cut_before_1900(temp_country)\n",
    "temp_country = keep_monthly_anomalies(temp_country)\n",
    "temp_country = temp_country.groupby([\"year\",\"country_code\"]).agg({\"temperature_anomaly\":\"mean\",\"country_name\":\"first\"})\n",
    "temp_country.reset_index(inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Format temp-land-region and berkleys regional data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apply formatting function to cut values before 1900"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_region = cut_before_1900(temp_region)\n",
    "temp_berkleys_region = cut_before_1900(temp_berkleys_region)\n",
    "temp_region = temp_region.groupby([\"year\",\"region_code\"]).agg({\"temperature_anomaly\":\"mean\",\"region_name\":\"first\"})\n",
    "temp_region.reset_index(inplace=True)\n",
    "temp_berkleys_region.reset_index(inplace=True)\n",
    "temp_berkleys_region = temp_berkleys_region.groupby([\"year\", \"region_name\"]).agg({\"temperature_anomaly\":\"mean\"})\n",
    "temp_berkleys_region.reset_index(inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Format temp-land-ocean-global"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apply formatting functions for only keeping monthly anomalies and cut values before 1900"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_global = cut_before_1900(temp_global)\n",
    "temp_global = keep_monthly_anomalies(temp_global)\n",
    "temp_global = temp_global.groupby([\"year\"])[\"temperature_anomaly\"].mean()\n",
    "temp_global = temp_global.reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compare berkleys and our regional data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def temp_calc_region_corr(temp_single_region):\n",
    "    temp_land_single_region = temp_region[(temp_region['region_name'] == temp_single_region)].set_index(['year'])\n",
    "    temp_berkleys_land_single_region = temp_berkleys_region[(temp_berkleys_region['region_name'] == temp_single_region)].set_index(['year'])\n",
    "\n",
    "    comp_temp_land_africa_joined = temp_berkleys_land_single_region.join(temp_land_single_region, on=['year'], lsuffix='_berkleys', rsuffix='_self')\n",
    "\n",
    "    temp_land_region_corr = comp_temp_land_africa_joined['temperature_anomaly_berkleys'].corr(comp_temp_land_africa_joined['temperature_anomaly_self'])\n",
    "\n",
    "    print('Correlation for region ' + temp_single_region + ' ' + str(temp_land_region_corr))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Correlation of temperature anomalies between our and berkleys regional temp data sets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_berkley_regions = ['Africa', 'Asia', 'Europe', 'Americas', 'Oceania']\n",
    "\n",
    "for temp_berkley_region in temp_berkley_regions:\n",
    "    temp_calc_region_corr(temp_berkley_region)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Check temperature_anomaly columns for NaN values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Country: \" + str(temp_country['temperature_anomaly'].isna().sum()))\n",
    "print(\"Region: \" + str(temp_region['temperature_anomaly'].isna().sum()))\n",
    "print(\"Global: \" + str(temp_global['temperature_anomaly'].isna().sum()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Handle NaN values in temp_land_country"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_country[temp_country['temperature_anomaly'].isna()].groupby('country_code').size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_country['temperature_anomaly'] = temp_country['temperature_anomaly'].interpolate(method='linear')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_country[temp_country['temperature_anomaly'].isna()==True].size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Save the data as csv files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Todo: Remove later\n",
    "temp_country.to_csv(TEMP_PROCESSED_FOLDER + 'temp-land-country.csv', index=False)\n",
    "temp_region.to_csv(TEMP_PROCESSED_FOLDER + 'temp-land-region.csv', index=False)\n",
    "temp_global.to_csv(TEMP_PROCESSED_FOLDER + 'temp-land-ocean-global.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_country.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_region.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_global.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "       year  population\n100  1900.0  1627123965\n101  1901.0  1639684222\n102  1902.0  1652898195",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>100</th>\n      <td>1900.0</td>\n      <td>1627123965</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>1901.0</td>\n      <td>1639684222</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>1902.0</td>\n      <td>1652898195</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_global_df.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "  country_code country_name    year  population\n0          AFG  Afghanistan  1900.0   4707744.0\n1          AFG  Afghanistan  1901.0   4751177.0\n2          AFG  Afghanistan  1902.0   4802500.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country_code</th>\n      <th>country_name</th>\n      <th>year</th>\n      <th>population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1900.0</td>\n      <td>4707744.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1901.0</td>\n      <td>4751177.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1902.0</td>\n      <td>4802500.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_country_df.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "   region_code region_name    year  population\n0          2.0      Africa  1900.0   138578556\n1          2.0      Africa  1901.0   139018147\n2          2.0      Africa  1902.0   139489077",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>region_code</th>\n      <th>region_name</th>\n      <th>year</th>\n      <th>population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1900.0</td>\n      <td>138578556</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1901.0</td>\n      <td>139018147</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1902.0</td>\n      <td>139489077</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_regions_df.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3 Temperature Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Load temp-land-country data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "temp_countries = temperature_countries_with_iso"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this section we load Berkleys land-temperature data for each country from the corresponding .txt files and combine them into a single dataframe. We also change the naming of the countries according to the naming standard of the UN and add their ISO3-Country code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  country_code country_name  year month  monthly_anomaly  monthly_unc  \\\n0          AFG  Afghanistan  1848     5           -0.297        2.037   \n1          AFG  Afghanistan  1848     6           -0.796        2.136   \n2          AFG  Afghanistan  1848     7           -0.113        1.937   \n3          AFG  Afghanistan  1848     8           -0.462        1.937   \n4          AFG  Afghanistan  1848     9           -1.272        1.865   \n\n   annual_anomaly  annual_unc  five_year_anomaly  five_year_unc  \\\n0             NaN         NaN                NaN            NaN   \n1             NaN         NaN                NaN            NaN   \n2          -0.777       0.639                NaN            NaN   \n3          -0.743       0.644                NaN            NaN   \n4          -0.676       0.669                NaN            NaN   \n\n   ten_year_anomaly  ten_year_unc  twenty_year_anomaly  twenty_year_unc  \n0               NaN           NaN                  NaN              NaN  \n1               NaN           NaN                  NaN              NaN  \n2               NaN           NaN                  NaN              NaN  \n3               NaN           NaN                  NaN              NaN  \n4               NaN           NaN                  NaN              NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country_code</th>\n      <th>country_name</th>\n      <th>year</th>\n      <th>month</th>\n      <th>monthly_anomaly</th>\n      <th>monthly_unc</th>\n      <th>annual_anomaly</th>\n      <th>annual_unc</th>\n      <th>five_year_anomaly</th>\n      <th>five_year_unc</th>\n      <th>ten_year_anomaly</th>\n      <th>ten_year_unc</th>\n      <th>twenty_year_anomaly</th>\n      <th>twenty_year_unc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1848</td>\n      <td>5</td>\n      <td>-0.297</td>\n      <td>2.037</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1848</td>\n      <td>6</td>\n      <td>-0.796</td>\n      <td>2.136</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1848</td>\n      <td>7</td>\n      <td>-0.113</td>\n      <td>1.937</td>\n      <td>-0.777</td>\n      <td>0.639</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1848</td>\n      <td>8</td>\n      <td>-0.462</td>\n      <td>1.937</td>\n      <td>-0.743</td>\n      <td>0.644</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AFG</td>\n      <td>Afghanistan</td>\n      <td>1848</td>\n      <td>9</td>\n      <td>-1.272</td>\n      <td>1.865</td>\n      <td>-0.676</td>\n      <td>0.669</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_land_country_column_names = [\"country_code\", \"country_name\", \"year\", \"month\", \"monthly_anomaly\", \"monthly_unc\", \"annual_anomaly\", \"annual_unc\", \"five_year_anomaly\", \"five_year_unc\", \"ten_year_anomaly\", \"ten_year_unc\", \"twenty_year_anomaly\", \"twenty_year_unc\"]\n",
    "temp_country = pd.DataFrame(columns=temp_land_country_column_names)\n",
    "\n",
    "temp_country_names = temp_countries['Country']\n",
    "\n",
    "# Todo when combining use the dictionary in the data-integration notebook\n",
    "new_country_names = {\n",
    "    \"Denmark (Europe)\": \"Denmark\",\n",
    "    \"France (Europe)\": \"France\",\n",
    "    \"Netherlands (Europe)\": \"Netherlands\",\n",
    "    \"United Kingdom (Europe)\": \"United Kingdom\",\n",
    "    \"Ãland\": \"Ãland Islands\",\n",
    "    \"Czech Republic\": \"Czechia\",\n",
    "    \"Turkey\": \"TÃ¼rkiye\",\n",
    "    \"Svalbard and Jan Mayen\": \"Svalbard and Jan Mayen Islands\",\n",
    "    \"Cape Verde\": \"Cabo Verde\",\n",
    "    \"Turks and Caicas Islands\": \"Turks and Caicos Islands\",\n",
    "    \"Swaziland\": \"Eswatini\",\n",
    "    \"Macedonia\": \"North Macedonia\",\n",
    "    \"CÃ´te d'Ivoire\": \"CÃ´te dâIvoire\",\n",
    "    \"Federated States of Micronesia\": \"Micronesia (Federated States of)\",\n",
    "    \"South Georgia and the South Sandwich Isla\": \"South Georgia and the South Sandwich Islands\",\n",
    "    \"Bonaire, Saint Eustatius and Saba\": \"Bonaire, Sint Eustatius and Saba\",\n",
    "    \"Congo (Democratic Republic of the)\": \"Democratic Republic of the Congo\",\n",
    "    \"South Korea\": \"Korea, South\",\n",
    "    \"North Korea\": \"Korea, North\",\n",
    "    \"Palestina\": \"State of Palestine\"\n",
    "}\n",
    "\n",
    "def swap_keys_values(d):\n",
    "    return {v: k for k, v in d.items()}\n",
    "\n",
    "map_country_to_filename = swap_keys_values(new_country_names)\n",
    "\n",
    "for temp_country_name in temp_country_names:\n",
    "    temp_country_file_name = temp_country_name\n",
    "    if temp_country_name in map_country_to_filename:\n",
    "        temp_country_file_name = map_country_to_filename[temp_country_name]\n",
    "    path_temp_country_txt_file = TEMP_RAW_FOLDER + PATH_COUNTRIES_LAND_FOLDER + temp_country_file_name + '.txt'\n",
    "    temp_land_one_country = pd.read_csv(path_temp_country_txt_file, comment=\"%\", header=None, delim_whitespace=True)\n",
    "    country_iso_name = temp_countries.loc[temp_countries['Country'] == temp_country_name]['ISO-alpha3'].iloc[0]\n",
    "\n",
    "    temp_land_one_country.insert(0, 'country_code', country_iso_name)\n",
    "    temp_land_one_country.insert(1, 'country_name', temp_country_name)\n",
    "\n",
    "    temp_land_one_country.columns=temp_land_country_column_names\n",
    "\n",
    "    temp_country = pd.concat([temp_country, temp_land_one_country])\n",
    "\n",
    "temp_country.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Create temp-land-region data from temp-land-country data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Berkleys offers a dataset with regional temperature anomly data, however because we also use datasets for natural disasters and population we have to ensure that regions in all datasets are including the same countries. The regions defined for Berkleys regional temp anomaly datasets differ from the definition of regions in the UN dataset we use, therefore we decided to calculate regional temp anomalies based on the berkleys temp anomaly data for individual countries for regions based on the region data of the UN dataset.\n",
    "\n",
    "Temperature anomalies tend to be similar across large regions geographical regions, even if the absolute temperature of two different measuring points differs for the same time period, their anomalies tend to be quite similar. To create regional temperature anomaly data we calculate the mean of temperature anomaly measures for all countries in a given region on a monthly basis. Source: https://data.giss.nasa.gov/gistemp/faq/#q101\n",
    "\n",
    "When computing a region it is possible that some countries are not included, because it is possible that countries started reporting earlier than others or stopped reporting for some time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_region_anomaly(temp_land_country_regions):\n",
    "    temp_return_land_region = temp_land_country_regions.groupby(['region_code', 'region_name', 'year', 'month'], as_index=False)['monthly_anomaly'].mean()\n",
    "    temp_return_land_region.rename({ 'monthly_anomaly': 'temperature_anomaly'}, axis=1,inplace=True)\n",
    "    return temp_return_land_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   region_code region_name  year  month  temperature_anomaly\n0          2.0      Africa  1787      1               0.1520\n1          2.0      Africa  1787      2              -0.3142\n2          2.0      Africa  1787      3              -0.7876\n3          2.0      Africa  1787      4              -0.9146\n4          2.0      Africa  1787      5              -0.5570",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>region_code</th>\n      <th>region_name</th>\n      <th>year</th>\n      <th>month</th>\n      <th>temperature_anomaly</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1787</td>\n      <td>1</td>\n      <td>0.1520</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1787</td>\n      <td>2</td>\n      <td>-0.3142</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1787</td>\n      <td>3</td>\n      <td>-0.7876</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1787</td>\n      <td>4</td>\n      <td>-0.9146</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>Africa</td>\n      <td>1787</td>\n      <td>5</td>\n      <td>-0.5570</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Todo: only load UN country codes once after combining notebook\n",
    "# load and format un-country-codes data\n",
    "\n",
    "un_country_codes = un_country_codes[['Region Code', 'Region Name', 'ISO-alpha3 Code']]\n",
    "un_country_codes.columns=['region_code', 'region_name', 'country_code']\n",
    "\n",
    "# Join UN country codes to temp data\n",
    "# exclude temp data for antarctica when calculating regional temp data\n",
    "temp_land_country_no_antarctica = temp_country[temp_country.country_name != 'Antarctica']\n",
    "temp_land_country_regions = pd.merge(temp_land_country_no_antarctica, un_country_codes, on='country_code', how='left')\n",
    "temp_land_country_regions.loc[temp_land_country_regions['country_name']=='Taiwan','region_name'] = 'Asia'\n",
    "temp_land_country_regions.loc[temp_land_country_regions['country_name']=='Taiwan','region_code'] = 142\n",
    "\n",
    "# calculate anomaly data for regions\n",
    "temp_region = calc_region_anomaly(temp_land_country_regions)\n",
    "\n",
    "temp_region.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load berkleys regional data\n",
    "\n",
    "We load Berkleys regional temperature anomaly data to compare it to the regional temperature anomaly data we calculated from the country temperature anomaly data and UN regions combined.\n",
    "\n",
    "We combine Berkleys North and South America region to one Region 'Americas' to be able to compare it to the combined 'Americas' region of the UN data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  region_name  year  month  temperature_anomaly\n0      Africa  1880      8               -0.181\n1      Africa  1880      9               -0.389\n2      Africa  1880     10               -0.274\n3      Africa  1880     11               -0.169\n4      Africa  1880     12               -0.396",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>region_name</th>\n      <th>year</th>\n      <th>month</th>\n      <th>temperature_anomaly</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Africa</td>\n      <td>1880</td>\n      <td>8</td>\n      <td>-0.181</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Africa</td>\n      <td>1880</td>\n      <td>9</td>\n      <td>-0.389</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Africa</td>\n      <td>1880</td>\n      <td>10</td>\n      <td>-0.274</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Africa</td>\n      <td>1880</td>\n      <td>11</td>\n      <td>-0.169</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Africa</td>\n      <td>1880</td>\n      <td>12</td>\n      <td>-0.396</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Todo: make this code segment prettier; reuse other code segments\n",
    "\n",
    "temp_land_country_column_names = [\"region_name\", \"year\", \"month\", \"monthly_anomaly\", \"monthly_unc\", \"annual_anomaly\", \"annual_unc\", \"five_year_anomaly\", \"five_year_unc\", \"ten_year_anomaly\", \"ten_year_unc\", \"twenty_year_anomaly\", \"twenty_year_unc\"]\n",
    "temp_berkley_regions = ['Africa', 'Asia', 'Europe', 'North America', 'Oceania', 'South America']\n",
    "temp_berkleys_region = pd.DataFrame(columns=temp_land_country_column_names)\n",
    "\n",
    "for temp_berkley_region in temp_berkley_regions:\n",
    "    region_file_name = temp_berkley_region\n",
    "    path = TEMP_RAW_FOLDER + PATH_REGIONS_LAND_FOLDER + region_file_name + '.txt'\n",
    "    one_region = pd.read_csv(path, comment=\"%\", header=None, delim_whitespace=True)\n",
    "    one_region.insert(0, 'region_name', temp_berkley_region)\n",
    "    one_region.columns=temp_land_country_column_names\n",
    "    temp_berkleys_region = pd.concat([temp_berkleys_region, one_region])\n",
    "\n",
    "temp_berkleys_region.loc[(temp_berkleys_region['region_name'] == 'South America') | (temp_berkleys_region['region_name'] == 'North America'), 'region_name'] = 'Americas'\n",
    "temp_berkleys_region = temp_berkleys_region[['region_name', 'year', 'month', 'monthly_anomaly']]\n",
    "temp_berkleys_region = temp_berkleys_region.groupby(by=['region_name', 'year', 'month'])['monthly_anomaly'].mean()\n",
    "temp_berkleys_region = temp_berkleys_region.reset_index()\n",
    "temp_berkleys_region = temp_berkleys_region.rename(columns={'monthly_anomaly': 'temperature_anomaly'})\n",
    "\n",
    "temp_berkleys_region.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Load temp-land-ocean-global data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Two versions exist that treat temperature anomalies at locations with sea ice:\n",
    "1. Anomalies are extrapolated from land-surface air temperature anomalies.\n",
    "2. Anomalies are extrapolated from sea-surface water temperature anomalies (usually collected from open water areas on the periphery of the sea ice).\n",
    "\n",
    "We choose to use the air temperature version based on Berkleys remark:\n",
    "\"We believe that the use of air temperatures above sea ice provides a more natural means of describing changes in Earth's surface temperature.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   year  month  monthly_anomaly  monthly_unc  annual_anomaly  annual_unc  \\\n0  1850      1           -0.736        0.389             NaN         NaN   \n1  1850      2           -0.202        0.526             NaN         NaN   \n2  1850      3           -0.363        0.333             NaN         NaN   \n3  1850      4           -0.589        0.334             NaN         NaN   \n4  1850      5           -0.614        0.217             NaN         NaN   \n\n   five_year_anomaly  five_year_unc  ten_year_anomaly  ten_year_unc  \\\n0                NaN            NaN               NaN           NaN   \n1                NaN            NaN               NaN           NaN   \n2                NaN            NaN               NaN           NaN   \n3                NaN            NaN               NaN           NaN   \n4                NaN            NaN               NaN           NaN   \n\n   twenty_year_anomaly  twenty_year_unc  \n0                  NaN              NaN  \n1                  NaN              NaN  \n2                  NaN              NaN  \n3                  NaN              NaN  \n4                  NaN              NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>month</th>\n      <th>monthly_anomaly</th>\n      <th>monthly_unc</th>\n      <th>annual_anomaly</th>\n      <th>annual_unc</th>\n      <th>five_year_anomaly</th>\n      <th>five_year_unc</th>\n      <th>ten_year_anomaly</th>\n      <th>ten_year_unc</th>\n      <th>twenty_year_anomaly</th>\n      <th>twenty_year_unc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1850</td>\n      <td>1</td>\n      <td>-0.736</td>\n      <td>0.389</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1850</td>\n      <td>2</td>\n      <td>-0.202</td>\n      <td>0.526</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1850</td>\n      <td>3</td>\n      <td>-0.363</td>\n      <td>0.333</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1850</td>\n      <td>4</td>\n      <td>-0.589</td>\n      <td>0.334</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1850</td>\n      <td>5</td>\n      <td>-0.614</td>\n      <td>0.217</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only include the dataset where anomalies are extrapolated from land-surface air temperature anomalies.\n",
    "temp_global = pd.read_csv(TEMP_RAW_FOLDER + TEMP_GLOBAL_LAND_OCEAN_FILE, comment=\"%\", header=None, delim_whitespace=True, engine='python', skipfooter=2079)\n",
    "temp_global.columns = [\"year\", \"month\", \"monthly_anomaly\", \"monthly_unc\", \"annual_anomaly\", \"annual_unc\", \"five_year_anomaly\", \"five_year_unc\", \"ten_year_anomaly\", \"ten_year_unc\", \"twenty_year_anomaly\", \"twenty_year_unc\"]\n",
    "temp_global.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Format the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We only keep entries after the year 1900 because our research questions focus on the past 100 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cut_before_1900(temp_data):\n",
    "    return temp_data[temp_data['year'] >= 1900]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We only keep monthly anomaly data and rename the column to temperature_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def keep_monthly_anomalies(temp_data):\n",
    "    drop_labels = [\"monthly_unc\", \"annual_anomaly\", \"annual_unc\", \"five_year_anomaly\", \"five_year_unc\", \"ten_year_anomaly\", \"ten_year_unc\", \"twenty_year_anomaly\", \"twenty_year_unc\"]\n",
    "    temp_data = temp_data.drop(labels=drop_labels, axis=1)\n",
    "    return temp_data.rename(columns={'monthly_anomaly': 'temperature_anomaly'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Format temp-land-country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Apply formatting functions for only keeping monthly anomalies and cut values before 1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_country = cut_before_1900(temp_country)\n",
    "temp_country = keep_monthly_anomalies(temp_country)\n",
    "temp_country = temp_country.groupby([\"year\",\"country_code\"]).agg({\"temperature_anomaly\":\"mean\",\"country_name\":\"first\"})\n",
    "temp_country.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Format temp-land-region and berkleys regional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Apply formatting function to cut values before 1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_region = cut_before_1900(temp_region)\n",
    "temp_berkleys_region = cut_before_1900(temp_berkleys_region)\n",
    "temp_region = temp_region.groupby([\"year\",\"region_code\"]).agg({\"temperature_anomaly\":\"mean\",\"region_name\":\"first\"})\n",
    "temp_region.reset_index(inplace=True)\n",
    "temp_berkleys_region.reset_index(inplace=True)\n",
    "temp_berkleys_region = temp_berkleys_region.groupby([\"year\", \"region_name\"]).agg({\"temperature_anomaly\":\"mean\"})\n",
    "temp_berkleys_region.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Format temp-land-ocean-global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Apply formatting functions for only keeping monthly anomalies and cut values before 1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_global = cut_before_1900(temp_global)\n",
    "temp_global = keep_monthly_anomalies(temp_global)\n",
    "temp_global = temp_global.groupby([\"year\"])[\"temperature_anomaly\"].mean()\n",
    "temp_global = temp_global.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Compare berkleys and our regional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def temp_calc_region_corr(temp_single_region):\n",
    "    temp_land_single_region = temp_region[(temp_region['region_name'] == temp_single_region)].set_index(['year'])\n",
    "    temp_berkleys_land_single_region = temp_berkleys_region[(temp_berkleys_region['region_name'] == temp_single_region)].set_index(['year'])\n",
    "\n",
    "    comp_temp_land_africa_joined = temp_berkleys_land_single_region.join(temp_land_single_region, on=['year'], lsuffix='_berkleys', rsuffix='_self')\n",
    "\n",
    "    temp_land_region_corr = comp_temp_land_africa_joined['temperature_anomaly_berkleys'].corr(comp_temp_land_africa_joined['temperature_anomaly_self'])\n",
    "\n",
    "    print('Correlation for region ' + temp_single_region + ' ' + str(temp_land_region_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Correlation of temperature anomalies between our and berkleys regional temp data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation for region Africa 0.9940777030851426\n",
      "Correlation for region Asia 0.8705964680397756\n",
      "Correlation for region Europe 0.9932100755076412\n",
      "Correlation for region Americas 0.94302727003253\n",
      "Correlation for region Oceania 0.8531672929606253\n"
     ]
    }
   ],
   "source": [
    "temp_berkley_regions = ['Africa', 'Asia', 'Europe', 'Americas', 'Oceania']\n",
    "\n",
    "for temp_berkley_region in temp_berkley_regions:\n",
    "    temp_calc_region_corr(temp_berkley_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Check temperature_anomaly columns for NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: 5\n",
      "Region: 0\n",
      "Global: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Country: \" + str(temp_country['temperature_anomaly'].isna().sum()))\n",
    "print(\"Region: \" + str(temp_region['temperature_anomaly'].isna().sum()))\n",
    "print(\"Global: \" + str(temp_global['temperature_anomaly'].isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Handle NaN values in temp_land_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "country_code\nGUM    2\nPNG    2\nSLB    1\ndtype: int64"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_country[temp_country['temperature_anomaly'].isna()].groupby('country_code').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_country['temperature_anomaly'] = temp_country['temperature_anomaly'].interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_country[temp_country['temperature_anomaly'].isna()==True].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Save the data as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Todo: Remove later\n",
    "temp_country.to_csv(TEMP_PROCESSED_FOLDER + 'temp-land-country.csv', index=False)\n",
    "temp_region.to_csv(TEMP_PROCESSED_FOLDER + 'temp-land-region.csv', index=False)\n",
    "temp_global.to_csv(TEMP_PROCESSED_FOLDER + 'temp-land-ocean-global.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "   year country_code  temperature_anomaly country_name\n0  1900          ABW            -0.061750        Aruba\n1  1900          AFG            -0.427250  Afghanistan\n2  1900          AGO             0.146167       Angola",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>country_code</th>\n      <th>temperature_anomaly</th>\n      <th>country_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1900</td>\n      <td>ABW</td>\n      <td>-0.061750</td>\n      <td>Aruba</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1900</td>\n      <td>AFG</td>\n      <td>-0.427250</td>\n      <td>Afghanistan</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1900</td>\n      <td>AGO</td>\n      <td>0.146167</td>\n      <td>Angola</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_country.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "   year  region_code  temperature_anomaly region_name\n0  1900          2.0            -0.016705      Africa\n1  1900          9.0            -0.307362     Oceania\n2  1900         19.0            -0.160803    Americas",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>region_code</th>\n      <th>temperature_anomaly</th>\n      <th>region_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1900</td>\n      <td>2.0</td>\n      <td>-0.016705</td>\n      <td>Africa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1900</td>\n      <td>9.0</td>\n      <td>-0.307362</td>\n      <td>Oceania</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1900</td>\n      <td>19.0</td>\n      <td>-0.160803</td>\n      <td>Americas</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_region.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "   year  temperature_anomaly\n0  1900            -0.125167\n1  1901            -0.199333\n2  1902            -0.362167",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>temperature_anomaly</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1900</td>\n      <td>-0.125167</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1901</td>\n      <td>-0.199333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1902</td>\n      <td>-0.362167</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_global.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}